\documentclass[12pt]{article}

\usepackage{amssymb, amsmath, amsfonts}
\usepackage{mathtools}
\usepackage{braket}
\usepackage{bussproofs}
\usepackage{amsthm}

\makeatletter
\newtheoremstyle{break}% name
    {12pt}%         Space above, empty = `usual value'
    {12pt}%         Space below
    {\addtolength{\@totalleftmargin}{1.5em}
     \addtolength{\linewidth}{-3em}
     \parshape 1 1.5em \linewidth
     \itshape}% Body font
    {}%         Indent amount (empty = no indent, \parindent = para indent)
    {\bfseries}% Thm head font
    {}%        Punctuation after thm head
    {\newline}% Space after thm head: \newline = linebreak
    {}%         Thm head spec
\theoremstyle{break}
\newtheorem{definition}{Definition}[section]
\theoremstyle{break}
\newtheorem{theorem}{Theorem}[section]
\theoremstyle{break}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{break}
\newtheorem{lemma}[theorem]{Lemma}


\newcommand{\bv}[1]{\boldsymbol{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\bc}[1]{\bv{\mc{#1}}}
\newcommand{\qq}[1]{``#1''}

\begin{document}
\title{Logic Notes}
\author{Justin Gerber}
\date{\today}
\maketitle

\section*{Old Introduction}

In this document I will try to get straight some of my thoughts on formal logic. At the moment the goal is to build up to a metalogic proof, in an extended Lemmon system of logic, that extensions by definition are conservative extensions to a formal theory. A lot of the difficulty comes from 1) the fact that the Lemmon system does not seem to be heavily used in the logic community so there are not really references; when syntactic (as opposed to semantic) proofs are given they often use the Hilbert system and 2) many references use semantic arguments to complete the proof (such as the completeness theorem and model theory generally) and I would like to see a purely syntactical proof.

I am following two references. ``Modern Logic: A Text in Elementary Symbolic Logic'' by Graehme Forbes and ``Lectures in Logic and Set Theory: Volume I'' by George Tourlakis. The former is my reference on Lemon logic and the rules therein while The Tourlakis book is more rigorous and does a cleaner job introducing ideas with the metalanguage or metatheory including the concept of inducting on formulas using the metalanguage. However, Tourlakis uses the Hilbert system and semantic proofs in cases which is why I need to adapt both approaches.

\section*{Preface}
The previous introduction was written around February 2019, the last time I took up investigations into formal logic. I'm revisiting formal logic again (Aug 2021).
Typically I find myself looking into formal logic after the following sequence of events.
1) I am interested in some physics topic, typically somewhat mathematically oriented in nature, 2) I look into the math supporting this topic, 3) I get curious about the deep mathematical definitions or theorems involved in the math topic 4) I get into very deep math subjects such as topology or the very definitions of functions 5) I finally find myself back facing formal logic.

Back in 2019 I believe I was interested in the relationship between classical and quantum random variables.
This led me to look into mathematical formulations of probability theory which took me eventually to measure theory.
In learning measure theory I was attempting to learn about Borel sets and some basic topology again.
I believe I then got curious about the definition of infinite unions and this brought me down to formal logic.
I was then curious at the time about how to build from ZFC up to larger mathematical theories.

Such an undertaking requires the extension by definition of a logical theory.
Extension by definition of a logical theory involves introducing a new symbol to the language and adding a ``defining'' axiom for that logic into the theory.
For appropriate defining axioms the new symbol should not change the theory in the sense that the extension of the theory is conservative.
An extension of a theory is conservative if a formula of the new language, which is a valid formula of the old language, can be deduced in the new theory exactly when it can be deduced in the old theory.
The challenge I faced last time was trying to prove the claim that appropriate extensions by definition are conservative.

I learned a lot about formal logic and had some luck, but the approach I was taking ended up being too tedious.
Two references I utilized addressed the problem: \textit{Lectures in Logic and Set Theory: Volume I} by George Tourlakis and \textit{Introduction to Mathematical Logic} by Elliott Mendelson.
However, both of these books were based on Hilbert style formal logic.

At the time I found this frustrating.
My background in formal logic (from a course I took my freshman year in college in 2009) was from an introductory textbook called \textit{Modern Logic: A Text in Elementary Symbolic Logic} by Graehme Forbes.
This book used a natural deduction approach for proofs and visualized proofs using a Lemmon logic tabular proof style.
I found this latter style to be very natural, and, combined with self study on the Zermelo-Fraenkel axioms of set theory, I, at least subconsciously, understood that it would be possible to put all of mathematics into this mathematical formalism.

I found the axiomatic approaches in the Hilbert style logic to be unnatural and annoying.
A theory didn't seem elegant when all tautologies are assumed as axioms.
It was much more natural to me to have a theory which has no ``logical'' axioms, but from which tautologies could be derived via the rules of inference.
This is of course the same story as the origin of Gentzen's natural deduction.

I have recently revisited this problem.
The physics problem I was trying to understand is how to derive the 3D multipole vector fields describing electromagnetic radiation from multipole charge and current distributions.
This led me to try to understand some theorems about differential equations.
In learning about existence theorems for solutions to, for example, the Laplace equation, i was directed towards, simultaneously, it seems, complex analysis and the fundamental theorem of algebra.
In proving the fundamental theorem of algebra I found myself visiting some familiar topics in multivariable calculus.
At this time I hit upon a confusion/frustration that has cropped its head up for me time and time again.
This frustration regards the notation for partial derivatives.

We often have, for example $\partial f/\partial x$.
It is implicitly understood that this means differentiation with respect to the `first parameter' of the function $f$.
In cases of nested functions the notation gets complicated, confusing, and sometimes ambiguous so I seek a rigorous definition.
A long story short, this led me to try to understand a set theoretic definition of a multivariable function.
Finally, in the context of ZFC set theory, function notation arises from symbols which have been added to the theory via extensions by definition.
This brought me back to formal logic.

I walked over many of the tracks I had explored before.
One thing I understood then and I understand now is that much of my difficulty in developing the proof I was interested in was that I did not have a formal enough definition of the rules of inference.
I was relying on the rules of inference as they were laid out in the Graehme textbook.
Unfortunately the definitions there were too heavily tied to the Lemmon tabular proof structure making it very difficult to reason generally about proof structure.

Just recently I came to a more thorough understanding of the sequent calculus.
I believe that the sequent calculus provides rigorous enough definitions of inference rules for me to complete the proof I am interested in now.
Additionally, I've hit upon a reference which is close to the flavor of formal logic I am interested in.
This is \textit{Structural Proof Theory} by Sara Negri and Jan Von Plato.
In fact, I learned that exactly what I am trying to do is a topic in the field of structural proof theory.

One of the major difficulties I found when trying to proof extensions by definition are conservative in the past is the fact that natural deduction logic typically has many inference rules.
When developing an inductive metalogic proof it is necessary to induct over all of these different rules making the proof extremely tedious.
To this end I became very curious if any of the inference rules are redundant.
That is, it is known that some logical connectives can be written in terms of others.
For example: $\mc{A}\land\mc{B} \equiv \lnot(\mc{A}\implies \lnot\mc{B})$.
If the rules of inference involving $\land$ could be derived from the rules of inference for $\implies$ and $\lnot$ then it would be possible to dispense with the $\land$ symbol as a native part of the language and simply maintain it as an abbreviation for the above expression, also taking the corresponding inference rules as derived rules.
The would reduce the number of cases over which we need to induct for the metalogical proofs that follow.
I believe this program is possible and that will be part of this document.

Another difficulty I faced was that the rules of inference involving quantifiers sometimes involve various restrictions on the formulas which are being replaced.
Without a nice way to notate these restrictions I've found it difficult to work with these rules in metalogical proofs.
I hope that in this revisit I'll be able to overcome this difficulty.


\newpage
\section{Introduction}

\subsection{Formal Logic and Language}
All of traditional mathematics can be expressed in terms of mathematical set theory.
Set theory itself is expressed in terms of a formal logic called first-order logic \textbf{FOL} which is expressed in the language of first-order logic \textbf{LFOL} .
The \textbf{LFOL} is a written formal language which means that it is composed of a clearly and well-defined set of symbols which constitutes the alphabet of that language and clear rules of syntax which identify `appropriate' ways in which the symbols can be combined to form well-formed formulas, or Wffs.
In addition to \textbf{LFOL}, \textbf{FOL} includes a deductive calculus based on inference rules which allows us to derive new Wffs from old Wffs in a way which will be made clear below.
\textbf{FOL} is fully captured by the language in which it is expressed, \textbf{LFOL}, and the inference rules which allow for deduction within the logic.

A formal theory within a formal logic begins with a set of axioms which are a subset of the Wffs of the language.
It is then possible, using the inference rules, to derive new formulas from these axioms using the inference rules.
The main question we ask about a formal theory is which Wffs can be proven from the given axioms?

\subsection{Syntax and Semantics}
There are two important topics in formal logic: syntax and semantics.
Syntax pertains to the formal rules listed above such as which symbols are in the alphabet, how can they be combined to form Wffs, and what are the inference rules which allows us to manipulate Wffs to derive new ones.
Semantics is the task of assigning meaning to Wffs.
In its simplest form semantics is the assigning of a truth value to Wffs.

Let's consider an example.
Suppose we would like to formalize the statement: If $x$ is an integer and $x$ is odd then $x \div 2$ is an integer.

$$
((\forall x)((\in x \mathbb{N} \land Ox)\implies \in \div x 2 \mathbb{N}))
$$

The smallest element in our formal language is the \textit{term}.
A term is one of three things (1) a concrete object such as the number \qq{2}, (2) a variable into which concrete objects can be `plugged in' such as \qq{x}, or (3) an $n$-ary function of other terms such as \qq{$\div$}.
In fact, below we will express a concrete object as a 0-ary function.
The terms in the above expression are:

\begin{align}
& x \\
& \mathbb{N} \\
& x \\
& \div x 2 \\
& x \\
& 2 \\
& \mathbb{N}
\end{align}

Here I have listed every term as it appears from left to right.
We can see that some terms appear multiple times in the formula.
The only variable which appears is \qq{$x$}.
We see \qq{2} and \qq{$\mathbb{N}$} appearing as 0-ary functions.
We see \qq{$\div$} appearing as a 2-ary function with arguments \qq{$x$} and \qq{2} which are a variable and 0-ary function respectively.
Note that in our formal language we always use prefix notation for functions and predicates so we symbolize \qq{$\div x2$} rather than \qq{$x \div 2$} as we would see in typical infix notation.

The next larger element in our formal language is the \textit{atomic formula}.
An atomic formula is the combination of an $n$-ary predicate symbol and $n$ terms.
We have 2 predicate symbols in the above formula: \qq{$\in$} is a 2-ary predicate and \qq{$O$} is a 1-ary predicate symbol.
The atomic formulas are then

\begin{align}
\mc{A} \equiv& \in x \mathbb{N}\\
\mc{B} \equiv& Ox\\
\mc{C} \equiv& \in \div x2 \mathbb{N}
\end{align}

Here $\mc{A}$, $\mc{B}$, and $\mc{C}$ are metalanguage symbols which stand for the corresponding Wffs on the right hand sides of the $\equiv$ symbol.

Finally, the next larger object in our language is the Wff.
Atomic formulas themselves are already Wffs, but we can form larger more complex Wffs by stringing together smaller Wffs with connectives such as \qq{$\land$} and \qq{$\implies$} or quantifying with $(\forall x)$.
We see sub-Wffs above:

\begin{align}
& ((\forall x)((\in x \mathbb{N} \land Ox)\implies \in \div x 2 \mathbb{N})) \\
& ((\in x \mathbb{N} \land Ox)\implies \in \div x 2 \mathbb{N}) \\
& (\in x \mathbb{N} \land Ox)\\
& \in x \mathbb{N}\\
& Ox \\
&  \in \div x 2 \mathbb{N}
\end{align}

The above discussion has been a purely syntactic analysis of the given Wff.
That is, we broke down the Wff into smaller parts of the formal language.
We will see below that this Wff is, in fact, a Wff according to the formal rules of syntax.

Semantics comes in when we give an \textit{interpretation} to the various expressions above.
For example, when we \textit{interpret} the symbol \qq{2} as the number 2, the expression \qq{$\div x 2$} as the division of the variable $x$ by the number 2, or predicate \qq{$Ox$} to mean that the variable $x$ is odd, we are imposing semantics onto the expression.
We also are making a semantic interpretation any time we impose or deduce a truth value for a given Wff.
For example, on all of the usual interpretations, we would assign a value of false to the Wff above because the odd integers are in fact, not divisible by 2.

In this document I would like to follow a game formalist approach to formal logic.
This is an approach which holds that it is possible to describe set theory, and by extension, all of mathematics using only syntactic methods.
Or, at the very least, it asks the question how far one can get using such an approach.
To that end, I would like it to be clear that all definitions and arguments should be self-contained within purely syntactic analyses.
At a few times I may make reference to semantic ideas, but this is only to motivate the definition.
One could, instead, proceed by dropping semantics entirely from the narrative.
In this case some definitions may appear unmotivated, at least at first, but that is not, technically, a problem for the game formalist.

\subsection{Outline}

The goal of this work is to prove that extensions by definition to a formal theory are conservative extensions.

I will begin with a thorough outlining of \textbf{LFOL}, starting with the alphabet, and then moving onto syntax.
In addition, some useful metalanguage concepts such as term substitution will be defined.
The conventions chosen in my definitions for \textbf{LFOL} are chosen with the goal of easing the proof about definition by extension to follow.
For example, to minimize metalogical inductive cases, I restrict to using a small subset of connectives, rather than the full intuitive set sometimes chosen.

After \textbf{LFOL} is laid out we move into proof theory.
We being by describing sequents and follow with the concept of a formal proof and our corresponding rules of inference.
We then derive a number of derived inference rules, including inference rules for the abbreviated symbols which were not included natively in the language.

Next I will define a logical theory based on a set of axioms.
At this point I will then define a conservative extension to a theory, and an extension by definition to a theory.
There are two types of extensions by definition.
We either define new predicate symbols, or new function symbols.
In the remainder of this document I will prove that these two forms of extensions by definition to a logical theory are in fact conservative extensions.
These proofs will utilize metalogical induction.

\newpage
\section{The Language of First Order Logic (LFOL)}

At the basis of a formal language is a set of symbols which can be concatenated together to form strings.
If a string follows certain allowed construction rules, specified by the syntax of the language, then the string is said to be a well-formed formula, or Wff.

There are multiple classes of symbol objects, each of which plays a different role in the syntax of the language.
We first distinguish between logical and non-logical symbols.
Logical symbols are symbols which are related purely to the formal presentation of the language.
Logical symbols include things like logical connectives and parentheses.
The second main class of symbols is the class of non-logical symbols.
The non-logical symbols include the predicate and function symbols.
The symbols included in the non-logical symbols may vary from one application of the formal language to the next.


\subsection{The Lexicon of LFOL:}

\begin{definition}[Logical Symbols]
The logical symbols are defined as follows.
\begin{itemize}
\item{\textbf{Variables:} Variable symbols such as $x_1, x_2, \ldots, v_1, v_2, \ldots$. We will try to use lower case letters from near the end of the alphabet for variables. The set of all variables is denoted $\textbf{Var}$.}
\item{\textbf{Logical Operators:} The logical operators for implication: $\implies$, and negation: $\lnot$. The set of all logical operators is denoted $\textbf{Lop}$.}
\item{\textbf{Quantifiers:} The for all quantifier: $\forall$. The set of all quantifiers is denoted $\textbf{Quant}$.}
\item{\textbf{Punctuation Marks:} Parenthesis $($ and $)$. The set of all punctuation marks is denoted $\textbf{Punct}$.}
\end{itemize}


The set of all logical symbols is denoted $\textbf{LogSymb}$ and is the disjoint union of the variables, logical operators, quantifiers and punctuation marks: $\textbf{LogSymb} = \textbf{Var} \sqcup \textbf{Lop} \sqcup \textbf{Quant} \sqcup \textbf{Punct}$.
\end{definition}

We have chosen to use a restricted set of logical operators in the theory to the exclusion of the conjunction, $\land$, disjunction, $\lor$, and equivalence, $\iff$ connectives and the existential quantifier, $\exists$.
This reduced set of logical symbols will reduce the number of cases over which we need to induct for metalogical proofs.
However, we will need to introduce the $\land$, $\lor$, and $\iff$ symbols as abbreviations and metalogically derive the appropriate corresponding rules of inference.

\begin{definition}[Non-Logical Symbols]
The non-logical symbols are defined as follows.
\begin{itemize}
\item{\textbf{Predicate Symbols:} for each arity $0 \le n \le N_{pred}$ predicate symbols such as $P, Q, R$. Examples are $\in, \subset, <$. These example all have arity 2 but other aritys are possible. The contradiction symbol, $\curlywedge$, is taken to be a 0-ary predicate in our language. The set of all $n$-ary predicate symbols is denoted $\textbf{Pred}_n$. The set of all predicate symbols is denoted $\textbf{Pred}$ and is the disjoint union of the sets of all $n$-ary predicate symbols: $\textbf{Pred} = \bigsqcup_{i=1}^{N_{\text{pred}}} \textbf{Pred}_i$.}
\item{\textbf{Function Symbols}: for each arity $0 \le n \le N_{\text{func}}$ function symbols such as $f, g, h$. Examples are $+$ and $\times$, these examples both have arity $2$. Note that function symbols of arity $0$ are the same as constants such as $a,b,c,\emptyset,4$. The set of all $n$-ary function symbols is denoted $\textbf{Func}_n$. The set of all function symbols is the disjoint union of the sets of all $n$-ary function symbols and is denoted $\textbf{Func} = \bigsqcup_{n=1}^{N_{\text{func}}} \textbf{Func}_i$. We reserve constants $\alpha_0, \alpha_1, \ldots \in \textbf{Func}_0$ as `placeholder' constants that exist in any logical theory. The placeholder constants will be used for proofs involving universal specification and generalization, but will not be permitted to appear in logical axioms.}
\end{itemize}

The set of all non-logical symbols is denoted $\textbf{NonLogSymb}$ and is the disjoint union of the predicate and function symbols: $\textbf{NonLogSymb} = \textbf{Pred} \sqcup \textbf{Func}$. If we are not worried about having an infinite number of distinct symbol types then we can let $N_{\text{pred}} = N_{\text{func}} = \infty$.
\end{definition}

\begin{definition}[The Set of All Symbols]
The set of all symbols in the language is denoted $\textbf{Symb}$ and is the disjoint union of the logical and non-logical symbols: $\textbf{Symb} = \textbf{LogSymb} \sqcup \textbf{NonLogSymb}$.
\end{definition}

\subsection{Strings}
\subsubsection{Definition of and Basic Facts about Lists}
The symbols in $\textbf{Symb}$ can be combined into collections of symbols called strings.
Formally, we will define strings as finite ordered lists of symbols.
Most of this section will be devoted to defining and describing finite ordered lists.

\begin{definition}[Natural Number Subset]
Let $[n]$ denote the (possibly empty) set of natural numbers (including 0) which are less than $n$
\end{definition}

As an example we have that $[5] = \{0, 1, 2, 3, 4\}$.
Note that $[0] = \{\} = \emptyset$.

\begin{definition}[Finite Ordered List]
If $\Sigma$ is any set of objects then a length $n$ finite ordered list of objects in $\Sigma$ is a functional mapping from $[n]$ to $\Sigma$.\footnote{When we use the term `list' below we always mean `finite ordered list'.}  The collection of all lists over a set of objects $\Sigma$ is denoted by $\Sigma^*$.
\end{definition}


An example of a list of elements of $\textbf{Symb}$ defined above is is
\begin{align*}
[&[0, x_1],\\
&[1, \lnot],\\
&[2, Q],\\
&[3, \curlywedge],\\
&[4, (]]
\end{align*}
This list could also be written as
$$
[[0, x_1], [1, \lnot], [2, Q], [3, \curlywedge], [4, (]]
$$
We can also abbreviate the list expression by excluding the explicit inclusion of the numbering:
$$
[x_1, \lnot, Q, \curlywedge, (]
$$

\begin{definition}[List Length]
If $\mc{L}$ is a mapping from $[n]$ into a set of objects $\Sigma$ then we say that $\mc{L}$ is a length $n$ list of elements of $\Sigma$.
We denote the length of the list by $|\mc{L}| = n$.
Note that $|\mc{L}| \ge 0$.
\end{definition}

The natural number length of a list is a fundamental part of its definition in this formal setting.

\begin{definition}[Symbol Equality]
If $\alpha, \beta \in \Sigma$ and $\alpha$ and $\beta$ represent the same object then we may write $\alpha \equiv \beta$.
Otherwise, if $\alpha$ and $\beta$ represent different objects we may write $\alpha \not \equiv \beta$.
\end{definition}

\begin{definition}[List Element Extraction]
Suppose $\mc{L}$ is a list with $|\mathcal{L}| = n$.
For all $i$ with $0 \le i < n$ we denote the $i^{\text{th}}$ element of $\mc{L}$ (with 0-indexing) by $\mc{L}[i]$.
\end{definition}

For example, in the list above we have $\mc{L}[2] \equiv Q$.

\begin{definition}[List Equality]
If $\mc{A}$ and $\mc{B}$ are two lists which satisfy $|\mc{A}| = |\mc{B}| = n$ and for all $0 \le i < n$ we have $\mc{A}[i] \equiv \mc{B}[i]$ then we say that $\mc{A}$ and $\mc{B}$ are the same list and we may write $\mc{A} \equiv \mc{B}$.
\end{definition}

Recall that a list can be a mapping from $[0] = \emptyset$.

\begin{definition}[Empty Lists]
If $\mc{L}$ is a mapping from the empty set $[0] = \emptyset$ then we say $\mc{L}$ is an empty list.
We can see that if $\mc{L}$ is an empty list then $|\mc{L}| = 0$.
An empty list could be written as $[]$.
\end{definition}

\begin{theorem}[The Empty List is Unique]
Suppose $\mc{E}$ and $\mc{E}'$ are both empty lists.
We have that $|\mc{E}| = |\mc{E}'| = 0 $.
Since there is no $i$ satisfying $0\le i < 0$ we need not check equality for any elements of these empty lists.
Therefore $\mc{E} \equiv \mc{E}'$.
\end{theorem}

\begin{definition}[The Empty List]
Because of the uniqueness of the empty list we are justified in defining a notation for it. The empty list can be expressed as $[]$ and we define $\mc{E} \equiv []$.
\end{definition}

\subsubsection{List Concatenation}
If we are given two lists, $\mc{A}$ and $\mc{B}$, we can define a third list by concatenating these two lists.

\begin{definition}[List Concatenation]
Suppose $\mc{A}$, $\mc{B}$, and $\mc{C}$ are lists.
If $\mc{C}$ satisfies
\begin{enumerate}
\item{$|\mc{C}| = |\mc{A}| + |\mc{B}|$}
\item{for all $i$ with $0 \le i < |\mc{A}|$ we have $\mc{C}[i] \equiv \mc{A}[i]$}
\item{for all $i$ with $0 \le i < |\mc{B}|$ we have $\mc{C}[|\mc{A}| + i] \equiv \mc{B}[i]$}
\end{enumerate}
Then we say $\mc{C}$ is the concatenation of $\mc{A}$ and $\mc{B}$ and we write
$$
\mc{C} \equiv \mc{A}\circ \mc{B}
$$
We often abbreviate $\mc{A}\circ\mc{B}$ as $\mc{A}\mc{B}$.
Note that such a $\mc{C}$ always exists.
\end{definition}

\begin{theorem}[Equality of Concatenation with list implies Equality of Lists]
Suppose $\mc{A}\mc{B} \equiv \mc{A}\mc{B}'$.
Then $\mc{B} \equiv \mc{B}'$.
First, $|\mc{A}\mc{B}| = |\mc{A}| + |\mc{B}| = |\mc{A}\mc{B}'| = |\mc{A}| + |\mc{B}'|$ so $|\mc{B}| = |\mc{B}'| = n$.
Choose $0\le i < n$.
By the definition of concatenation we have $\mc{A}\mc{B}[|\mc{A}| + i] \equiv \mc{A}\mc{B}'[|\mc{A}| + i] \equiv \mc{B}[i] \equiv \mc{B}'[i]$.
Since this holds for $0\le i < n$ we have $\mc{B} \equiv \mc{B}'$.

Now suppose $\mc{A}\mc{B}\equiv \mc{A}'\mc{B}$. Then $\mc{A}\equiv \mc{A}'$.
First $|\mc{A}\mc{B}| \equiv |\mc{A}| + |\mc{B}| = |\mc{A}'\mc{B}| = |\mc{A}'\mc{B}| = |\mc{A}'| + |\mc{B}|$ so $|\mc{A}| = |\mc{A}'| = n$. Consider $0\le i < n$.
By the definition of concatenation we have $\mc{A}\mc{B}[i] \equiv \mc{A}'\mc{B}[i] \equiv \mc{A}[i] \equiv \mc{A}'[i]$.
Since this holds for $0 \le i < n$ we have $\mc{A} \equiv \mc{A}'$.


\end{theorem}

\begin{theorem}[Concatenation with the Empty List]
Here we prove that $\mc{E}\mc{B} \equiv \mc{B} \mc{E} \equiv \mc{B}$.

For the first case, suppose $|\mc{B}|= 0$ so that $\mc{B}\equiv \mc{E}$.
We can check that the set $\mc{E}$ satisfies the conditions to be the concatenation $\mc{E}\mc{E}$.
First $|\mc{E}| = 0 = 0 + 0 = |\mc{E}| + |\mc{E}|$.
Next, there is no $i$ satisfying $0 \le i < |\mc{B}|$ or $0 \le i < |\mc{E}|$ so the two element equality conditions for concatenation are vacuously satisfied.
Therefore $\mc{E}\mc{E} \equiv \mc{E}\mc{B} \equiv \mc{B}\mc{E} \equiv \mc{E}$.

Consider $|\mc{B}|>0$ .

Let $\mc{C} \equiv \mc{E}\mc{B}$.
First we see that $|\mc{C}| = |\mc{E}| + |\mc{B}| = |\mc{B}|$.
By the definition of concatenation we have that for $0 \le i < |\mc{B}|$ that $\mc{C}[|\mc{E}| + i] \equiv \mc{C}[i] \equiv \mc{B}[i]$.
This means that $\mc{E}\mc{B}\equiv\mc{B}$.

Let $\mc{C} \equiv \mc{B}\mc{E}$.
Again $|\mc{C}| \equiv |\mc{B}|+|\mc{E}| = |\mc{B}|$.
By the definition of concatenation we have that for $0 \le i < |\mc{B}|$ that $\mc{C}[i] \equiv \mc{B}[i]$.
This means $\mc{B}\mc{E} \equiv \mc{B}$.
\end{theorem}


\begin{theorem}[List Concatenation is Associative]
\label{thm:concatassoc}
Here we will prove that $\Braket{\mc{A}\circ \mc{B}}\circ \mc{C} \equiv \mc{A} \circ \Braket{\mc{B} \circ \mc{C}}$.
The angle brackets here denote the order of operations for the multiple concatenations.

First, if $\mc{A} \equiv \mc{E}$ then $\Braket{\mc{A}\mc{B}}\mc{C} \equiv \mc{B}\mc{C} \equiv \mc{A}\Braket{\mc{B}\mc{C}}$.
If $\mc{B}\equiv \mc{E}$ then $\Braket{\mc{A}\mc{B}}\mc{C} \equiv \mc{A}\mc{C} \equiv \mc{A}\Braket{\mc{B}\mc{C}}$.
If $\mc{C}\equiv \mc{E}$ then $\Braket{\mc{A}\mc{B}}\mc{C} \equiv \mc{A}\mc{B} \equiv \mc{A}\Braket{\mc{B}\mc{C}}$. So the theorem holds if any of $\mc{A}$, $\mc{B}$ or $\mc{C}$ is empty.

Now suppose $\mc{A}$, $\mc{B}$, and $\mc{C}$ are all nonempty so that $|\mc{A}|, |\mc{B}|, |\mc{C}| > 0$.
First we have that
$$
|\Braket{\mc{A}\mc{B}}\mc{C}| = |\mc{A}\mc{B}| + |\mc{C}| = |\mc{A}| + |\mc{B}| + |\mc{C}| = |\mc{A}| + |\mc{B}\mc{C}| = |\mc{A}\Braket{\mc{B}\mc{C}}|.
$$

Consider $0 \le i < |\mc{A}| + |\mc{B}| + |\mc{C}|$.
We must show
$$
\Braket{\Braket{\mc{A}\mc{B}}\mc{C}}[i] \equiv \Braket{\mc{A}\Braket{\mc{B}\mc{C}}}[i]
$$
There are three cases
\begin{itemize}
\item{
Suppose $0 \le i < |\mc{A}|$.
Consider $\Braket{\mc{A}\mc{B}}\mc{C}$.
Since $0 \le i < |\mc{A}| \le |\mc{A}\mc{B}|$ we have that $\Braket{\Braket{\mc{A}\mc{B}}\mc{C}}[i] \equiv \Braket{\mc{A}\mc{B}}[i]$.
Again, since $0\le i < |\mc{A}|$, we have $\Braket{\mc{A}\mc{B}}[i] \equiv \mc{A}[i]$ so that $\Braket{\Braket{\mc{A}\mc{B}}\mc{C}}[i] \equiv \mc{A}[i]$.
On the other hand, consider $\mc{A}\Braket{\mc{B}\mc{C}}$.
Since $0 \le i < |\mc{A}|$ we have $\Braket{\mc{A}\Braket{\mc{B}\mc{C}}} \equiv \mc{A}[i]$.
So in this case $\Braket{\Braket{\mc{A}\mc{B}}\mc{C}}[i] \equiv \Braket{\mc{A}\Braket{\mc{B}\mc{C}}}[i]$.
}
\item{
Now suppose $|\mc{A}| \le i < |\mc{A}| + |\mc{B}|$.
Define $j = i - |\mc{A}|$ so that $0 \le j < |\mc{B}|$.
Consider $\Braket{\mc{A}\mc{B}}\mc{C}$.
Because $0 \le |\mc{A}| \le i < |\mc{A}| + |\mc{B}| = |\mc{A}\mc{B}|$ we have $\Braket{\Braket{\mc{A}\mc{B}}\mc{C}}[i] \equiv \Braket{\mc{A}\mc{B}}[i] \equiv \Braket{\mc{A}\mc{B}}[|\mc{A}| + j]$. Because $0 \le j < |\mc{B}|$ we have $\Braket{\mc{A}\mc{B}}[|\mc{A}|+j] = \mc{B}[j] \equiv \mc{B}[i - |\mc{A}|]$.
On the other hand, consider $\mc{A}\Braket{\mc{B}\mc{C}}$.
Because $0 \le j < |\mc{B}| \le |\mc{B}\mc{C}|$ we have $\Braket{\mc{A}\Braket{\mc{B}\mc{C}}}[i] \equiv \Braket{\mc{A}\Braket{\mc{B}\mc{C}}}[|\mc{A}| + j] \equiv \Braket{\mc{B}\mc{C}}[j]$. But, since $0 \le j < |\mc{B}|$ we have $\Braket{\mc{B}\mc{C}}[j] \equiv \mc{B}[j] \equiv \mc{B}[i - |\mc{A}|]$. So in this case $\Braket{\Braket{\mc{A}\mc{B}}\mc{C}}[i] \equiv \Braket{\mc{A}\Braket{\mc{B}\mc{C}}}[i]$.
}
\item{
Now suppose $|\mc{A}| + |\mc{B}| \le i < |\mc{A}| + |\mc{B}| + |\mc{C}|$.
Define $j = i - |\mc{A}| - |\mc{B}|$ so that $0 \le j < |\mc{C}|$. Consider $\langle\mc{A}\mc{B}\rangle\mc{C}$.
Because $0 \le j < |\mc{C}|$ we have $\Braket{\Braket{\mc{A}\mc{B}}\mc{C}}[i] \equiv \langle\langle\mc{A}\mc{B}\rangle\mc{C}\rangle[|\mc{A}| + |\mc{B}| + j] \equiv \langle\langle\mc{A}\mc{B}\rangle\mc{C}\rangle[|\mc{A}\mc{B}| + j] \equiv \mc{C}[j] \equiv \mc{C}[i - |\mc{A}| - |\mc{B}|]$.
On the other hand, consider $\mc{A}\Braket{\mc{B}\mc{C}}$.
We have $\Braket{\mc{A}\Braket{\mc{B}\mc{C}}}[i] \equiv \Braket{\mc{A}\Braket{\mc{B}\mc{C}}}[|\mc{A}| + |\mc{B}| + j]$.
We have that $0 \le |\mc{B}| \le |\mc{B}| + j < |\mc{B}| + |\mc{C}| = |\mc{B}\mc{C}|$ so $\Braket{\mc{A}\Braket{\mc{B}\mc{C}}}[|\mc{A}| + |\mc{B}| + j] \equiv \Braket{\mc{B}\mc{C}}[|\mc{B}| + j]$.
But, $0 \le j < |\mc{C}|$ so $\Braket{\mc{B}\mc{C}}[|\mc{B}|+j] \equiv \mc{C}[j] \equiv \mc{C}[i - |\mc{A}|-|\mc{B}|]$.
So in this case $\langle\langle\mc{A}\mc{B}\rangle\mc{C}\rangle[i] \equiv \langle\mc{A}\langle\mc{B}\mc{C}\rangle\rangle[i].$
}

In all cases we have $\langle\langle\mc{A}\mc{B}\rangle\mc{C}\rangle[i] \equiv \langle\mc{A}\langle\mc{B}\mc{C}\rangle\rangle[i]$.
This means that $\langle\mc{A}\mc{B}\rangle\mc{C} \equiv \mc{A}\langle\mc{B}\mc{C}\rangle$.
\end{itemize}
\end{theorem}

\begin{definition}[Multiple Concatenation]
Here we define the set of $n$-concatenations over a list of lists.
Note that lists of lists are different than lists which are themselves lists of objects.
If $S$ is a list of $n$ lists, i.e. $S\in \textbf{Str}^*$ and $|S| = n$, then $\circ_n(S)$ is defined to be the set of all lists which arise from concatenations of the $n$ lists in $S$.
We define $\circ_n$ recursively.

\begin{itemize}
\item{We define $\circ_1([\mc{A}]) = \{\mc{A}\}$}
\item{$\circ_n([\mc{A}_0,\ldots, \mc{A}_{n-1}]) = \bigcup_{i=1}^{n-1} \{\mc{X}\circ\mc{Y}:\mc{X} \in \circ_i([\mc{A}_0,\ldots \mc{A}_{i-1}]) \text{ and } \mc{Y}\in \circ_{n-i}([\mc{A}_{i}, \ldots, \mc{A}_{n-1}])\}$}
\end{itemize}
We say that $\circ_n(S)$ is the set of all parenthesizations of the concatenations of the $n$ lists in $S$.

\end{definition}

As an example consider $\circ_4([\mc{A}, \mc{B}, \mc{C}, \mc{D}])$. This will be equal to
\begin{align*}
\circ_4(\mc{A}, \mc{B}, \mc{C}, \mc{D}) = \{&\Braket{\mc{A}\circ\Braket{\mc{B}\circ\Braket{\mc{C}\circ\mc{D}}}},\\
&\Braket{\mc{A}\circ\Braket{\Braket{\mc{B}\circ\mc{C}}\circ\mc{D}}},\\
&\Braket{\Braket{\mc{A}\circ\mc{B}}\circ\Braket{\mc{C}\circ\mc{D}}},\\
&\Braket{\Braket{\mc{A}\circ\Braket{\mc{B}\circ\mc{C}}}\circ\mc{D}},\\
&\Braket{\Braket{\Braket{\mc{A}\circ\mc{B}}\circ\mc{C}}\circ\mc{D}}\}
\end{align*}


\begin{theorem}[Generalized Associativity of Multiple Concatenation]
The generalized associativity property for multiple concatenation states that, for $S\in\textbf{Str}^*$ with $S=[\mc{A}_0,\ldots,A_{n-1}]$ that $\circ_n(S)$ has only one element.
That is, if $\mc{A}, \mc{B}\in\circ_n(S)$ then $\mc{A}\equiv \mc{B}$.
We may denote this unique element as $\mc{A}_0\circ\ldots\circ\mc{A}_{n-1}$, which can be abbreviated as $\mc{A}_0\ldots\mc{A}_{n-1}$.
We proceed by induction on $n$.

If $n=1$ then $|S|=1$ and $S = [\mc{A}]$.
We then have $\circ_1(S) = \{\mc{A}\}$.
This set only has one element so we are done.

The induction hypothesis is that for all $m<n$ that $\circ_m([\mc{A}_0, \ldots, \mc{A}_{m-1}])$ contains a single unique element.
This element is denoted by $\mc{A}_0\ldots\mc{A}_{m-1}$.

We have the following useful result.
Suppose $1 \le l \le {m-1}$.
Since $l < m < n$ we have, by the induction hypothesis, that $\circ_l([\mc{A}_0, \ldots, \mc{A}_{l-1}])$ has a single unique element denoted by $\mc{A}_0\ldots\mc{A}_{l-1}$ (or simply $\mc{A}_0 \equiv \mc{A}_{l-1}$ if $l=1$).
Since $m-l < m < n$ we also have, again by the induction hypothesis, that $\circ_{m-l}([\mc{A}_l, \ldots, \mc{A}_{m-1}])$ has a single unique element denoted by $\mc{A}_l\ldots\mc{A}_{m-1}$ (or simply $\mc{A}_l \equiv \mc{A}_{m-1}$ if $l=m-1$).
This means that
$$
\mc{A}_0\ldots\mc{A}_{l-1} \circ \mc{A}_l\ldots\mc{A}_{m-1} \in \circ_m([\mc{A}_0, \ldots, \mc{A}_{m-1}])
$$
But, by the induction hypothesis, we know this latter set has a single unique element which means
$$
\mc{A}_0\ldots\mc{A}_{m-1} \equiv \mc{A}_0\ldots\mc{A}_{l-1}\circ \mc{A}_l \ldots \mc{A}_{m-1}
$$
We will use this result below.

Consider now
\begin{align*}
\circ_n([\mc{A}_0,\ldots, \mc{A}_{n-1}]) = \bigcup_{i=1}^{n-1} \{\mc{X}\circ\mc{Y}:\mc{X} \in \circ_i([\mc{A}_0,\ldots \mc{A}_{i-1}]) \text{ and } \mc{Y}\in \circ_{n-i}([\mc{A}_{i}, \ldots, \mc{A}_{n-1}])\}
\end{align*}
Because $i<n$ we know, by the induction hypothesis, that $\circ_i([\mc{A}_0, \ldots, \mc{A}_{i-1}])$ has a single unique element denoted by $\mc{A}_0\ldots\mc{A}_{i-1}$.
Likewise, because $n-i < n$, we know, again by the induction hypothesis, that $\circ_{n-i}([\mc{A}_i, \ldots, \mc{A}_{n-1}])$ has a single unique element denoted by $\mc{A}_i \ldots \mc{A}_{n-1}$.
We then have
\begin{align}
\circ_n([\mc{A}_0, \ldots, \mc{A}_{n-1}]) = \bigcup_{i=1}^{n-1} \{\mc{A}_0\ldots\mc{A}_{i-1}\circ\mc{A}_i\ldots\mc{A}_{n-1}\}
\end{align}

Now suppose $\mc{X}, \mc{Y} \in \circ([\mc{A}_0, \ldots, \mc{A}{n-1}])$.
This means there exists $j, k$ with $1 \le j \le n-1$ and $1 \le k \le n-1$ with
\begin{align}
\mc{X} \equiv& \mc{A}_0\ldots\mc{A}_{j-1} \circ \mc{A}_j \ldots \mc{A}_{n-1}\\
\mc{Y} \equiv& \mc{A}_0\ldots\mc{A}_{k-1} \circ \mc{A}_k \ldots \mc{A}_{n-1}
\end{align}
We will now show $\mc{X} \equiv \mc{Y}$.

If $j=k$ then clearly $\mc{X}\equiv \mc{Y}$.
We now suppose, without loss of generality, that $k>j$.
By the intermediate result on the induction hypothesis above, and $n-j < n$ and $k<n$, we can write this as
\begin{align}
\mc{X} \equiv& \mc{A}_0\ldots\mc{A}_{j-1} \circ \langle\mc{A}_j\ldots\mc{A}_{k-1} \circ \mc{A}_k\ldots\mc{A}_{n-1}\rangle\\
\mc{Y} \equiv& \langle \mc{A}_0\ldots\mc{A}_{j-1}\circ\mc{A}_j\ldots\mc{A}_{k-1}\rangle\circ\mc{A}_k\ldots\mc{A}_{n-1}
\end{align}
But, we can see by Theorem \ref{thm:concatassoc} that $\mc{X}\equiv \mc{Y}$.
This implies that $\circ_n(\mc{A}_0,\ldots, \mc{A}_{n-1})$ has a single unique element which we may denote as
\begin{align*}
\mc{A}_0\ldots\mc{A}_{n-1}
\end{align*}

\end{theorem}

\begin{theorem}[Length of Multiple Concatenations]
\label{thm:multconcat}
Suppose $\mc{A} \equiv \mc{A}_0 \ldots \mc{A}_{N-1}$.
We will prove by induction on $N$ that $|\mc{A}| = \sum_{k=0}^{N-1} |\mc{A}|_k$.

Suppose $N=1$.
Then $\mc{A} \equiv \mc{A}_0$ so $|\mc{A}| = |\mc{A}_0|$.

Now suppose the theorem holds for any $m<N$.
Let $m=N-1$.
We can write
$$
\mc{A} \equiv \Braket{\mc{A}_0 \ldots \mc{A}_{m-1}} \circ \mc{A}_{N-1}
$$
Because the term in the angle brackets is a concatenation of $m<N$ strings we have that
$$
|\mc{A}_0\ldots \mc{A}_{m-1}| = \sum_{k=0}^{m-1} |\mc{A}_k|
$$
By the definition of concatenation, the length of $\mc{A}$ must then be
$$
|\mc{A}| = \sum_{k=0}^{N-2} |\mc{A}_k| + |\mc{A}_{N-1}| = \sum_{k=0}^{N-1}|\mc{A}_k|
$$
\end{theorem}

\begin{theorem}[Multiple Concatenation Element Extraction]
\label{thm:multconcatextract}
Suppose $\mc{A} \equiv \mc{A}_0\ldots\mc{A}_{N-1}$.
Now choose $0\le k \le N-1$ and let $L_k = \sum_{j=0}^{k-1} |\mc{A}_j|$, noting that $L_0=0$.
I will prove by induction on $N$ that if $\mc{A}_k$ is non-empty, then for $0 \le i < |\mc{A}_k|$ that $\mc{A}[L_k + i] \equiv \mc{A}_k[i]$.

Suppose $N=1$.
Then $\mc{A} \equiv \mc{A}_0$ and we must choose $k=0$ with $L_0=0$ and $0 \le i < |\mc{A}[0]|$.
In this case $\mc{A}[i]$ and $\mc{A}_0[i]$ are both defined and $\mc{A}[i] \equiv \mc{A}_0[i]$.

As our induction hypothesis we suppose that for $m = N-1 < N$ that, if $\mc{B}\equiv \mc{A}_0 \ldots \mc{A}_{m-1}$ that for $0\le k \le m-1$ if $\mc{A}_k$ is non-empty and $0 \le i < |\mc{A}_k|$ then $\mc{B}[L_k+i] \equiv \mc{A}_k[i]$.

Consider $\mc{A} \equiv \mc{A}_0 \ldots \mc{A}_{N-1}$.
We write $\mc{A}$ as
\begin{align*}
\mc{A} \equiv \Braket{\mc{A}_0\ldots \mc{A}_{m-1}} \circ \mc{A}_{N-1} \equiv \mc{B}\mc{A}_{N-1}
\end{align*}
where I've defined $\mc{B} \equiv \mc{A}_0 \ldots \mc{A}_{m-1}$.
Note that by the Theorem \ref{thm:multconcat} we have $|\mc{B}| = L_m$.
Choose $k$ with $0 \le k \le N-1$ with $\mc{A}_k$ non-empty and choose $i$ with $0 \le i < |\mc{A}_k|$.

If $k=m=N-1$ then we have $0\le i < |\mc{A}_{N-1}|$ which means, by the definition of concatenation, that
$$
\mc{A}[|\mc{B}|+i] \equiv \mc{A}[L_k+i] \equiv \mc{A}_{N-1}[i] \equiv \mc{A}_k[i]
$$

If $k < m$ then $k+1 \le m$ so $L_{k+1} \le L_m = |\mc{B}|$.
Consider $j = L_k + i$.
We have
$$
0\le L_k \le j < L_k + |\mc{A}_k| = L_{k+1} \le |\mc{B}|
$$
This means, by the definition of concatenation, that
$$
\mc{A}[j] \equiv \mc{B}[j] \equiv \mc{B}[L_k + i]
$$
But, the induction hypothesis holds for $\mc{B}$, which means $\mc{B}[L_k+i] \equiv \mc{A}_k[i]$.
This means $\mc{A}[L_k+i] \equiv \mc{A}_k[i]$.

\end{theorem}

\subsubsection{List Slicing and Sublists}
In addition to combining lists to form larger lists, we can also slice lists to form smaller lists.

\begin{definition}[List Slicing]
Suppose $\mc{A}$ is a list.
We define $\mc{A}[i:j]$ recursively on the difference $j-i$. Suppose $0 \le i \le j \le |\mc{A}|$.

\begin{itemize}
\item{If $j-i=0$ then $\mc{A}[i:j] \equiv \mc{E}$.}
\item{If $j-i=1$ then $\mc{A}[i:j] \equiv [\mc{A}[i]]$.\footnote{Note the difference between $[\mc{A}[i]]\in \Sigma^*$, the list containing the object $\mc{A}[i]$ and the object $\mc{A}[i]\in \Sigma$ itself.}}
\item{If $j-i>1$ then $\mc{A}[i:j] \equiv \mc{A}[i:j-1]\circ\mc{A}[j-1:j]$}
\end{itemize}
\end{definition}

\begin{definition}[Sublists and Initial Parts]
Suppose $\mc{A}$ is a list, $0\le i \le j \le |\mc{A}|$ and $\mc{B} \equiv \mc{A}[i:j]$.
\begin{itemize}
\item{We say $\mc{B}$ is a sublist of $\mc{A}$.}
\item{If $i>0$ or $j<|\mc{A}|$ then we say $\mc{B}$ is a proper sublist of $\mc{A}$.}
\item{If $i=0$ then we say $\mc{B}$ is an initial part of $\mc{A}$.}
\item{If $i=0$ and $j < |\mc{A}|$ then we say $\mc{B}$ is a proper initial part of $\mc{A}$.}
Clearly if $\mc{B}$ is a proper initial part of $\mc{A}$ then it is an initial part of $\mc{A}$, if $\mc{B}$ is an initial part of $\mc{A}$ it is a sublist of $\mc{A}$, and if $\mc{B}$ is a proper sublist of $\mc{A}$ then it is a sublist of $\mc{A}$.
\end{itemize}
\end{definition}

\begin{theorem}[The Empty List is an Initial Part of Every List]
\label{thm:emptylistinitialpart}
If $\mc{A}$ is a list then $\mc{A}[0:0] \equiv \mc{E}$ so $\mc{E}$ is an initial part of $\mc{A}$.
\end{theorem}

\begin{theorem}[Length of Sublist]

Suppose $\mc{A}$ is a list and $0 \le i \le j \le |\mc{A}|$. Here we will prove that $|\mc{A}[i:j]| = j-i$ by induction on $j-i$.

\begin{itemize}
\item{If $j-i=0$ then $\mc{A}[i:j] \equiv \mc{E}$ and $|\mc{A}[i:j]|=0 = j-i$.}
\item{If $j-i=1$ then $\mc{A}[i:j] = [\mc{A}[i]]$ so $|\mc{A}[i:j]| = 1 = j-i$.}
\item{Now consider $j-i > 1$.
Suppose that if $j'-i' < j-i$ that $|\mc{A}[i':j']| = j'-i'$.
Consider $\mc{A}[i:j] \equiv \mc{A}[i:j-1] \circ \mc{A}[j-1:j]$.
We have, from the definition of concatenation, that $|\mc{A}[i:j]| = |\mc{A}[i:j-1]| + |\mc{A}[j-1:j]|$.
But, $(j-1) - i < j-i$ and $j - (j-1) = 1 < j-i$ so $|\mc{A}[i:j-1]| + |\mc{A}[j-1:j]| = (j-1)-i + j - (j-1) = j-i$.}
\end{itemize}
\end{theorem}

\begin{theorem}[Sublist Element Extraction]
\label{thm:sublistextraction}
Suppose $\mc{A}$ is non-empty and $0 \le i < j \le |\mc{A}|$ and $0 \le k < j-i$.
We will prove that $\mc{A}[i:j][k] \equiv \mc{A}[k + i]$. We will induct on $j$.

If $j=1$ then $i=k=0$. $\mc{A}[0:1][0] \equiv [\mc{A}[0]][0] \equiv \mc{A}[0]$. Now suppose that $j>1$ but for any $j'<j$ and $0 \le i < j'$ and $0 \le k < j'-i$ that $\mc{A}[i:j'][k]\equiv \mc{A}[k+i]$. Suppose $0 \le i < j \le |\mc{A}|$ and $0 \le k < j-i$ and consider $\mc{A}[i:j][k] \equiv (\mc{A}[i:j-1]\circ\mc{A}[j-1:j])[k]$. Note that $|\mc{A}[i:j-1]| = j-1-i$. If $0 \le k < j-1-i$ then $\mc{A}[i:j][k] \equiv \mc{A}[i:j-1][k]$. Since $j-1<j$ this means $\mc{A}[i:j][k] \equiv \mc{A}[k+i]$. If $k=j-i-1$ then $\mc{A}[i:j][k] \equiv \mc{A}[i:j][j-i-1 + 0] \equiv \mc{A}[j-1:j][0] \equiv [\mc{A}[j-1]][0] \equiv \mc{A}[j-1] = \mc{A}[k+i]$. Therefore $\mc{A}[i:j][k+i] \equiv \mc{A}[k+i]$.
\end{theorem}

\begin{corollary}[A List is a Sublist of Itself]
Taking $i=0$ and $j = |\mc{A}|$ we see that $\mc{A}[0:|\mc{A}|] \equiv \mc{A}$, therefore $\mc{A}$ is a sublist of itself.
\end{corollary}

\begin{theorem}[Mutual Initial Parts are Equal]
Suppose $\mc{A}$ is an initial part of $\mc{A}'$ and $\mc{A}'$ is an initial part of $\mc{A}$.
This means $\mc{A} \equiv \mc{A}'[0:j]$ so $|\mc{A}| = j\le|\mc{A}'|$ and $\mc{A}'\equiv \mc{A}[0:j']$ so $|\mc{A}'| = j'\le |\mc{A}|$.
Since $|\mc{A}|\le |\mc{A}|'$ and $|\mc{A}'| \le |\mc{A}|$ we have $|\mc{A}| = |\mc{A}'| = j = j'$.
So $\mc{A}' \equiv \mc{A}[0:|\mc{A}|] \equiv \mc{A}$.
\end{theorem}

The following theorem will be critical for proving facts about strings within the formal language.

\begin{theorem}[Corresponding Intermediate Parts]
\label{thm:corrintpart}
We will prove that if $\mc{A}\equiv \mc{B}\mc{C}\mc{D} \equiv \mc{B}\mc{C}'\mc{D}'$ with $|\mc{C}| \le |\mc{C}'|$ that either $\mc{C}$ is an initial part of $\mc{C}'$.
We will show that $\mc{C} \equiv \mc{C}'[0:|\mc{C}|]$.

If $\mc{C}\equiv\mc{E}$ then $\mc{C}$ is a sublist of $\mc{C}'$ by Theorem \ref{thm:emptylistinitialpart}.

If $\mc{C}$ is non-empty, consider $0 \le j < |\mc{C}| \le |\mc{C}'|$.
We have $\mc{A} \equiv (\mc{B}\mc{C})\mc{D} \equiv (\mc{B}\mc{C}')\mc{D}'$.
Let $i = j + |\mc{B}|$ so that $0 \le i < |\mc{B}|+|\mc{C}| =|\mc{B}\mc{C}| \le |\mc{B}|+|\mc{C}'| = |\mc{B}\mc{C}'|$ so we have that $\mc{A}[i] \equiv (\mc{B}\mc{C})[i] \equiv (\mc{B}\mc{C}')[i]$.

We rewrite this as $(\mc{B}\mc{C})[|\mc{B}|+j] \equiv (\mc{B}\mc{C}')[|\mc{B}|+j] \equiv \mc{C}[j] \equiv \mc{C}'[j] \equiv \mc{C}'[0:|\mc{C}|][j]$ by Theorem \ref{thm:sublistextraction}.
Since this holds for all $0 \le j < |\mc{C}|$ we have that $\mc{C} \equiv \mc{C}'[0:|\mc{C}|]$.

This means that $\mc{C}$ is an initial part of $\mc{C}'$.
\end{theorem}

\begin{corollary}{Corresponding Intermediate Parts}
\label{corr:intpart}
If $\mc{A} \equiv \mc{B}\mc{C}\mc{D} \equiv \mc{B}\mc{C}'\mc{D}'$ then either $\mc{C}$ is an initial part of $\mc{C}'$ of $\mc{C}'$ is an initial part of $\mc{C}$.
This follows from the fact that either $|\mc{C}| \le |\mc{C}'|$ or $|\mc{C}'| \le |\mc{C}|$ and Theorem \ref{thm:corrintpart}.
\end{corollary}

\subsubsection{Strings from Lists}
Up until now we have been discussing lists of over a general set of objects $\Sigma$.

\begin{definition}[String]
A string is an element of $\textbf{Str} = \textbf{Symb}^*$.
In other words, every string is a finite ordered list of symbols and any finite ordered list of symbols is a string.
\end{definition}

The only distinctive feature about strings compared to general lists is that if we have a list of symbols such as

$$
[x_1, \lnot, Q, \curlywedge, (]
$$

We will typically abbreviate it as

$$
x_1\lnot Q\curlywedge (
$$

dropping the separating commas and enclosing square brackets.
This abbreviation is generally ok.
The main ambiguity in this abbreviation is that when we write down, for example, the symbol $\lnot$ it is not clear if we are talking about the symbol $\lnot$ or the list containing the symbol $[\lnot] \equiv [[0, \lnot]]$.
In practice, this distinction is typically not important so the ambiguity is harmless.
If we say, for example, that $\alpha \in \textbf{Str}$ where $\alpha$ is a symbol we mean $[\alpha] \in \textbf{Str}$.
For strings we replace the terms `sublist' and `proper sublist' with `substring' and `proper substring'.


\subsection{The Syntax of LFOL}

The smallest syntactic elements of \textbf{LFOL} are the terms defined as follows.

\begin{definition}[Terms]

$t \in \textbf{Term}$ iff $t \in \textbf{Str}$ and satisfies one of
\begin{itemize}
\item{\textbf{Variables:} $t\equiv [x]$ with $x \in \textbf{Var}$.}
\item{\textbf{Functions:} $t \equiv ft_1 \ldots t_n$ with $f \in \textbf{Func}_n$ and $t_1, \ldots, t_n \in \textbf{Term}$. Often we will abuse notation and write this as $f(t_1, \ldots, t_n)$ or $f(\bv{t})$. Note that if $f$ is a 0-ary function symbol then $f$ is a term representing a constant.}
\end{itemize}
\end{definition}

Semantically, if the expressions of the formal language are making statements, then terms are the subjects of those statements.
0-ary functions represent constants or concrete objects while variables represent place-holders into which other terms can be `plugged in'.

We define the set of free variables $FV(t)$ of a term $t$ recursively.

\begin{definition}[Free Variables in Terms]
% TODO: Can't define this here before unique readability
If $t\in \textbf{Term}$ then $t$ we define the set $FV(t)$ as follows.
\begin{itemize}
\item{\textbf{Variables:} If $t\equiv [x]$ with $FV = \{x\}$}
\item{\textbf{Functions:} If $t\equiv ft_1\ldots t_n$ then $FV(t) = FV(t_1)\cup\ldots \cup FV(t_n)$.}
\end{itemize}
\end{definition}

If $FV(t)$ is empty then we say that $t$ is closed, otherwise we say that $t$ is open.
Closed terms can be thought of as concrete objects while open terms can be thought of as placeholders into which concrete objects can be `plugged in'.

The next larger syntactic elements in \textbf{LFOL} are the atomic formulas defined as follows.

\begin{definition}[Atomic Formulas]
$\mc{A} \in \textbf{Atom}$ iff $\mc{A} \in \textbf{Str}$ and satisfies

\begin{itemize}
\item{\textbf{Atomic Formulas:} $\mc{A} \equiv Pt_1\ldots t_n$ with $P\in \textbf{Pred}_n$ and $t_1, \ldots, t_n \in \textbf{Term}$. Again we will abuse notation and write this as $P(t_1,t_2,\ldots,t_n)$ or $P(\bv{t})$. In our notation $\curlywedge$ is an atomic formula.}
\end{itemize}
\end{definition}

Semantically, an atomic formula express a boolean statement about the terms within its scope.
If the terms are all closed then we can, semantically, think of the atomic formula as having a truth value.
If any of the terms are open then the atomic formula would only have a truth value once closed terms are `plugged in' for all variables.

The final layer of syntax is the formula, sometimes called a well-formed formula  (Wff).
Formulas combine multiple atomic formulas into more complex expressions using the logical operators.
We will define formulas in two phases.
We will first present \textit{permissive} formulas, then we will present \textit{strict} formulas.
permissive formulas and strict formulas will differ in their treatment of quantifiers.
In short, permissive formulas will allow repeated quantification (the binding of a variable $x$ within the scope of a quantifier already binding over $x$) and redundant quantification (the quantification over a variable $x$ over an expression which does not include $x$).
Both repeated and redundant quantification are in some-sense unnatural, in that they don't translate well into English example sentences, and that the quantification feels either confusing (in the case of repeated quantification) or wasteful (in the case of redundant quantification).

Strict formulas will explicitly forbid both repeated and redundant quantification.
The downside, however, is that it will be much more difficult to prove metalogical theorems about strict formulas.
This is because care will need to be taken at each step to ensure that each expression satisfies the provisos necessary for strict formulas.
It is typically the case that logic references do allow repeated and redundant quantification for precisely this reason.
In fact, one might worry that semantically different theorems could be derived using permissive formulas compared to strict formulas, but this is in fact not the case.

Nonetheless, in this document we will mainly be using strict formulas, with permissive formulas only as a temporary stepping stone to help more easily define strict formulas.
Moving forward `formula' will refer to `strict formula' unless otherwise specified.

\begin{definition}[Permissive Formulas]
$\mc{A} \in \textbf{pForm}$ iff $\mc{A} \in \textbf{Str}$ and satisfies one of

\begin{itemize}
\item{\textbf{Atomic Formulas:} $\mc{A}$ is an atomic formula.}
\item{\textbf{Implication:} $\mc{A}\equiv (\mc{B}\implies \mc{C})$ and $\mc{B}, \mc{C} \in \textbf{pForm}$.}
\item{\textbf{Negation:} $\mc{A}\equiv (\lnot \mc{B})$ and $\mc{B} \in \textbf{pForm}$.}
\item{\textbf{Quantification:} $\mc{A}\equiv ((\forall x)\mc{B})$ and $x\in \textbf{Var}$, $\mc{B}\in \textbf{pForm}$. We say all substrings of $\mc{B}$ (including $\mc{B}$) appear in the scope of $(\forall x)$ in $\mc{A}$.}
\end{itemize}
\end{definition}

We define the free variables, $FV(\mc{A})$ in a formula $\mc{A}$ similarly to the free variables of a term.

\begin{definition}[Free Variables in Permissive Formulas]
% TODO: Can't define this here before unique readability
If $\mc{A}$ is a permissive formula then we define $FV(\mc{A})$ recursively.
\begin{itemize}
\item{if $\mc{A} \equiv Pt_1\ldots t_n$ for $n$-ary predicate $P$ then $FV(\mc{A}) = FV(t_1)\cup\ldots\cup FV(t_n)$. Note that this implies $FV(P)=\emptyset$ if $P$ is a 0-ary predicate such as $\curlywedge$.}
\item{if $\mc{A} \equiv (\mc{B}\implies \mc{C})$ for Wffs $\mc{B}$ and $\mc{C}$ then $FV(\mc{A}) = FV(\mc{B})\cup FV(\mc{C})$.}
\item{if $\mc{A} \equiv (\lnot\mc{B})$ then $FV(\mc{A}) = FV(\mc{B})$.}
\item{if $\mc{A} \equiv ((\forall x)\mc{B})$ then $FV(\mc{A}) = FV(\mc{B}) - \{x\}$.}
\item{If $FV(\mc{A}) = \emptyset$ then we say $\mc{A}$ is closed.}
\end{itemize}
\end{definition}


Semantically, like atomic formulas, formulas express, now more complex, statements about terms.
Note that, $((\forall x) \mc{B})$ is a permissive formula even if $x\not \in FV(\mc{B})$.

\subsection{Unique Readability}

\begin{theorem}[No Proper Initial Part of a Term is a Term]
\label{thm:termnopropinit}
I will prove that if $t\in\textbf{Term}$ then if $t'$ is a proper initial part of $t$ then $t'\not \in \textbf{Term}$. I will induct on $|t|$.

$|t|$ must be greater than zero because if $|t| = 0$ then $t \equiv \mc{E}$ and $\mc{E}\not \in \textbf{Term}$.

If $|t| = 1$ then the only proper initial part of $t$ is $t[0:0] = \mc{E}$ and $\mc{E} \not \in \textbf{Term}$.

Now suppose $|t|>1$.
Suppose, as the induction hypothesis, that for any $s\in\textbf{Term}$ with $|s|<|t|$ that if $s'$ is a proper initial part of $s$ that $s'\not \in \textbf{Term}$.
Suppose, for contradiction, that $t'$ is a proper initial part of $t$ and that $t'$ is a term.
We then have that $t \equiv ft_1\ldots t_n$ and $t' \equiv f't_1'\ldots t_n'$ With $t_1, \ldots t_n, t_1', \ldots t_n' \in \textbf{Term}$.
We must have $t' \equiv t[0:j]$ for some $0 < j < |t|$ so $t'[0] \equiv t[0]$ by Theorem \ref{thm:sublistextraction}.
This means $f' \equiv f$ so that $t' \equiv ft_1'\ldots t_n'$.
By Corollary \ref{corr:intpart} it must be the case that either $t_1'$ is an initial part of $t_1$ or $t_1$ is an initial part of $t_1'$. But, $|t_1|, |t_1'| < |t|$ which means that neither $t_1$ nor $t_1'$ can have a proper initial part that is a term which means that either $t_1$ is a non-proper initial part of $t_1$ or $t_1$ is a non-proper initial part of $t_1'$, in both cases, we get that $t_1\equiv t_1'$.
This argument can be repeated up to $t_n$ showing that $t_k \equiv t_k'$ for $1 \le k \le n$ which means that $t \equiv t'$ contradicting the assumption that $t'$ is a proper initial substring of $t$.
\end{theorem}

\begin{theorem}[Initial Symbols in a Term]
\label{thm:initsymbterm}
Suppose $t$ is a term. If $|t|=1$ then either $t[0] \in \textbf{Var}$ or $t[0] \in \textbf{Func}_0$.
If $|t|>1$ then $t\equiv ft_1\ldots t_n$ with $f\in\textbf{Func}$ and $t_1\ldots t_n\in\textbf{Term}$ so $t[0]\equiv f \in \textbf{Func}$.
This means that for any term $t$ that $t[0] \in \textbf{Var}$ or $t[0]\in\textbf{Func}$.
\end{theorem}

\begin{theorem}[Initial Symbols in a Permissive Formula]
\label{thm:initsymbform}
Suppose $\mc{A}\in \textbf{pForm}$.
We will use Theorem \ref{thm:multconcatextract} throughout.

If $\mc{A}$ is atomic then $\mc{A}[0] \in \textbf{Pred}$.
If $\mc{A}$ is an implication, negation, or quantification, then $\mc{A}[0] \equiv ($. So for every permissive formula $\mc{A}[0]$ is in $\textbf{Pred}$ or equal to $($.

Suppose $|\mc{A}| > 1$.
If $\mc{A}$ is atomic then $\mc{A}\equiv Pt_1\ldots t_n$ with $P\in\textbf{Pred}$ and $t_1\ldots t_n\in\textbf{Term}$.
This means $\mc{A}[1]\equiv t_1[0]$ and, by Theorem \ref{thm:initsymbterm} $t_1[0]\in\textbf{Var}$ or $t_1[0]\in\textbf{Func}$.

If $\mc{A}$ is an implication then $\mc{A}\equiv (\mc{B}\implies \mc{C})$ for $\mc{B},\mc{C}\in\textbf{pForm}$.
We see that $\mc{A}[1]\equiv \mc{B}[0]$ and from above, $\mc{B}[0]$ is either in $\textbf{Pred}$ or equal to $($.

If $\mc{A}$ is a negation then $\mc{A}\equiv (\lnot \mc{B})$ and $\mc{A}[1] \equiv \lnot$.

If $\mc{A}$ is a quantification then $\mc{A}\equiv ((\forall x)\mc{B}0$ and $\mc{A}[1] \equiv ($.

This all means that if $|\mc{A}|>1$ that $\mc{A}[1]$ is either in $\textbf{Var}$, in $\textbf{Func}$, in $\textbf{Pred}$, equal to $($, or equal to $\lnot$.
\end{theorem}

\begin{theorem}[Unique Range of Permissive Formula Construction Rules]
\label{thm:uniquerange}
Suppose $\mc{A}\in\textbf{pForm}$.
We will use Theorem \ref{thm:multconcatextract} throughout.

Suppose $\mc{A}$ is atomic.
Then $\mc{A}[0] \in \textbf{Pred}$.
If $\mc{A}$ were either an implication, negation, or quantification we would have $\mc{A}[0]\equiv (\not\in \textbf{Pred}$ so $\mc{A}$ is not an implication negation or quantification.



Suppose $\mc{A}$ is an implication so that $\mc{A} \equiv (\mc{B}\implies \mc{C})$ with $\mc{B},\mc{C}\in\textbf{pForm}$.
We know from above that $\mc{A}$ is not atomic.

We see that $\mc{A}[1]\equiv \mc{B}[0]$ so by Theorem \ref{thm:initsymbform}, $\mc{A}[1]\equiv\mc{B}[0]$ is in $\textbf{Pred}$ or equal to $($.
If $\mc{A}$ were also a negation then we would have $\mc{A}[1]\equiv \lnot$, so $\mc{A}$ is not a negation.

If $|\mc{B}| = 1$ then $\mc{A}[2]\equiv \implies$.
If $|\mc{B}| > 1$ then $\mc{A}[2] \equiv \mc{B}[1]$ which, by Theorem \ref{thm:initsymbform}, is either in $\textbf{Var}$, in $\textbf{Func}$, in $\textbf{Pred}$, equal to $($, or equal to $\lnot$.
If $\mc{A}$ were a quantification then we would have $\mc{A}[2] \equiv \forall$, so $\mc{A}$ is not a quantification.

Suppose $\mc{A}$ is a negation so that $\mc{A} \equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$.
We know from above that $\mc{A}$ is not atomic and $\mc{A}$ is not an implication.
$\mc{A}[2] \equiv \mc{B}[0]$ which must either be in $\textbf{Pred}$ or equal to $($.
If $\mc{A}$ were a quantification then we would have $\mc{A}[2] \equiv \forall$, so $\mc{A}$ is not a quantification.

If $\mc{A}$ were a quantification then we know from above that it is not atomic, not an implication, and not a negation.

Therefore if $\mc{A}\in\textbf{pForm}$ then it exactly one of atomic, implication, negation, or quantification.
\end{theorem}

\begin{theorem}[Unique Readability of Atomic Formulas]
\label{thm:atomunique}
Suppose $\mc{A}\in \textbf{pForm}$ and $\mc{A}$ is atomic.
Then $\mc{A} \equiv Pt_1\ldots t_n$ with $P\in\textbf{Pred}_n$ and $t_1,\ldots,t_n\in\textbf{Term}$.
We know from \ref{thm:uniquerange} that $\mc{A}$ is not an implication, negation, or quantification.
Suppose that $\mc{A} \equiv P't_1'\ldots t_k'$ with $P'\in\textbf{Pred}_k$ and $t_1',\ldots, t_k'\in\textbf{Term}$.
We see that $\mc{A}[0] \equiv P \equiv P'$ which also means $k=n$.
We then have $\mc{A}\equiv P t_1'\ldots t_k'$.
We can see by Corollary \ref{corr:intpart} that $t_1$ must be an initial part of $t_1'$ or $t_1'$ must be an initial part of $t_1$, but because of Theorem \ref{thm:termnopropinit} no term can have a proper initial part, so we must have $t_1\equiv t_1'$.
This same argument is repeated for $t_2$ through $t_n$ so that $t_i \equiv t_i'$ for all $1 \le i \le n$.
\end{theorem}

\begin{theorem}[No Proper Initial Part of a Permissive Formula is a Permissive Formula]
\label{thm:permpropinit}
Suppose $\mc{A}\in\textbf{pForm}$.
We proceed by induction on $|\mc{A}|$ to show that $\mc{A}$ has no proper initial part that is a permissive formula.

If $|\mc{A}| = 1$ then the only proper initial part of $\mc{A}$ is $\mc{E} \not \in \textbf{pForm}$.

Suppose that $|\mc{A}| = n>1$.
The induction hypothesis is that for any $\mc{B}$ with $|\mc{B}|<n$ that $\mc{B}$ has no proper initial part that is a permissive formula.
We need one case for each type of permissive formula.

Suppose $\mc{A}$ is atomic and suppose $\mc{A}'\in \textbf{pForm}$ and is an initial part of $\mc{A}$.
We have $\mc{A} \equiv Pt_1\ldots t_n$ with $P\in \textbf{Pred}$ and $t_1,\ldots,t_n\in\textbf{Term}$.
We can see that $\mc{A}'[0] \equiv \mc{A}[0] \equiv  P$ so we see that $\mc{A}'$ is atomic.
We have $\mc{A}' \equiv Pt_1'\ldots t_n'$.
By the same argument as in Theorem \ref{thm:atomunique} we conclude that $t_i' \equiv t_i$ for all $1 \le i \le n$.
Therefore $\mc{A}' \equiv \mc{A}$ so that $\mc{A}'$ cannot be a proper initial part of $\mc{A}$.
Note that this case does not require the induction hypothesis.

If $\mc{A}$ is an implication then we have $\mc{A} \equiv (\mc{B} \implies \mc{C})$ with $\mc{B}, \mc{C} \in \textbf{pForm}$.
Suppose, for contradiction, that $\mc{A}'$ is a proper initial part of $\mc{A}$ and $\mc{A}\in\textbf{pForm}$.
If $|\mc{A}'| = 0$ then $\mc{A}' \equiv \mc{E}$ and so $\mc{A}' \not \in \textbf{pForm}$.
If $|\mc{A}'| = 1$ then $\mc{A}' \equiv [(]$ and $\mc{A}' \not\in \textbf{pForm}$.
Suppose $|\mc{A}'|>1$.
Then $\mc{A}'[1] \equiv \mc{A}[1] \equiv \mc{B}[0]$ which is either in $\textbf{Pred}$ or equal to $($. This means that $\mc{A}'$ is not a negation.
If $|\mc{A}'| < 3$ then $\mc{A}'$ is not a quantification.
If $|\mc{A}'| \ge 3$ then $\mc{A}'[2] \equiv \mc{A}[2] \equiv \mc{B}[1]$ which is either in $\textbf{Var}$, $\textbf{Func}$, or $\textbf{Pred}$ or equal to $($ or $\lnot$. But if $\mc{A}'$ were a quantification then we would have $\mc{A}'[2] \equiv \forall$, so $\mc{A}'$ is not a quantification.
So $\mc{A}'$ is an implication with $\mc{A}' \equiv (\mc{B}' \implies \mc{C}')$ with $\mc{B}', \mc{C}' \in \textbf{pForm}$.
We can see by Corollary \ref{corr:intpart} that $\mc{B}$ must be an initial part of $\mc{B'}$ or $\mc{B}'$ must be an initial part of $\mc{B}$.
But $|\mc{B}|$ and $|\mc{B}'|$ are less than $n$, so, by the induction hypothesis, neither can have a proper initial part that is also a permissive formula.
Therefore $\mc{B} \equiv \mc{B}'$ and $\mc{A}' \equiv (\mc{B} \implies \mc{C}')$.
By the same argument we find $\mc{C} \equiv \mc{C}'$.
This means $\mc{A} \equiv \mc{A}'$, contradicting the assumption that $\mc{A}'$ is a proper initial part of $\mc{A}$.

If $\mc{A}$ is a negation then $\mc{A} \equiv (\lnot \mc{B})$ with $\mc{B} \in \textbf{pForm}$.
Suppose that $\mc{A}'$ is a proper initial part of $\mc{A}$ and $\mc{A}'\in\textbf{pForm}$.
Since $\mc{A}'[0] \equiv \mc{A}[0] \equiv ($ we know that $\mc{A}'$ is not atomic.
If $\mc{A}'$ were an implication then we would have $\mc{A}' \equiv (\mc{B}' \implies \mc{C}')$ with $\mc{B}', \mc{C}' \in \textbf{pForm}$.
In this case we would have $\mc{A}[1] \equiv \mc{B}[0]$ which can be either in $\textbf{Pred}$ or equal to $($, but $\mc{A}[1] \equiv \lnot$ so $\mc{A}'$ is not an implication.
If $\mc{A}'$ were a quantification we would have $\mc{A}' \equiv ((\forall x) \mc{B}')$ so that $\mc{A}'[2] \equiv \forall$. But $\mc{A}'[2] \equiv \mc{B}[0]$ which is either in $\textbf{Pred}$ or equal to $($ so $\mc{A}'$ is not a quantification.
$\mc{A}'$ must be a negation so that $\mc{A}' \equiv (\lnot \mc{B}')$ with $\mc{B}' \in \textbf{pForm}$.
But, by Corollary \ref{corr:intpart} we have $\mc{B}$ must be an initial part of $\mc{B}'$ or $\mc{B}'$ must be an initial part of $\mc{B}$.
But, $|\mc{B}|$ and $|\mc{B}'|$ are both less than $n$, so by the induction hypothesis, neither can have a proper initial part that is also a permissive formula so we must have $\mc{B}' \equiv \mc{B}$.
This means $\mc{A}' \equiv \mc{A}$, contradicting the assumption that $\mc{A}'$ is a proper initial part of $\mc{A}$.

If $\mc{A}$ is a quantification then $\mc{A} \equiv ((\forall x) \mc{B})$ with $x \in \textbf{Var}$ and $\mc{B} \in \textbf{pForm}$.
Suppose $\mc{A}'$ is a proper initial part of $\mc{A}$ and $\mc{A}' \in \textbf{pForm}$.
$\mc{A}'[0] \equiv \mc{A}[0] \equiv ($ so $\mc{A}'$ is not atomic.
Also, $\mc{A}'[1] \equiv \mc{A}[1] \equiv \lnot$ so $\mc{A}'$ is not a negation.
If $\mc{A}'$ were an implication then we would have $\mc{A}' \equiv (\mc{B}' \implies \mc{C}')$ with $\mc{B}', \mc{C}' \in \textbf{pForm}$.
$\mc{A}'[2]$ would be $\mc{B}'[1]$ which is either in $\textbf{Var}$, $\textbf{Func}$, $\textbf{Pred}$, or equal to $($ or $\lnot$, but $\mc{A}'[2] \equiv \mc{A}[2] \equiv \forall$, so $\mc{A}'$ is not an implication.
$\mc{A}'$ must be a quantification with $\mc{A}' \equiv ((\forall x') \mc{B}')$ with $x' \in \textbf{Var}$ and $\mc{B}' \in \textbf{pForm}$.
$\mc{A}'[3] \equiv x' \equiv \mc{A}[3] \equiv x$ so $x' \equiv x$.
We can then see, by an application of Corollary \ref{corr:intpart}, that $\mc{B}$ must be an initial part of $\mc{B}'$ or $\mc{B}'$ must be an initial part of $\mc{B}$.
But since $|\mc{B}|$ and $|\mc{B}'| $ are less than $n$ neither can have a proper initial part that is also a permissive formula, so we must have $\mc{B} \equiv \mc{B}'$.
This means $\mc{A}' \equiv \mc{A}$ contradicting the assumption that $\mc{A}'$ is a proper initial part of $\mc{A}$.
\end{theorem}

\begin{theorem}[Unique Readability of Permissive Formulas]
\label{thm:uniqueread}
If $\mc{A}$ is atomic with $\mc{A}\equiv Pt_1\ldots t_n$ with $P\in \textbf{Pred}$ and $t_1, \ldots, t_n \in \textbf{Term}$ then we know from Theorem \ref{thm:uniquerange} that $\mc{A}$ is not an implication, negation, or quantification.
We also know from Theorem \ref{thm:atomunique} that if $\mc{A} \equiv P't_1'\ldots t_n'$ with $P'\in\textbf{Pred}$ and $t_1',\ldots, t_n'\in\textbf{Term}$ that $P' \equiv P$ and $t_i' \equiv t_i$ for all $1 \le i \le n$.

Suppose $\mc{A}$ is an implication with $\mc{A} \equiv (\mc{B} \implies \mc{C})$.
We know from Theorem \ref{thm:uniquerange} that $\mc{A}$ is not atomic, a negation, or a quantification.
Suppose $\mc{A} \equiv (\mc{B}'\implies \mc{C}')$.
Corollary \ref{corr:intpart} implies that $\mc{B}'$ is an initial part of $\mc{B}$ or $\mc{B}$ is an initial part of $\mc{B}'$, but, by Theorem \ref{thm:permpropinit}, no permissive formula can have a proper initial part that is a permissive formula so we must have $\mc{B} \equiv \mc{B}'$.
The same argument follows to conclude $\mc{C} \equiv \mc{C}'$.

Suppose $\mc{A}$ is a negation with $\mc{A} \equiv (\lnot \mc{B})$.
We know from Theorem \ref{thm:uniquerange} that $\mc{A}$ is not atomic, an implication, or a quantification.
Suppose $\mc{A} \equiv (\lnot \mc{B}')$.
Corollary \ref{corr:intpart} implies that $\mc{B}'$ is an initial part of $\mc{B}$ or $\mc{B}$ is an initial part of $\mc{B}'$, but, by Theorem \ref{thm:permpropinit}, no permissive formula can have a proper initial part that is a permissive formula so we must have $\mc{B} \equiv \mc{B}'$.

Suppose $\mc{A}$ is a quantification with $\mc{A} \equiv ((\forall x) \mc{B})$ with $x \in \textbf{Var}$ and $\mc{B}\in\textbf{pForm}$.
We know from \ref{thm:uniquerange} that $\mc{A}$ is not atomic, an implication, or a negation.
Suppose $\mc{A} \equiv ((\forall x') \mc{B}')$.
Since $\mc{A}[3] \equiv x' \equiv x$ we have $x' \equiv x$.
Corollary \ref{corr:intpart} implies that $\mc{B}'$ is an initial part of $\mc{B}$ or $\mc{B}$ is an initial part of $\mc{B}'$, but, by Theorem \ref{thm:permpropinit}, no permissive formula can have a proper initial part that is a permissive formula so we must have $\mc{B} \equiv \mc{B}'$.
\end{theorem}

The unique readability of permissive formulas allows to define recursively define functions on permissive formulas as we demonstrate below.

\begin{definition}[Depth of a Permissive Formula]
If $\mc{A}\in\textbf{pForm}$ then $D(\mc{A})$ is the formula depth (or depth) of $\mc{A}$ and is defined recursively.
\begin{itemize}
\item{If $\mc{A}$ is atomic then $D(\mc{A}) = 0$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ with $\mc{B}, \mc{C} \in \textbf{pForm}$ then $D(\mc{A}) = \text{max}(D(\mc{B}), D(\mc{C})) + 1$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$ then $D(\mc{A}) = D(\mc{B}) + 1$.}
\item{If $\mc{A} \equiv ((\forall x) \mc{B})$ with $x\in\textbf{Var}$ and $\mc{B} \in \textbf{pForm}$ then $D(\mc{A}) = D(\mc{B}) + 1$.}
\end{itemize}
\end{definition}

Note that it would not have been possible to define $D(\mc{A})$ without Theorem \ref{thm:uniqueread} and the other theorems of this section.
For example, suppose we have $\mc{A} \equiv (\lnot \mc{B})$.
However, suppose it is also the case that $\mc{A} \equiv (\lnot \mc{B}')$.
The function $D(\mc{A})$ would not be well defined unless we had a guarantee that $D(\mc{B}) = D(\mc{B}')$.
For a more extreme example suppose $\mc{A} \equiv (\lnot \mc{B})$ and $\mc{A} \equiv (\mc{C} \implies \mc{D})$.
We would then need a guarantee that $D(\mc{B}) = \text{max}(D(\mc{C}, \mc{D}))$.
Unique readability ensures that there is only one way to parse, or decompose, a permissive formula thus allowing us to define functions on the parsing of the permissive formula.

\subsection{Free and Bound Variable Identification}

In this section we will define the free and bound variables in a permissive formula $\mc{A}$.


\begin{definition}[Free Variables in Terms]
If $t\in \textbf{Term}$ then $t$ we define the set $FV(t)$ as follows.
\begin{itemize}
\item{\textbf{Variables:} If $t\equiv x$ with $x\in\textbf{Var}$ then $FV = \{x\}$}
\item{\textbf{Functions:} If $t\equiv ft_1\ldots t_n$ with $f\in\textbf{Func}_n$ and $t_1,\ldots,t_n \in \textbf{Term}$ then $FV(t) = FV(t_1)\cup\ldots \cup FV(t_n)$.}
\item{If $x\in \textbf{Var}$ then if $x\in FV(t)$ we say $x$ appears free in $t$. Otherwise we say $x$ does not appear free in $t$.}
\item{if $FV(t)=\emptyset$ then we say $t$ is closed. Otherwise we say $t$ is open.}
\end{itemize}
\end{definition}

Semantically closed terms are concrete objects about which statements can be made while open terms contain variables into which other terms can be plugged into to eventually realize a particular closed object and concrete object.

\begin{definition}[Free Variables in Permissive Formulas]
If $\mc{A}\in\textbf{pForm}$ then we define $FV(\mc{A})$ recursively.
\begin{itemize}
\item{If $\mc{A} \equiv Pt_1\ldots t_n$ for $n$-ary predicate $P$ then $FV(\mc{A}) = FV(t_1)\cup\ldots\cup FV(t_n)$. Note that this implies $FV(P)=\emptyset$ if $P$ is a 0-ary predicate such as $\curlywedge$.}
\item{If $\mc{A} \equiv (\mc{B}\implies \mc{C})$ for Wffs $\mc{B}$ and $\mc{C}$ then $FV(\mc{A}) = FV(\mc{B})\cup FV(\mc{C})$.}
\item{If $\mc{A} \equiv (\lnot\mc{B})$ then $FV(\mc{A}) = FV(\mc{B})$.}
\item{If $\mc{A} \equiv ((\forall x)\mc{B})$ then $FV(\mc{A}) = FV(\mc{B}) - \{x\}$.}
\item{If $x \in \textbf{Var}$ and $x\in FV(\mc{A})$ then we say $x$ appears free in $\mc{A}$. Otherwise we say $x$ does not appear free in $\mc{A}$.}
\item{If $FV(\mc{A}) = \emptyset$ then we say $\mc{A}$ is closed. Otherwise we say $\mc{A}$ is open.}
\end{itemize}
\end{definition}

Note that $((\forall x) \mc{B})$ is a permissive formula even if $x\not \in FV(\mc{B})$.
In such cases $\mc{A}$ exhibits redundant quantification.
For example $((\forall x) Py)$ is a permissive formula.

Semantically closed formulas are concrete statements which can have a truth value whereas open formulas contain variables into which terms can be plugged in to eventually realize a closed formula and concrete statement.

\begin{definition}[Quantified Variables in Permissive Formulas]
If $\mc{A}$ is a permissive formula then we define $QV(\mc{A})$ recursively.

\begin{itemize}
\item{If $\mc{A}$ is an atomic formula then $QV(\mc{A}) = \emptyset$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$ then $QV(\mc{A}) \equiv QV(\mc{B})$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ with $\mc{B}, \mc{C}\in\textbf{pForm}$ then $QV(\mc{A}) = QV(\mc{B}) \cup QV(\mc{C})$.}
\item{If $\mc{A} \equiv ((\forall x) \mc{B})$ with $x\in\textbf{Var}$ and $\mc{B}\in\textbf{pForm}$ then $QV(\mc{A}) = QV(\mc{B}) \cup \{x\}$.}
\end{itemize}
\end{definition}

Note that $((\forall x)\mc{B})$ is a permissive formula even if $x\in QV(\mc{B})$.
In such cases $\mc{A}$ exhibits repeated quantification.

Note also that it is possible to have $x\in FV(\mc{A})$ and $x\in QV(\mc{A})$.
For example
\begin{align}
\mc{A} \equiv (Px \implies ((\forall x) Qy))
\end{align}
We have $FV(\mc{A}) = \{x, y\}$ and $QV(\mc{A}) = \{x\}$.

\subsection{Strict Formulas}

We are now in a position to define strict formulas.
\begin{definition}[Strict Formulas]
$\mc{A}\in\textbf{sForm} = \textbf{Form}$ iff $\mc{A} \in \textbf{pForm}$ and satisfies one of
\begin{itemize}
\item{\textbf{Atomic Formulas}: $\mc{A}$ is an atomic formula.}
\item{\textbf{Implication}: $\mc{A} \equiv (\mc{B} \implies \mc{C})$ and $\mc{B}, \mc{C} \in \textbf{Form}$.}
\item{\textbf{Negation}: $\mc{A} \equiv (\lnot \mc{B})$ and $\mc{B}\in\textbf{Form}$.}
\item{\textbf{Quantification}: $\mc{A} \equiv ((\forall x)\mc{B})$ with $x\in\textbf{Var}$, $\mc{B}\in\textbf{Form}$, and $x\in FV(\mc{B})$ and $x\not\in QV(\mc{B})$.}
\end{itemize}
\end{definition}


\subsection{Free Variable Substitution}

\begin{definition}[Free Variable Substitution]
We define recursively the concept of free variable substitution.
Suppose $s, t \in \textbf{Term}$ and $x\in \textbf{Var}$.
\begin{itemize}
\item{If $s\in\textbf{Var}$ then if $s\equiv x$ we define $s[t/x] \equiv t$. If $s\not \equiv x$ then $s[t/x] \equiv x$.}
\item{If $s \equiv fs_1\ldots s_n$ with $f\in\textbf{Func}_n$ and $s_1, \ldots s_n \in \textbf{Term}$ then $s[t/x] \equiv fs_1[t/x]\ldots s_n[t/x]$. This definition implies that if $f\in \textbf{Func}_0$ and $s\equiv f$ then $s[t/x] \equiv s \equiv f$.}
\end{itemize}

Suppose $\mc{A} \in \textbf{pForm}$.
\begin{itemize}
\item{If $\mc{A}$ is atomic then $\mc{A} \equiv Ps_1\ldots s_n$ with $P\in\textbf{Pred}_n$ and $s_1\ldots s_n\in\textbf{Term}$. We define $\mc{A}[t/x] \equiv Ps_1[t/x]\ldots s_n[t/x]$. This definition implies that if $P\in\textbf{Pred}_0$ and $\mc{A}\equiv P$ then $\mc{A}[t/x] \equiv \mc{A} \equiv P$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ with $\mc{B}, \mc{C}\in\textbf{pForm}$ then $\mc{A}[t/x] \equiv (\mc{B}[t/x] \implies \mc{C}[t/x])$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$ then $\mc{A}[t/x] \equiv (\lnot \mc{B}[t/x])$.}
\item{If $\mc{A} \equiv ((\forall y) \mc{B})$ with $x\in\textbf{Var}$ and $\mc{B}\in\textbf{pForm}$ then if $x\not \equiv y$ then $\mc{A}[t/x] \equiv ((\forall y) \mc{B}[t/x])$. If $x \equiv y$ then $\mc{A}[t/x] \equiv \mc{A}$.}
\end{itemize}

\end{definition}

\begin{theorem}[Free Variable Substitution Within a Term Results in a Term]
\label{thm:fvsubintermgivesterm}
Suppose $s, t\in\textbf{Term}$ and $x\in\textbf{Var}$.
We prove by induction on $|s|$ that $s[t/x] \in \textbf{Term}$.

if $|s| = 1$ then there are two cases.
First suppose $s\in \textbf{Var}$.
If $s\in \textbf{Var}$ then if $s\equiv x$ $s[t/x] \equiv t$ so $s[t/x]\in\textbf{Term}$.
If $s \not \equiv x$ then $s[t/x] \equiv s$ so again $s[t/x] \in \textbf{Term}$.
The other case is $s\equiv f$ with $f\in \textbf{Func}_0$.
In this case $s[t/x] \equiv s$ so $s[t/x] \in \textbf{Term}$.

Now suppose $|s| > 1$.
The induction hypothesis is that for any term $r$ with $|r|<|s|$ we have $r[t/x] \in \textbf{Term}$.
We have $s\equiv ft_1\ldots t_n$ with $f\in\textbf{Func}_n$. $s[t/x] \equiv f t_1[t/x]\ldots t_n[t/x]$, but each of $t_i[t/x]\in\textbf{Term}$ for $1\le i \le n$ so $s[t/x]\in\textbf{Term}$.
\end{theorem}

\begin{theorem}[Free Variable Substitution in Term for Variable the Does Not Appear Free]
\label{thm:fvsubtermvarnotappear}
Suppose $s, t \in \textbf{Term}$ and $x \in \textbf{Var}$. Suppose that $x\not \in FV(s)$. Then $s[t/x] \equiv s$.

If $|s| = 1$ then there are two cases. First suppose $s\in \textbf{Var}$. Since $x\not \in FV(s)$ we must have $s \equiv y \not \equiv x$ so $s[t/x] \equiv s$. The other case is that $s\in \textbf{Func}_0$ in which case $s[t/x]\equiv s$.

Now suppose $|s|>1$ The induction hypothesis is that for any term $r$ with $|r| < |s|$ that if $x \not \in FV(r)$ that $r[t/x] \equiv r$. We have that $s \equiv fr_1\ldots r_n$ with $f$ in $\textbf{Func}_n$. We have that $FV(s) = FV(r_1)\cup\ldots\cup FV(r_n)$. Since $x\not \in FV(s)$ it must be the case that $x \not \in FV(r_i)$ for $1\le i \le n$. Now $s[t/x] \equiv fr_1[t/x]\ldots r_n[t/x]$ but since $|r_i|<|s|$ and $x\not \in FV(r_i)$ we have that $r_i[t/x] \equiv r_i$ for each $1\le i \le n$ so that $s[t/x] \equiv fr_1\ldots r_n \equiv s$.
\end{theorem}

\begin{theorem}[Free Variable Substitution Within a Permissive Formula Results in a Permissive Formula]
Suppose $\mc{A}\in\textbf{pForm}$, $x\in\textbf{Var}$ and $t \in \textbf{Term}$.
We prove, by induction of $D(\mc{A})$, that $\mc{A}[t/x] \in \textbf{pForm}$.

Suppose $D(\mc{A}) = 0$.
Then $\mc{A}$ is atomic so $\mc{A}\equiv Pt_1\ldots t_n$ with $P\in\textbf{Pred}$ and $t_1,\ldots, t_n\in\textbf{Term}$. $\mc{A}[t/x] \equiv Pt_1[t/x]\ldots t_n[t/x]$, but, by the previous theorem, $t_1[t/x],\ldots,t_n[t/x]\in\textbf{Term}$ so $\mc{A}[t/x]$ is atomic so $\mc{A}[t/x]\in\textbf{pForm}$.

Now suppose that $D(\mc{A}) > 0$ but that for any $\mc{B}$ with $D(\mc{B}) < D(\mc{A})$ we have that $\mc{B}[t/x] \in \textbf{pForm}$.

$\mc{A}$ cannot be atomic since $D(\mc{A})>0$.

Suppose $\mc{A} \equiv (\mc{B}\implies \mc{C})$ with $\mc{B}, \mc{C}\in\textbf{pForm}$.
Then $\mc{A}[t/x] \equiv (\mc{B}[t/x] \implies \mc{C}[t/x])$.
Since $D(\mc{A}) = \text{max}(D(\mc{B}) + D(\mc{C}))$ we have $D(\mc{B}), D(\mc{C}) < D(\mc{A})$ so, by the induction hypothesis, $\mc{B}[t/x], \mc{C}[t/x]\in\textbf{pForm}$ which means $\mc{A}[t/x]\in\textbf{pForm}$.

Suppose $\mc{A} \equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$.
Then $\mc{A}[t/x] \equiv (\mc{B}[t/x])$.
$D(\mc{B}) = D(\mc{A}) -1 < D(\mc{A})$ so, by the induction hypothesis, $\mc{B}[t/x]\in\textbf{pForm}$ which means $\mc{A}[t/x]\in\textbf{pForm}$.

Suppose $\mc{A} \equiv ((\forall y) \mc{B})$ with $y\in\textbf{Var}$ and $\mc{B}\in\textbf{pForm}$.
If $x\equiv y$ then $\mc{A}[t/x] \equiv \mc{A}$ so $\mc{A}[t/x]\in\textbf{pForm}$.
If $x\not\equiv y$ then $\mc{A}[t/x] \equiv ((\forall y) \mc{B}[t/x])$.
$D(\mc{B}) = D(\mc{A})-1$ so $\mc{B}[t/x]\in\textbf{pForm}$ which means $\mc{A}[t/x]\in\textbf{pForm}$.

\end{theorem}

\begin{theorem}[Free Variable Substitution in a Permissive Formula for a Variable that Does Not Appear Free]
\label{thm:fvsubpformnotappear}
Suppose $t\in \textbf{Term}$, $x\in \textbf{Var}$ and $\mc{A} \in \textbf{pForm}$. If $x\not \in FV(\mc{A})$ then $\mc{A}[t/x] \equiv \mc{A}$.

Suppose $D(\mc{A}) = 0$. Then $\mc{A}$ is atomic so $\mc{A} \equiv P t_1\ldots t_n$. $FV(\mc{A}) \equiv FV(t_1)\cup\ldots\cup FV(t_n)$. Since $x\not \in FV(\mc{A})$ we must have $x\not \in FV(t_i)$ for $1\le i \le n$. $\mc{A}[t/x] \equiv Pt_1[t/x]\ldots t_n[t/x]$. But since $x\not \in FV(t_i)$ we have, by Theorem \ref{thm:fvsubtermvarnotappear}, that $t_i[t/x] \equiv t_i$ so that $\mc{A}[t/x] \equiv Pt_1\ldots t_n \equiv \mc{A}$.

Now suppose that $D(\mc{A}) > 0$ but that for any $\mc{B}\in\textbf{pForm}$ with $D(\mc{B}) < D(\mc{A})$ that if $x \not \in FV(\mc{B})$ that $\mc{B}[t/x] \equiv \mc{B}$.

$\mc{A}$ cannot be atomic because $D(\mc{A})>0$.

Suppose $\mc{A}$ is an implication with $\mc{A} \equiv (\mc{B} \implies \mc{C})$ with $\mc{B}, \mc{C}\in\textbf{pForm}$.
Since $x\not \in FV(\mc{A})$ we must have $x\not \in FV(\mc{B})$ and $x \not \in FV(\mc{C})$.
We also have $D(\mc{B}), D(\mc{C}) < D(\mc{A})$.
We have $\mc{A}[t/x] \equiv (\mc{B}[t/x] \implies \mc{C}[t/x])$ but, by the induction hypothesis, we have that $\mc{B}[t/x] \equiv \mc{B}$ and $\mc{C}[t/x] \equiv \mc{C}$ so $\mc{A}[t/x] \equiv \mc{A}$.

Suppose $\mc{A}$ is a negation with $\mc{A} \equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$.
Since $x\not \in FV(\mc{A})$ we must have $x\not \in FV(\mc{B})$.
We also have $D(\mc{B}) < D(\mc{A})$.
We have $\mc{A}[t/x] \equiv (\lnot \mc{B}[t/x])$ but, by the induction hypothesis, we have $\mc{B}[t/x] \equiv \mc{B}$ so that $\mc{A}[t/x] \equiv \mc{A}$.

Suppose $\mc{A}$ is a quantification with $\mc{A} \equiv ((\forall y)\mc{B})$ with $y\in\textbf{Var}$ and $\mc{B}\in\textbf{pForm}$.
We have the $D(\mc{B}) < D(\mc{A})$.
If $x\equiv y$ then $\mc{A}[t/x] \equiv \mc{A}$.
We have that $FV(\mc{A}) = FV(\mc{B})\setminus\{y\}$.
If $x \not \equiv y$ then $x\not \in FV(\mc{A})$ implies that $x\not \in FV(\mc{B})$.
In this case $\mc{A}[t/x] \equiv ((\forall y) \mc{B}[t/x])$ but the induction hypothesis gives us that $\mc{B}[t/x] \equiv \mc{B}$ so $\mc{A}[t/x] \equiv \mc{A}$.
\end{theorem}

\begin{theorem}[Free Variables in a Term after Free Variable Substitution]
\label{thm:fvtermafterfvsub}
Suppose $s, t \in \textbf{Term}$ and $x\in\textbf{Var}$. If $x \not \in FV(s)$ then $s[t/x] \equiv s$ so $FV(s[t/x]) = FV(s)$. If $x\in FV(s)$ then $FV(s[t/x]) = (FV(s)\setminus \{x\})\cup FV(t)$. We prove this by induction.

Suppose $|s|=1$. Since $x\in FV(s)$ this implies that $s \equiv x$ so that $FV(s)=\{x\}$. Then $s[t/x] \equiv t$ so $FV(s[t/x]) = FV(t)$ but $(FV(s)\setminus\{x\})\cup FV(t) = FV(t)$ as needed.

Now suppose that $|s|>1$ and for $r\in \textbf{Term}$ with $|r|<|s|$ that if $x\in FV(r)$ that $FV(r[t/x]) = (FV(r)\setminus \{x\})\cup FV(t)$. We have that $s\equiv fr_1\ldots r_n$ and $FV(s) = FV(r_1)\cup\ldots\cup FV(r_n)$.

Since $x\in FV(s)$ by assumption then it must be in at least one of the $r_i$. Suppose that there are $1\le j \le n$ values of $i$ for which $x \in FV(r_i)$ and $k = n-j$ values of $i$ for which $x \not \in FV(r_i)$.

Suppose that $x\in FV(r_i)$ for $1\le j \le n$ values of $i$ and $x \not \in FV(r_i)$ for $k = n-j$ values of $i$. Let $a_i$ for $1 \le i \le j$ enumerate the values for which $x\in FV(r_{a_i})$ and let $b_i$ for $1 \le i \le n-j$ enumerate the values for which $x \not \in FV(r_{b_i})$.

We then have $s[t/x] \equiv fr_1[t/x]\ldots r_n[t/x]$ and we can write
$$
FV(s[t/x]) = FV(r_{a_1}[t/x])\cup\ldots\cup FV(r_{a_j}[t/x]) \cup FV(r_{b_1}[t/x])\cup \ldots \cup FV(r_{b_k}[t/x])
$$

By the induction hypothesis we have $FV(r_{a_i}[t/x]) = (FV(r_{a_i})\setminus \{x\}) \cup FV(t)$ and $FV(r_{b_i}[t/x]) = FV(r_{b_i})$ so

\begin{align}
FV(s[t/x]) = \big(&((FV(r_{a_1})\setminus \{x\}) \cup FV(t)) \cup \ldots \cup ((FV(r_{a_j})\setminus \{x\}) \cup FV(t)) \nonumber\\
&\cup FV(r_{b_1}) \cup\ldots \cup FV(r_{b_k})\big) \nonumber
\end{align}

Using some set identities and the fact that $x \not \in FV(r_{b_i})$ we can rewrite this as

\begin{align}
FV(s[t/x]) &= \left(\left(FV(r_{a_1}) \cup \ldots \cup FV(r_{a_j}) \cup FV(r_{b_1}) \cup \ldots \cup FV(r_{b_{n-j}})\right) \setminus \{x\}\right) \cup FV(t) \nonumber\\
&= \left(\left(FV(r_1)\cup \ldots \cup FV(r_n)\right)\setminus \{x\}\right)\cup FV(t) \nonumber\\
&= (FV(s)\setminus \{x\})\cup FV(t) \nonumber
\end{align}
\end{theorem}

\begin{theorem}[Free Variables in a Permissive Formula after Free Variable Substitution]
Suppose $\mc{A} \in \textbf{pForm}$, $x \in \textbf{Var}$ and $t\in\textbf{Term}$. If $x\not \in \mc{A}$ then $\mc{A}[t/x]\equiv \mc{A}$ according to Theorem \ref{thm:fvsubpformnotappear} so $FV(\mc{A}[t/x]) =  FV(\mc{A})$. If $x\in FV(\mc{A})$ then $FV(\mc{A}[t/x]) = (FV(\mc{A})\setminus \{x\})\cup FV(t)$.

Suppose $\mc{A}$ is atomic so that $\mc{A} \equiv Pt_1\ldots t_n$. $FV(\mc{A}) = FV(t_1)\cup\ldots \cup FV(t_n)$. Since $x\in FV(\mc{A})$ then $x$ must be in at least one $t_i$ for $1 \le i \le n$. Suppose that there are $1\le j \le n$ values of $i$ for which $x\in FV(t_i)$ and $k=n-j$ values of $i$ fo which $x\not \in FV(t_i)$. Let $a_i$ for $1\le i \le j$ enumerate the values which $x\in FV(t_{a_i})$ and let $b_i$ for $1 \le i \le n-j$ enumerate the values for which $x\not \in FV(t_{b_i})$.

We have $\mc{A}[t/x] \equiv Pt_1[t/x]\ldots t_n[t/x]$ so

\begin{align}
FV(\mc{A}[t/x]) &= FV(t_1[t/x])\cup\ldots \cup FV(t_n[t/x]) \nonumber\\
&= FV(t_{a_1}[t/x])\cup\ldots\cup FV(t_{a_j}[t/x]) \cup FV(t_{b_1}[t/x]) \cup \ldots \cup FV(t_{b_k}[t/x]) \nonumber\\
\end{align}

According to Theorem \ref{thm:fvtermafterfvsub}, $FV(t_{a_i}[t/x]) = (FV(t_{a_i})\setminus\{x\})\cup FV(t)$ and $FV(t_{b_i}[t/x]) = FV(t_{b_i})$ so

\begin{align}
FV(\mc{A}[t/x]) =& \begin{aligned}[t]\big( &((FV(t_{a_1})\setminus\{x\})\cup FV(t)) \cup \ldots \cup ((FV(t_{a_j})\setminus\{x\})\cup FV(t)) \\
& \cup FV(t_{b_1}) \ldots \cup FV(t_{b_k}) \big)\end{aligned} \notag \\
=& \left(\left(FV(t_{a_1})\cup \ldots \cup FV(t_{a_j}) \cup FV(t_{b_1}) \cup \ldots \cup FV(t_{b_k})\right)\setminus \{x\} \right) \cup FV(t) \notag\\
=& \left(\left(FV(t_1)\cup \ldots \cup FV(t_n)\right)\setminus\{x\}\right)\cup FV(t) \notag\\
=& (FV(\mc{A})\setminus\{x\})\cup FV(t) \notag
\end{align}

\end{theorem}

\begin{theorem}[Quantified Variables after Free Variable Substitution]
Suppose $\mc{A} \in \textbf{pForm}$, $x\in\textbf{Var}$ and $t\in \textbf{Term}$. Then $QV(\mc{A}[t/x]) = QV(\mc{A})$.

Suppose $D(\mc{A}) = 0$. Then $\mc{A}$ is an atomic formula so $QV(\mc{A}) = \emptyset$. But $\mc{A}[t/x]$ is also an atomic formula so $QV(\mc{A}[t/x]) = \emptyset = QV(\mc{A})$.

Now assume that $D(\mc{A})>0$ and for $\mc{B}\in \textbf{pForm}$ that if $D(\mc{B})<D(\mc{A})$ that $QV(\mc{B}[t/x]) = QV(\mc{B})$.

If $\mc{A}$ is a negation then $\mc{A}\equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$ and $D(\mc{B}) < D(\mc{A})$. We have that $QV(\mc{A}[t/x]) = QV((\lnot \mc{B}[t/x])) = QV(\mc{B}[t/x]) = QV(\mc{B}) =  QV(\mc{A})$.

If $\mc{A}$ is an implication then $\mc{A} \equiv (\mc{B} \implies \mc{C})$ with $\mc{B}, \mc{C} \in \textbf{pForm}$ and $D(\mc{B}), D(\mc{C}) < D(\mc{A})$. We have
\begin{align}
QV(\mc{A}[t/x]) &= QV((\mc{B}[t/x]\implies \mc{C}[t/x])) = QV(\mc{B}[t/x]) \cup QV(\mc{C}[t/x]) \notag\\
&= QV(\mc{B}) \cup QV(\mc{C}) = QV(\mc{A}) \notag
\end{align}

If $\mc{A}$ is a quantification then $\mc{A} \equiv ((\forall y) \mc{B})$ with $y\in \textbf{Var}$, $\mc{B}\in\textbf{pForm}$, and $D(\mc{B}) < D(\mc{A})$. If $y\equiv x$ then $\mc{A}[t/x] \equiv \mc{A}$ so $QV(\mc{A}[t/x]) = QV(\mc{A})$. If $y\not \equiv x$ then $\mc{A}[t/x] \equiv ((\forall y) \mc{B}[t/x])$. In this case $QV(\mc{A}[t/x]) = QV(\mc{B}[t/x]) \cup \{y\} = QV(\mc{B})\cup\{y\} = QV(\mc{A})$.

\end{theorem}

\begin{theorem}[Free Variable Substitution in a Strict Formula Results in a Strict Formula]
Suppose $\mc{A}\in\textbf{sForm} = \textbf{Form}$, $x\in\textbf{Var}$ and $t\in\textbf{Term}$. We prove by induction on $D(\mc{A})$ that $\mc{A}[t/x]\in\textbf{Form}$.

If $D(\mc{A}) = 0$ then $\mc{A}$ is atomic. $\mc{A}[t/x]$ is also atomic so $\mc{A}[t/x]\in\textbf{Form}$.

Now suppose $D(\mc{A})>0$ but that for any $\mc{B}\in\textbf{Form}$ that if $D(\mc{B}) < D(\mc{A})$ that $\mc{B}[t/x]\in\textbf{Form}$.

$\mc{A}$ cannot be atomic because $D(\mc{A})\neq 0$.

If $\mc{A}$ is a negation then $\mc{A} \equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{Form}$ and $D(\mc{B})<D(\mc{A})$. We have $\mc{A}[t/x] \equiv (\lnot \mc{B}[t/x])$ but $\mc{B}[t/x]\in\textbf{Form}$ by the induction hypothesis so $\mc{A}[t/x]\in\textbf{Form}$.

If $\mc{A}$ is an implication then $\mc{A}\equiv (\mc{B}\implies \mc{C})$ with $\mc{B}, \mc{C}\in\textbf{Form}$ and $D(\mc{B}), D(\mc{C}) < D(\mc{A})$. We have $\mc{A}[t/x] \equiv (\mc{B}[t/x] \implies \mc{C}[t/x])$ but by the induction hypothesis $\mc{B}[t/x], \mc{C}[t/x]\in\textbf{Form}$ so $\mc{A}[t/x]\in\textbf{Form}$.

If $\mc{A}$ is a quantification then $\mc{A} \equiv ((\forall y) \mc{B})$ with $y\in\textbf{Var}$, $\mc{B}\in \textbf{Form}$, $D(\mc{B})<D(\mc{A})$, $y\in FV(\mc{B})$ and $y\not \in QV(\mc{B})$. If $y= x$ then $\mc{A}[t/x] \equiv \mc{A}$ so $\mc{A}[t/x]\in\textbf{Form}$. If $y\not \equiv x$ then $\mc{A}[t/x] \equiv ((\forall y) \mc{B}[t/x])$. By the induction hypothesis $\mc{B}[t/x]\in\textbf{Form}$. Because $x\in FV(\mc{B})$, we have, by the above theorem, that $FV(\mc{B}[t/x]) = (FV(\mc{B})\setminus\{x\})\cup FV(t)$. Since $y \neq x$ this means $y \in FV(\mc{B}[t/x])$. We also have, by another theorem above, that $QV(\mc{B}[t/x]) = QV(\mc{B})$ so $y\not \in QV(\mc{B}[t/x])$. This means that $\mc{A}[t/x]\in\textbf{Form}$.

\end{theorem}

Note that there are some term substitutions which are valid syntactically, but which are not "logically valid" in a way which will be described below.
As an example consider
$$
\mc{A} \equiv (Px \land ((\forall y) Qxy))
$$
Now let us substitute $y$ for $x$:
$$
\mc{A}[y/x] \equiv (Py \land ((\forall y) Qyy)
$$
Note how the semantic meaning of this formulas has been changed by the substitution.
In particular the problem is that the term being substituted in, $y$ in this case, became quantified over after being substituted for the $x$ in $Qxy$ within the scope of $(\forall y)$.
In such a circumstance we say the variable $y$ (which occurred in the substitution term) was "captured" by the $(\forall y)$ quantifier in $\mc{A}$.
To avoid such variable capture in logical deductions below we define the concept of substitutability.

\begin{definition}[Free Variable Substitutability]
Suppose $\mc{A}\in\textbf{pForm}$, $x\in\textbf{Var}$ and $t\in\textbf{Term}$. If $t$ is substitutable for $x$ in $\mc{A}$ then we say $S(\mc{A}, t, x)$. If it is the case that $S_F(\mc{A},t , x)$ then we may indicate this by letting $\mc{A} \equiv _{[t/x]}\mc{A}$.

\begin{itemize}
\item{If $\mc{A}$ is an atomic formula that $S_F(\mc{A}, t, x)$.}
\item{If $\mc{A} \equiv (\mc{B}\implies \mc{C})$ then $S_F(\mc{A}, t, x)$ if $S_F(\mc{B}, t, x)$ and $S_F(\mc{C}, t, x)$.}
\item{If $\mc{A}\equiv (\lnot \mc{B})$ then $S_F(\mc{A}, t, x)$ if $S(\mc{B}, t, x)$.}
\item{If $\mc{A}\equiv((\forall y) \mc{B})$ with $y\in\textbf{Var}$ then $S_F(\mc{A}, t, x)$ if $S_F(\mc{B}, t, x)$ and $y\not \in FV(t)$ or $x\not \in FV(\mc{B})$.}
\end{itemize}

\end{definition}

\begin{theorem}[Permissive Formula Depth after Free Variable Substitution]
\label{thm:pformdepthaftertermsub}
Suppose $\mc{A} \in \textbf{pForm}$, $x\in\textbf{Var}$ and $t\in\textbf{Term}$. Then $D(\mc{A}[t/x] = D(\mc{A})$.

Suppose $D(\mc{A}) = 0$.
Then $\mc{A}$ is atomic so $\mc{A}\equiv Pt_1\ldots t_n$ with $P\in\textbf{Pred}_n$ and $t_1,\ldots,t_n \in \textbf{Term}$.
$\mc{A}[t/x] \equiv P t_1[t/x]\ldots t_n[t/x]$.
By Theorem \ref{thm:fvsubintermgivesterm}, $t_1[t/x],\ldots, t_n[t/x]\in\textbf{Term}$ so $\mc{A}[t/x]$ is atomic so $D(\mc{A}[t/x])=0=D(\mc{A})$.

Now suppose that $D(\mc{A})>0$ but if $\mc{B}\in\textbf{pForm}$ with $D(\mc{B}) < D(\mc{A})$ that $D(\mc{B}[t/x]) = D(\mc{B})$.

$\mc{A}$ can't be atomic because $D(\mc{A}) >0$.

If $\mc{A}\equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$ then $D(\mc{A}) = D(\mc{B}) + 1$.
We have $\mc{A}[t/x] \equiv (\lnot \mc{B}[t/x])$ so $D(\mc{A}[t/x]) = D(\mc{B}[t/x]) + 1$
But, $D(\mc{B}) < D(\mc{A})$ so $D(\mc{B}[t/x]) = D(\mc{B})$ so $D(\mc{A}[t/x]) = D(\mc{A})$.

If $\mc{A} \equiv (\mc{B}\implies \mc{C})$ with $\mc{B}, \mc{C}\in\textbf{pForm}$ then $D(\mc{A}) = \text{max}(D(\mc{B}), D(\mc{C})) + 1$.
We have that $\mc{A}[t/x] \equiv (\mc{B}[t/x] \implies \mc{C}[t/x])$ so $D(\mc{A}[t/x]) = \text{max}(D(\mc{B}[t/x]), D(\mc{C}[t/x])) + 1$.
But $D(\mc{B}), D(\mc{C}) < D(\mc{A})$ so $D(\mc{B}[t/x]) = D(\mc{B})$ and $D(\mc{C}[t/x]) = D(\mc{C})$ so $D(\mc{A}[t/x]) = D(\mc{A})$.

If $\mc{A} \equiv ((\forall y)\mc{B})$ with $y\in\textbf{Var}$ and $\mc{B}\in\textbf{pForm}$ then $D(\mc{A}) = D(\mc{B}) + 1$.
If $x\equiv y$ then $\mc{A}[t/x] = \mc{A}$ so $D(\mc{A}[t/x]) = D(\mc{A})$.
If $x\not \equiv y$ then $\mc{A}[t/x] \equiv ((\forall y)\mc{B}[t/x])$ and $D(\mc{A}[t/x]) = D(\mc{B}[t/x]) + 1$.
But, $D(\mc{B}) < D(\mc{A})$ so $D(\mc{B}[t/x]) = D(\mc{B})$ so $D(\mc{A}[t/x]) = D(\mc{A})$.
\end{theorem}

\begin{definition}[Absent Term]
Suppose $t, s\in\textbf{Term}$ and $\mc{A}\in\textbf{pForm}$.
We recursively define the concept that $t$ does not appear in $s$ or $\mc{A}$.
If $t$ does not appear in $s$ or $\mc{A}$ then we may write $s\equiv {_{\{t\}}s}$ or $\mc{A}\equiv {_{\{t\}}\mc{A}}$.

Suppose $s\in\textbf{Term}$.
\begin{itemize}
\item{If $s\equiv t$ the $t$ appears in $s$.}
\item{If $s\equiv f s_1\ldots s_n$ with $f\in\textbf{Func}_n$ and $s_1,\ldots,s_n\in\textbf{Term}$ then $t$ appears in $s$ if $t$ appears in at least one of $s_1,\ldots, s_n$.}
\end{itemize}

Suppose $\mc{A}\in\textbf{pForm}$.
\begin{itemize}
\item{If $\mc{A} \equiv Ps_1\ldots s_n$ with $P\in\textbf{Pred}_n$ and $s_1,\ldots, s_n\in\textbf{Term}$ then $t$ appears in $\mc{A}$ if $t$ appears in at least one of $s_1,\ldots, s_n$.}
\item{If $\mc{A} \equiv (\mc{B}\implies \mc{C})$ with $\mc{B}, \mc{C}\in\textbf{pForm}$ then $t$ appears in $\mc{A}$ if $t$ appears in at least one of $\mc{B}$ or $\mc{C}$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$ then $t$ appears in $\mc{A}$ if $t$ appears in $\mc{B}$.}
\item{If $\mc{A} \equiv ((\forall x) \mc{B})$ with $\mc{B}\in\textbf{pForm}$ then $t$ appears in $\mc{A}$ if $t$ appears in $\mc{B}$.}
\end{itemize}

\end{definition}

\section{Bound Variable Substitution}

In addition to `plugging' terms into free variables, we are also interested in replacing one bound variable with a different variable.
This will help us to plug one formula into another by allowing us to replace all the bound variables in the first formula with variables which are unused in the second formula.
This is  necessary to avoid double quantification.

\begin{definition}[Bound Variable Substitution]
Suppose $\mc{A}\in\textbf{pForm}$ and $x, y \in \textbf{pForm}$.
We define $\mc{A}\{y/x\}$, the bound variable substitution of $y$ for $x$ in $\mc{A}$, as follows.

\begin{itemize}
\item{If $\mc{A}$ is atomic then $\mc{A}\{y/x\} \equiv \mc{A}$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$ then $\mc{A}\{y/x\} \equiv (\lnot \mc{B}\{y/x\})$.}
\item{If $\mc{A}\equiv (\mc{B}\implies\mc{C})$ with $\mc{B}, \mc{C} \in \textbf{pForm}$ then $\mc{A}\{y/x\} \equiv (\mc{B}\{y/x\} \implies \mc{C}\{y/x\})$.}
\item{Suppose $\mc{A} \equiv ((\forall z) \mc{B})$ with $z\in\textbf{Var}$ and $\mc{B}\in\textbf{pForm}$. If $z \equiv x$ then $\mc{A}\{y/x\} \equiv ((\forall y) (\mc{B}[y/x])\{y/x\})$. If $z\not \equiv x$ then $\mc{A}\{y/x\} \equiv ((\forall z)\mc{B}\{y/x\})$.}
\end{itemize}
\end{definition}

\begin{theorem}[Bound Variable Substitution in a Permissive Formula Results in a Permissive Formula]
Suppose $\mc{A}\in\textbf{pForm}$ and $x, y \in \textbf{Var}$. Then $\mc{A}\{y/x\}\in\textbf{pForm}$.

Suppose $D(\mc{A}) = 0$ so that $\mc{A}$ is atomic. Then $\mc{A}\{y/x\}\equiv \mc{A}$ so $\mc{A}\{y/x\} \in \textbf{pForm}$.

Now suppose $D(\mc{A}) >0$ and that if $\mc{B}\in\textbf{pForm}$ with $D(\mc{B}) < D(\mc{A})$ that $\mc{B}\{y/x\} \in \textbf{pForm}$.

$\mc{A}$ cannot be atomic because $D(\mc{A}) > 0$.

Suppose $\mc{A}\equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$.
Then $\mc{A}\{y/x\} \equiv (\lnot \mc{B}\{y/x\})$ but $D(\mc{B})<D(\mc{A})$ so $\mc{B}\{y/x\}\in\textbf{pForm}$ so $\mc{A}\{y/x\}\in\textbf{pForm}$.

Suppose $\mc{A} \equiv (\mc{B}\implies \mc{C})$ with $\mc{B}, \mc{C}\in\textbf{pForm}$.
Then $\mc{A}\{y/x\} \equiv (\mc{B}\{y/x\} \implies \mc{C}\{y/x\})$.
But, $D(\mc{B}), D(\mc{C}) < D(\mc{A})$ so $\mc{B}\{y/x\}, \mc{C}\{y/x\}\in\textbf{pForm}$ so $\mc{A}\in\textbf{pForm}$.

Suppose $\mc{A}\equiv ((\forall z)\mc{B})$ with $z\in\textbf{Var}$ and $\mc{B}\in\textbf{pForm}$.
If $z\not \equiv x$ then $\mc{A}\{y/x\} \equiv ((\forall z)\mc{B}\{y/x\})$.
But $D(\mc{B}) < D(\mc{A})$ so $\mc{B}\{y/x\} \in \textbf{pForm}$ so $\mc{A}\{y/x\}\in\textbf{pForm}$.
If $z\equiv x$ then $\mc{A}\{y/x\} \equiv ((\forall y) (\mc{B}[y/x])\{y/x\})$.
By Theorem \ref{thm:pformdepthaftertermsub}, $D(\mc{B}[t/x]) = D(\mc{B}) < D(\mc{A})$ so $(\mc{B}[y/x])\{y/x\}\in\textbf{pForm}$ so $\mc{A}\{y/x\}\in\textbf{pForm}$.
\end{theorem}

\begin{theorem}[Bound Variable Substitution for a Variable the Does Not Appear Bound]
\label{thm:bndvarsubforvarnotappear}
Suppose $\mc{A}\in\textbf{pForm}$ and $x, y \in \textbf{Var}$ and $x\not \in QV(\mc{A})$. Then $\mc{A}\{y/x\}\equiv \mc{A}$.

Suppose $D(\mc{A}) = 0$ so that $\mc{A}$ is atomic.
Then $\mc{A}\{y/x\} \equiv \mc{A}$.

Now suppose that $D(\mc{A}) > 0$ and that if $\mc{B}\in\textbf{pForm}$ with $D(\mc{B}) < D(\mc{A})$ that if $x\not \in QV(\mc{B})$ that $\mc{B}\{y/x\} \equiv \mc{B}$.

Suppose $\mc{A} \equiv ((\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$.
We have $D(\mc{B}) < D\mc{A})$.
We also have that $x\not \in QV(\mc{B})$ because $x\not \in QV(\mc{A})$.
We have that $\mc{A}\{y/x\} \equiv (\lnot \mc{B}\{y/x\})$ but, by the induction hypothesis, $\mc{B}\{y/x\} \equiv \mc{B}$ so $\mc{A}\{y/x\}\equiv \mc{A}$.

Suppose $\mc{A} \equiv (\mc{B}\implies \mc{C})$ with $\mc{B}, \mc{C} \in \textbf{pForm}$.
We have $D(\mc{B}), D(\mc{C}) < D(\mc{A})$.
We also have that $x\not \in QV(\mc{B})$ and that $x\not \in QV(\mc{C})$ because $x\not \in QV(\mc{A})$.
We have that $\mc{A}\{y/x\} \equiv (\mc{B}\{y/x\} \implies \mc{C}\{y/x\})$ but, by the induction hypothesis, $\mc{B}\{y/x\} \equiv \mc{B}$ and $\mc{C}\{y/x\}\equiv \mc{C}$ so $\mc{A}\{y/x\} \equiv \mc{A}$.

Suppose $\mc{A}\equiv ((\forall z) \mc{B})$ with $z\in\textbf{Var}$ and $\mc{B}\in\textbf{pForm}$.
We have that $D(\mc{B}) < D(\mc{A})$.
Because $x\not \in QV(\mc{A})$ we know that $z\not \equiv x$ and that $x\not \in QV(\mc{B})$.
We have that $\mc{A}\{y/x\} \equiv ((\forall z)\mc{B}\{y/x\})$ but, by the induction hypothesis, $\mc{B}\{y/x\} \equiv \mc{B}$ so $\mc{A}\{y/x\} \equiv \mc{A}$.
\end{theorem}

\begin{theorem}[Quantified Variables after Bound Variable Substitution]
Suppose $\mc{A}\in\textbf{pForm}$ and $x, y\in\textbf{Var}$.
If $x\not \in QV(\mc{A})$ then by Theorem \ref{thm:bndvarsubforvarnotappear} we know that $\mc{A}\{y/x\} \equiv \mc{A}$ so $QV(\mc{A}\{y/x\}) = QV(\mc{A})$. If $x\in QV(\mc{A})$ then we now prove that $QV(\mc{A}\{y/x\}) = (QV(\mc{A})\setminus\{x\})\cup\{y\}$.

Suppose $D(\mc{A}) = 0$ so that $\mc{A}$ is atomic.
Then $QV(\mc{A}) = \emptyset$ so $x\not \in QV(\mc{A})$ so we know $QV(\mc{A}\{y/x\}) = QV(\mc{A})$.

Now suppose that $D(\mc{A}) > 0$ but that for $\mc{B}\in\textbf{pForm}$ with $D(\mc{B}) < D(\mc{A})$ that, if $x\in QV(\mc{B})$, that $QV(\mc{B}\{y/x\}) = (QV(\mc{B})\setminus\{x\})\cup\{y\}$.

$\mc{A}$ cannot be atomic because $D(\mc{A})>0$.

Suppose $\mc{A}\equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$ so that $QV(\mc{A}) = QV(\mc{B})$.
We have $D(\mc{B}) < D(\mc{A})$.
Since $x \in QV(\mc{A})$ we have $x \in QV(\mc{B})$.
We have $\mc{A}\{y/x\} \equiv (\lnot \mc{B}\{y/x\})$ so $QV(\mc{A}\{y/x\}) = QV(\mc{B}\{y/x\})$ but, by the induction hypothesis, $QV(\mc{B}\{y/x\}) = (QV(\mc{B})\setminus \{x\})\cup\{y\}$ so $QV(\mc{A}\{y/x\}) = QV(\mc{A})$.

Suppose $\mc{A}\equiv (\mc{B}\implies \mc{C})$ with $\mc{B}, \mc{C}\in\textbf{pForm}$ so that $QV(\mc{A}) = QV(\mc{B}) \cup QV(\mc{C})$.
We have $D(\mc{B}), D(\mc{C}) < D(\mc{A})$.
Since $x \in QV(\mc{A})$ we have $x \in QV(\mc{B})$ and/or $x \in QV(\mc{C})$.
We have $\mc{A}\{y/x\} \equiv (\mc{B}\{y/x\} \implies \mc{C}\{y/x\})$ so $QV(\mc{A}\{y/x\}) = QV(\mc{B}\{y/x\}) \cup QV(\mc{C}\{y/x\})$.

If $x\in QV(\mc{B})$ then, by the induction hypothesis, $QV(\mc{B}\{y/x\}) = (QV(\mc{B})\setminus\{x\})\cup\{y\}$. If $x\not \in QV(\mc{B})$ then $QV(\mc{B}\{y/x\}) = QV(\mc{B})$.
Likewise, if $x\in QV(\mc{C})$ then $QV(\mc{C}\{y/x\}) = (QV(\mc{C})\setminus\{x\}) \cup \{y\}$ but if $x\not \in QV(\mc{C})$ then $QV(\mc{C}\{y/x\}) = QV(\mc{C})$.

There are three cases.
If $x\in QV(\mc{B})$ and $x\in QV(\mc{C})$ then
\begin{align}
QV(\mc{A}\{y/x\}) =& ((QV(\mc{B})\setminus\{x\})\cup \{y\}) \cup ((QV(\mc{C})\setminus\{x\})\cup \{y\}) \notag\\
=& ((QV(\mc{B}) \cup QV(\mc{C}))\setminus\{x\})\cup\{y\} \notag\\
=& (QV(\mc{A})\setminus\{x\})\cup\{y\} \notag
\end{align}

If $x\in QV(\mc{B})$ but $x\not \in QV(\mc{C})$ then
\begin{align}
QV(\mc{A}\{y/x\}) =& ((QV(\mc{B})\setminus\{x\})\cup\{y\})\cup QV(\mc{C}) \notag\\
=& ((QV(\mc{B})\cup QV(\mc{C}))\setminus\{x\})\cup\{y\} \notag\\
=& (QV(\mc{A})\setminus\{x\})\cup\{y\} \notag
\end{align}
Where the second line follows in part because $x\not \in QV(\mc{C})$.

If $x\not \in QV(\mc{C})$ but $x\in QV(\mc{C})$ then
\begin{align}
QV(\mc{A}\{y/x\}) =& QV(\mc{B}) \cup ((QV(\mc{C})\setminus\{x\})\cup\{y\}) \notag\\
=& ((QV(\mc{B})\cup QV(\mc{C}))\setminus\{x\})\cup\{y\} \notag\\
=& (QV(\mc{A})\setminus\{x\})\cup\{y\} \notag
\end{align}
Where the second line follows in part because $x\not \in QV(\mc{B})$.

In all cases $QV(\mc{A}\{y/x\}) = (QV(\mc{A})\setminus\{x\})\cup\{y\}$.

\end{theorem}

There are two possible issues with bound variable substitution.
The first issue is that bound variable substitution


\section{Formal Proof}

We now transition from discussing the syntax for making and manipulating formulas within the formal language into combining the formulas into natural deduction proofs.
Semantically a proof encapsulates the idea that, if some set of formulas, called the \textit{premises}, are assumed to be true, then we can logically deduce other formulas which are also true.
Recall that, semantically speaking, we can only assign truth values to closed formulas.
For this reason, we will require that all formulas involved in natural deduction proofs be closed.

\subsection{Sequents}

Syntactically, we represent the idea that a (closed) formula $\mc{A}$ logically follows from a set of (closed) formulas $\Gamma$ using a \textit{sequent} or \text{judgment} which we express as
\begin{align}
\Gamma \vdash \mc{A}
\end{align}

The symbol in the middle $\vdash$ is the turnstile symbol.
The set of formulas in the set $\Gamma$ on the left hand side are called the antecedents of the sequent and the formula $\mc{A}$ on the right hand side is called the subsequent.
We say the subsequent of $\mc{A}$ is derivable form the antecedents $\Gamma$.
A more concrete example of a sequent is
$$
\{(\mc{A}\implies \mc{B}), \mc{A}\} \vdash \mc{B}
$$

We will use shorthand notation in writing the antecedents of sequents.
If we have a sequent
$$
\Gamma_1\cup \ldots \cup \Gamma_n \cup \{\mc{A}_1\}\cup \ldots \cup \{\mc{A}_n\} \vdash \mc{B}
$$
Then we can abbreviate this as
$$
\Gamma_1,\ldots, \Gamma_n, \mc{A}_1,\ldots, \mc{A}_n \vdash \mc{B}
$$
That is, singleton sets containing formulas have curly brackets suppressed and union operators are replaced by commas.
The notation $\Gamma_1 - \Gamma_2$ denotes the subtraction of set $\Gamma_2$ from $\Gamma_1$.
If $\Gamma = \{\mc{A}_1, \ldots, \mc{A}_n\}$ and $t$ does not appear in any of $\mc{A}_i$ (that is $\mc{A}_i \equiv {_{\{t\}}\mc{A}}$ for all $1\le i \le n$) then we say $t$ does not appear in $\Gamma$ and we may write $\Gamma = {_{\{t\}}\Gamma}$.

In what follows we will develop our proof theoretic framework which is essentially a calculus of the $\vdash$ symbol over the closed formulas of the language.
The framework will be based around rules of inference which allow us to derive new sequents from old sequents.

\section{Proofs and Proof Tables}

As mentioned above we begin with a set of premises and, through logical manipulations, we draw new conclusions.
In the context of syntactic proof theory, the premises serve as the starting point for a logical symbol manipulation game whereby the rules of inference allow us to infer or derive new logical formulas from the premises and other formulas.

In a proof we begin with one or more sets of closed formulas called premises.
For examples, one set of premises may be may be
$$
\Gamma = \{\gamma_1, \ldots, \gamma_n\}
$$
where each of $\gamma_i$ for $1\le i \le n$ is a closed formula.
We'll give an example of one of the simplest proofs possible.
Suppose
$$
\Gamma = \{(\mc{A}\implies \mc{B}), \mc{A}\}
$$
We can then write down a simple proof table.
\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{3cm} }
(1) & $\Gamma \vdash (\mc{A} \implies \mc{B})$ & Prem\\
(2) & $\Gamma \vdash \mc{A}$ & Prem\\
(3) & $\Gamma \vdash \mc{B}$ & $\implies E$, 1, 2
\end{tabular}
\end{center}

The proof table has three columns.
The left column numbers inferences within the proof, the middle column is a sequent, and the third column indicates the logical justification or inference rule for the conclusion of the sequent in the middle column.

The rules for a proof table are as follows.
The premises for a proof must be specified at the outset, though it is not strictly that the first lines of a proof outline any or all of the premise of the proof..
The premises appear as one or more sets $\Gamma_i$ of closed formulas.
If $\gamma$ is a formula in $\Gamma$ then it may be introduced to the proof via the Prem inference rule as shown above.

Each line of derivation is labeled (1), (2), $\ldots$ (n).
Each line of the proof follows from an inference rule (tabulated and explained in the following section) which may leverage some earlier lines in the proof.
Both the name of the inference rule and the lines on which the inference rule relies are specified in the right column.

The content of the proof above is that if we begin with the set of premises $\Gamma$, we can logically deduce $\mc{B}$.
Generally, if a sequent like $\Gamma \vdash \mc{B}$ appears on line ($n$) of a proof table then we say the proof constitutes a proof of length $n$ of the subsequent of the sequent on line ($n$) from the antecedents of the sequent on line ($n$).
Note that it is not strictly necessary that every premise appears as a Prem line in a proof.
Furthermore, it is not necessary that every premise appears in the antecedent of the final sequent.

Note that this proof format in fact allows us to derive new sequents from old sequents.
Alternatively, it is possible to develop a natural deduction in which the lines of the proofs are formulas rather than sequents.
The advantage of the sequent oriented approach is that it is very natural to keep track of on which earlier formulas a particular conclusion relies.
In the formula based approach the dependency formulas must be kept track of independently.

\subsection{Inference Rules}

Below I will write down all of the native inference rules for this system of natural deduction.
Each of these rules can be used in a formal proof as described above.
The rules of inference will be notated as one set of sequents above a line and one sequent below the line.
This indicates that the set of sequents above the line (the dependencies) logically allow the deduction of the sequent below the line (the conclusion).
If certain lines within a proof table match the format of the dependencies of a particular inference rule then we may append a new row to the proof table whose sequent is the conclusion sequent of the inference rule and indicating the inference rule and dependency lines appropriately.

\hrulefill
\begin{definition}[Rule of Prem (Premise)]
\leavevmode
If the premises for a proof are the sets $\{\Gamma_1, \ldots \Gamma_N\}$ and $\gamma_{ij} \in \Gamma_i$ then we have
\begin{prooftree}
\AxiomC{}
\UnaryInfC{$\Gamma_i \vdash \gamma_{ij}$}
\end{prooftree}

\end{definition}

\begin{definition}[Rule of A (Assumption)]
\leavevmode
\begin{prooftree}
\AxiomC{}
\UnaryInfC{$\mc{A}\vdash \mc{A}$}
\end{prooftree}
We can always assume that a (closed) formula $\mc{A}$ derives itself from no premises.
\end{definition}

\begin{definition}[Rule of W (Weakening)]
\leavevmode
\begin{prooftree}
\AxiomC{$\Gamma_1 \vdash \mc{A}$}
\UnaryInfC{$\Gamma_1, \Gamma_2 \vdash \mc{A}$}
\end{prooftree}

Note that $\Gamma_2$ may be the empty set. This allows us to derive a sequent that has already been derived on a later line. This will be useful to allow to rewrite the same sequent with the inclusion or exclusions of convenient abbreviations. (Each formula in $\Gamma_2$ must be closed)
\end{definition}

\begin{definition}[Rule of $\implies I$ (Implication Introduction)]
\leavevmode
\begin{prooftree}
\AxiomC{$\Gamma \vdash \mc{B}$}
\UnaryInfC{$\Gamma\backslash \{\mc{A}\} \vdash (\mc{A} \implies \mc{B})$}
\end{prooftree}

Note that $\mc{A}$ need not appear in $\Gamma$ for this inference rule. ($\mc{A}$ must be closed).
\end{definition}

\begin{definition}[Rule of $\implies E$ (Implication Elimination)]
\leavevmode
\begin{prooftree}
\AxiomC{$\Gamma_1 \vdash (\mc{A} \implies \mc{B})$}
\AxiomC{$\Gamma_2 \vdash \mc{A}$}
\BinaryInfC{$\Gamma_1, \Gamma_2 \vdash \mc{B}$}
\end{prooftree}
\end{definition}

\begin{definition}[Rule of $\lnot E$ (Negation Elimination)]
\leavevmode
\begin{prooftree}
\AxiomC{$\Gamma_1 \vdash \mc{A}$}
\AxiomC{$\Gamma_2 \vdash (\lnot \mc{A})$}
\BinaryInfC{$\Gamma_1, \Gamma_2 \vdash \curlywedge$}
\end{prooftree}
Here $\curlywedge\in\textbf{Pred}_0$ is the contradiction symbol.
\end{definition}

\begin{definition}[Rule of $\lnot I$ (Negation Introduction)]
\leavevmode
\begin{prooftree}
\AxiomC{$\Gamma \vdash \curlywedge$}
\UnaryInfC{$\Gamma \backslash \{\mc{A}\} \vdash (\lnot \mc{A})$}
\end{prooftree}
Note that $\mc{A}$ need not appear in $\Gamma$. ($\mc{A}$ must be closed.)
\end{definition}

\begin{definition}[Rule of $DN$ (Double Negation)]
\leavevmode
\begin{prooftree}
\AxiomC{$\Gamma \vdash (\lnot(\lnot \mc{A}))$}
\UnaryInfC{$\Gamma \vdash \mc{A}$}
\end{prooftree}
\end{definition}

\begin{definition}[Rule of $\forall I$ (Universal Introduction/Generalization)]
Suppose $\mc{A}\in \textbf{Form}$ with $FV(\mc{A}) = \{x\}$, $t\in \textbf{Term}$ is closed and $S(\mc{A}, t, x)$. Furthermore suppose $t$ does not appear in $\Gamma$ or $\mc{A}$.
Then we have

\begin{prooftree}
\AxiomC{$\Gamma \vdash \mc{A}[t/x]$}
\UnaryInfC{$\Gamma \vdash ((\forall x) \mc{A})$}
\end{prooftree}
\end{definition}

\begin{definition}[Rule of $\forall E$ (Universal Elimination/Instantiation/Specification)]
Suppose $t$ is a closed term, then
\begin{prooftree}
\AxiomC{$\Gamma \vdash ((\forall x)\mc{A})$}
\UnaryInfC{$\Gamma \vdash \mc{A}[t/x]$}
\end{prooftree}
\end{definition}
\hrulefill

We can modularize our proof finding apparatus by introducing so-called derived inference rules.
Suppose we have

Recall that when we have a proof table we may say that we have proven the sequent appearing on any line (typically the last line) from the premises.
Whenever we prove one sequent from some set of sequents we can introduce a new derived inference rule.
Specifically, if we have derived $\Theta \vdash \mc{B}$ on line $(n)$ then we can introduce a new inference rule.
We collect all premises which were used on lines $(j)$ with $j \le n$ and introduce:

\hrulefill

\textbf{Rule of [Der] (Derived Inference Rule)}

\begin{prooftree}
\AxiomC{$\Gamma_{i_1} \vdash \gamma_{j_1}, \ldots, \Gamma_{i_n} \vdash \gamma_{j_n}$}
\UnaryInfC{$\Theta \vdash \mc{B}$}
\end{prooftree}

\hrulefill

A derived inference rule can be thought of as an abbreviation for the proof that justifies the introduction of that derived rule.
That is, rather than repeat the proof of the derived inference in a new proof, one can skip the proof and simply use the derived inference rule.
Each derived inference rule may have its own name.

\newpage

We introduce a number of logical abbreviations to allow us to express the usual full range of logical connectives.

\begin{definition}[Logical Abbreviations]

Suppose $\mc{A}, \mc{B}\in \textbf{pForm}$

\begin{itemize}
\item{\textbf{Conjunction}: We let $(\mc{A} \land \mc{B})$ abbreviate $(\lnot(\mc{A} \implies (\lnot \mc{B})))$}
\item{\textbf{Disjunction}: We let $(\mc{A} \lor \mc{B})$ abbreviate $((\lnot \mc{A}) \implies \mc{B})$}
\item{\textbf{Equivalence}: We let $(\mc{A} \iff \mc{B})$ abbreviate $((\mc{A} \implies \mc{B}) \land (\mc{B} \implies \mc{A}))$}
\item{\textbf{Existential Quantification}: We let $((\exists x) \mc{A})$ abbreviate $(\lnot((\forall x)(\lnot \mc{A})))$. .}
\end{itemize}
\end{definition}

We also introduce abbreviations for removing parentheses.
This abbreviation language is copied directly from Tourlakis.

\begin{definition}[Parentheses Abbreviation]
\begin{itemize}
\item{\textbf{Parentheses:} ``To minimize the use of brackets in the metanotation we adopt standard \textit{priorities} of connectives: $\forall, \exists,$ and $\lnot$ have the highest, and then we have (in decreasing order of priority) $\land, \lor, \implies, \iff$, and we agree not to use outermost brackets. all \textit{associativities} are \textit{right} - that is if we write $\mc{A} \implies \mc{B} \implies \mc{C}$, then it is a (sloppy) counterpart for $(\mc{A} \implies ( \mc{B} \implies \mc{C}))$.''}
\end{itemize}
\end{definition}

\subsection{Metalanguage Concepts}

The next (final?) metalanguage concept we will introduce is that of sub-Wff replacement.
This will allow us to perform the operation of replacing an atomic formula, or sub-Wff within a Wff, by another Wff.

\noindent\fbox{\parbox{\textwidth}{
\textbf{Wff Substitution}

If $\mc{A}$ is a Wff, $\mc{X}$ is an atomic formula and $\mc{P}$ is a Wff then we define the replacement of $\mc{X}$ by $\mc{P}$ in $\mc{A}$, notated as $\mc{A}[\mc{P}/\mc{X}]$ by:

\begin{itemize}
\item{If $\mc{A}$ is atomic then if $\mc{A}\equiv \mc{X}$ then $\mc{A}[\mc{P}/\mc{X}] \equiv \mc{P}$. If $\mc{A}\not\equiv \mc{X}$ then $\mc{A}[\mc{P}/\mc{X}] \equiv \mc{A}$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ the $\mc{A}[\mc{P}/\mc{X}] \equiv (\lnot \mc{B}[\mc{P}/\mc{X}])$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $\mc{A}[\mc{P}/\mc{X}] \equiv (\mc{B}[\mc{P}/\mc{X}] \implies \mc{C}[\mc{P}/\mc{X}])$.}
\item{If $\mc{A} \equiv ((\forall x)\mc{B})$ then $\mc{A}[\mc{P}/\mc{X}] \equiv ((\forall x) (\mc{B}[\mc{P}/\mc{X}])$.}
\end{itemize}
}}

Above, we saw that in term substitution we always ended up with either a term or a Wff after the term substitution took place.
However, it was necessary to define the concept of substitutability to highlight substitutions that did or didn't fundamentally change the structure of a Wff.
In particular, it was important to be able to rule out substitution which would result in variable capture.

The case for Wff substitution is even more drastic.
Wff substitution as defined above does not even always result in a Wff.
There are three ways $\mc{A}[\mc{P}/\mc{X}]$ can be problematic.
The first one results in redundant quantification, the second results in repeated quantification, and the third results in variable capture.

\begin{itemize}
\item{If $\mc{X}$ is within the scope of a quantifier $(\forall x)$ but $x\not \in FV(\mc{P})$, then it is possible that after substitution of $\mc{P}$ for $\mc{X}$ that the quantifier $(\forall x)$ will not quantify over any variable $x$ resulting in redundant quantification. For example, let $\mc{X} \equiv Q_x$, $\mc{P} \equiv P$ and $\mc{A} \equiv ((\forall x) Q_x)$. Then $\mc{A}[\mc{P}/\mc{X}] \equiv ((\forall x) P)$. We can see this is not a valid Wff because $(\forall x)$ is a redundant quantifier. Note however that, even though $FV(\mc{X})$ contains an element not in $FV(\mc{P})$ it is not always the case that replacement of $\mc{X}$ by $\mc{P}$ will result in redundant quantification. For example if $\mc{A} \equiv ((\forall x)(Q_x \land S_x))$ then $\mc{A}[\mc{P}/\mc{X}] \equiv ((\forall x)(P\land S_x))$ which is in fact a Wff with no redundant quantification.}
\item{If $\mc{X}$ appears in $\mc{A}$ within the scope of some quantifier $(\forall x)$ and $\mc{P}$ quantifies over that same variable then $\mc{A}[\mc{P}/\mc{X}]$ will have repeated quantification. For example if $\mc{X} \equiv Q$, $\mc{P} \equiv ((\forall y) P_y)$ and $\mc{A} \equiv ((\forall y) (Q\land S_y))$ then $\mc{A}[\mc{P}/\mc{X}] \equiv ((\forall y)(((\forall y)P_y)\land S_y))$ which we can see is not a Wff because we have repeated quantification of $(\forall y)$.}
\item{Finally, if $\mc{X}$ is within the scope of some quantifier $(\forall x)$ but $x\not \in FV(\mc{X})$ but $x\in FV(\mc{P})$ then the free occurrences of $x$ in $\mc{P}$ will become captured after substitution. For example if $\mc{X} \equiv Q$, $\mc{P} \equiv P_x$ and $\mc{A} \equiv ((\forall x) (Q\land R_x))$ then $\mc{A} \equiv ((\forall x)(P_x\land R_x))$.}
\end{itemize}

We can always avoid the first and final issues by requiring that $FV(\mc{P}) = FV(\mc{X})$ when performing atomic formula substitution.
The second issue can be avoided by noting (as we will prove later) that any Wff $\mc{P}$ is equivalent to one in which all bound variables are replaced by new, fresh, bound variables which do not appear in $\mc{A}$.

We saw above that in term substitution it was necessary to define the concept of substitutability on top of substitution.
In the case of term substitution it was always the case that term substitution resulted in either a term or a Wff after substitution

Similar to the case



Note that $\mc{A}[\mc{P}/\mc{X}]$ is only a Wff if none of the quantifiers with $\mc{P}$ result in repeated quantification after substitution in $\mc{A}$.
For example consider

$$
\mc{A} \equiv ((\forall x) (Q_x \land \mc{X}))
$$

with $Q_x$ a 1-ary predicate and $\chi$ a 0-ary predicate.
Let $\mc{P} \equiv ((\forall x) S_x)$.
We see that $FV(\mc{X}) = FV(\mc{P}) = \emptyset$ so $\mc{A}[\mc{P}/\mc{X}]$ is define as above by

$$
\mc{A}[\mc{P}/\mc{X}] \equiv ((\forall x)(Q_x \land ((\forall x) S_x)))
$$

But we can see that this is not a Wff because of the repeated quantification over $x$.
This issue

Similarly to term replacement, we must also define a concept of substitutability for atoms. We say $\mc{S}(\mc{A}, \mc{P}, \mc{X})$ to mean that $\mc{P}$ is substitutable for $\mc{X}$ in $\mc{A}$. $\mc{S}(\mc{A}, \mc{P}, \mc{X})$ if (1) if $(\forall x)$ appear in $\mc{P}$ that $\mc{X}$ is not within the scope of any $(\forall x)$ quantifier in $\mc{A}$ and (2) no

\subsubsection{Draft}

Bound Variable Substitution

\noindent\fbox{\parbox{\textwidth}{
\textbf{Bound Variable Substitution}

If $\mc{A}$, $\mc{B}$, and $\mc{C}$ are Wffs and $x$ and $y$ are variables then we define $\mc{A}\{y/x\}$ as

\begin{itemize}
\item{If $\mc{A}$ is an atomic formula then $\mc{A}\{y/x\} \equiv \mc{A}$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $\mc{A}\{y/x\} \equiv (\mc{B}\{y/x\} \implies \mc{C}\{y/x\})$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ then $\mc{A}\{y/x\} \equiv (\lnot \mc{B}\{y/x\})$.}
\item{If $\mc{A} \equiv ((\forall z) \mc{B})$ then if $x \equiv z$ then $\mc{A}\{y/x\} \equiv ((\forall y) \mc{B}[y/z])$ and if $x\not \equiv z$ then $\mc{A}\{y/x\} \equiv \mc{A}$.}
\end{itemize}
}}

\noindent\fbox{\parbox{\textwidth}{
\textbf{Bound Variable Substitutability}

If $\mc{A}$, $\mc{B}$, and $\mc{C}$ are Wffs and $x$ and $y$ are variables then we define $\mc{A}\{y/x\}$ as

\begin{itemize}
\item{If $\mc{A}$ is an atomic formula then $S_B(\mc{A}, y, x)$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $S_B(\mc{A}, y, x)$ if $S_B(\mc{B}, y, x)$ and $S_B(\mc{C}, y, x)$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ then $S_B(\mc{A}, y, x)$ if $S_B(\mc{B}, y, x)$.}
\item{If $\mc{A} \equiv ((\forall z) \mc{B})$ then if $x \equiv z$ then $S_B(\mc{A}, y, x)$ if $S_F(\mc{B}, y, z)$. If $x\not \equiv z$ then $S_B(\mc{A}, y, x)$.}
\end{itemize}
}}

Proof that if $S_B(\mc{A}, y, x)$ then $\mc{A}\{y/x\}$ is a Wff. We induct on $D(\mc{A})$

Base case: If $D(\mc{A}) = 0$ then $\mc{A}$ is an atomic formula. Then $\mc{A}\{y/x\}$ is also an atomic formula so $\mc{A}\{y/x\}$ is a Wff.

Induction: Assume that, for any Wff $\mc{B}$, that if $D(\mc{B}) < n$ then $\mc{B}\{y/x\}$ is a Wff. Now assume $D(\mc{A}) = n$

If $\mc{A} \equiv (\lnot \mc{B})$ then $D(\mc{B}) = D(\mc{A}) - 1 < n$. $\mc{A}\{y/x\} \equiv (\lnot \mc{B}\{y/x\})$ but $\mc{B}\{y/x\}$ is a Wff so $\mc{A}\{y/x\}$ is also a Wff.

If $\mc{A} \equiv (\mc{B}\implies \mc{C})$ then $D(\mc{A}) = D(\mc{B}) + d(\mc{C}) + 1$ so $D(\mc{B}), D(\mc{C}) < \mc{A}$ so $\mc{B}\{y/x\}$ and $\mc{C}\{y/x\}$ are Wffs by induction hypothesis. $\mc{A}\{y/x\} \equiv (\mc{B}\{y/x\} \implies \mc{C}\{y/x\})$. Since $\mc{B}\{y/x\}$ and $\mc{C}\{y/x\}$ are Wffs we have that $\mc{A}\{y/x\}$ is a Wff.

Now suppose $\mc{A} \equiv ((\forall z)\mc{B})$. if $x\equiv z$ then $\mc{A}\{y/x\} \equiv ((\forall y)\mc{B}[y/x])$. For this to be a Wff we must have that $\mc{B}[y/z]\equiv$ is a Wff, $\mc{B}[y/z]$ contains $y$, but does not contain $(\forall y)$.

$\mc{B}[y/z]$ is a Wff because $\mc{B}$ is a Wff. <<need some proofs about term substitution to prove that since $z$ appears in $\mc{B}$ (because it's in the scope of $\forall z$ in $\mc{A}$) that $y$ appears in $\mc{B}[y/z]$. I think this will proceed by showing that $z \in FV(\mc{B})$ and that $FV(\mc{B}[y/z]) = FV(\mc{B})\backslash \{z\} \cup \{y\}$ so that $y\in FV(\mc{B}[y/z])$ and if $y\in FV(\mc{B}[y/z])$ then $y$ appears in $\mc{B}[y/z]$>>

But, because $S_B(\mc{A}, y, x)$, we have that $S_F(\mc{B}, y, z)$.

\newpage

\section{Deriving Inference Rules for Abbreviated Logical Symbols}

We will now derive the inference rules for the abbreviation logical symbols $\land, \lor, \iff$ and $\exists$.
We will first introduce some convenience inference rules.

\subsection{Convenience Inference Rules}

\subsubsection*{Weaker $\implies I$}

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{2cm} }
(1) & $\Gamma, \mc{A} \vdash \mc{B}$ & Prem\\
(2) & $\Gamma/\{\mc{A}\} \vdash (\mc{A} \implies \mc{B})$ & $\implies I$, 1\\
(3) & $\Gamma \vdash (\mc{A} \implies \mc{B})$ & $W$, 2
\end{tabular}
\end{center}

\subsubsection*{Weaker $\lnot I$}

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{2cm} }
(1) & $\Gamma, \mc{A} \vdash \curlywedge$ & Prem\\
(2) & $\Gamma/\{\mc{A}\} \vdash (\lnot \mc{A})$ & $\lnot I$, 1\\
(3) & $\Gamma \vdash (\lnot \mc{A})$ & $W$, 2
\end{tabular}
\end{center}

These gives us the weaker, but useful, versions of the inference rules.
On the surface these inference rules look similar, but they are different in the case that $\mc{A} \in \Gamma$.
This is because $(\Gamma \cup \{\mc{A}\})/\{\mc{A}\} = \Gamma / \{\mc{A}\}$ which is not necessarily equal to $\Gamma$.

\hrulefill

\textbf{Rule of $\implies I_W$ (Implication Introduction - Weak)}

\begin{prooftree}
\AxiomC{$\Gamma, \mc{A} \vdash \mc{B}$}
\UnaryInfC{$\Gamma \vdash (\mc{A} \implies \mc{B})$}
\end{prooftree}

\hrulefill

\textbf{Rule of $\lnot I_W$ (Negation Introduction - Weak)}

\begin{prooftree}
\AxiomC{$\Gamma, \mc{A} \vdash \curlywedge$}
\UnaryInfC{$\Gamma \vdash (\lnot \mc{A})$}
\end{prooftree}

\hrulefill

\subsection{Modus Tollens}

I'll prove three forms of Modus Tollens and give three corresponding derived rules.

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{3cm} }
(1) & $\Gamma_1 \vdash (\mc{A} \implies \mc{B})$ & Prem\\
(2) & $\Gamma_2 \vdash (\lnot \mc{B})$ & Prem\\
(3) & $\mc{A} \vdash \mc{A}$ & A\\
(4) & $\Gamma_1, \mc{A} \vdash \mc{B}$ & $\implies E$, 1, 3\\
(5) & $\Gamma_1, \Gamma_2, \mc{A} \vdash \curlywedge$ & $\lnot E$, 2, 4\\
(6) & $\Gamma_1, \Gamma_2 \vdash (\lnot \mc{A})$ & $\lnot I_W$, 5
\end{tabular}
\end{center}

From this we get our first form of the Modus Tollens derived inference rule.

\hrulefill

\textbf{Rule of $MT1$ (Modus Tollens)}

\begin{prooftree}
\AxiomC{$\Gamma_1 \vdash (\mc{A} \implies \mc{B})$}
\AxiomC{$\Gamma_2 \vdash (\lnot \mc{B})$}
\BinaryInfC{$\Gamma_1, \Gamma_2 \vdash (\lnot \mc{A})$}
\end{prooftree}

\hrulefill

We can use the derived inference rule to derive two closely related forms of the Modus Tollens inference rule.

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{3cm} }
(1) & $\Gamma \vdash (\mc{A}\implies \mc{B})$ & Prem\\
(2) & $(\lnot \mc{B}) \vdash (\lnot \mc{B})$ & A\\
(3) & $\Gamma, (\lnot \mc{B}) \vdash (\lnot \mc{A})$ & $MT1$, 1, 2\\
(4) & $\Gamma \vdash ((\lnot \mc{B}) \implies (\lnot \mc{A}))$ & $\implies I_W$, 3
\end{tabular}
\end{center}

\hrulefill

\textbf{Rule of $MT2$ (Modus Tollens)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash (\mc{A} \implies \mc{B})$}
\UnaryInfC{$\Gamma, (\lnot \mc{B}) \vdash (\lnot \mc{A})$}
\end{prooftree}

\hrulefill

\textbf{Rule of $MT3$ (Modus Tollens)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash (\mc{A} \implies \mc{B})$}
\UnaryInfC{$\Gamma \vdash ((\lnot \mc{B}) \implies (\lnot \mc{A}))$}
\end{prooftree}

\hrulefill

\subsection{Conjunction Inference Rules}
\subsubsection*{Proof for Right Conjunction Elimination}

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{2cm} }
(1) & $\Gamma \vdash (\mc{A}\land\mc{B})$ & Prem\\
(2) & $\Gamma \vdash (\lnot(\mc{A}\implies(\lnot\mc{B})))$ & $W$, 1\\
(3) & $(\lnot\mc{B}) \vdash (\lnot\mc{B})$ & $A$ \\
(4) & $(\lnot \mc{B}) \vdash (\mc{A} \implies (\lnot \mc{B}))$ & $\implies I$, 3\\
(5) & $\Gamma, (\lnot \mc{B})\vdash \curlywedge$ & $\lnot E$, 2, 4\\
(6) & $\Gamma \vdash (\lnot(\lnot \mc{B}))$ & $\lnot I_W$, 5\\
(7) & $\Gamma \vdash \mc{B}$ & $DN$, 6
\end{tabular}
\end{center}

\hrulefill

\textbf{Rule of $\land E_R$ (Conjunction Elimination - Right)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash (\mc{A} \land \mc{B})$}
\UnaryInfC{$\Gamma \vdash \mc{B}$}
\end{prooftree}


\hrulefill

\subsubsection*{Proof for Left Conjunction Elimination}

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{2cm} }
(1) & $\Gamma \vdash (\mc{A}\land\mc{B})$ & Prem\\
(2) & $\Gamma \vdash (\lnot(\mc{A}\implies(\lnot\mc{B})))$ & $W$, 1\\
(3) & $(\lnot\mc{A})\vdash (\mc{\lnot A})$ & A\\
(4) & $\mc{A} \vdash \mc{A}$ & A\\
(5) & $\mc{A}, (\lnot \mc{A}) \vdash \curlywedge$ & $\lnot E$, 3, 4\\
(6) & $\mc{A}, (\lnot \mc{A}) \vdash (\lnot \mc{B})$ & $\lnot I$, 5\\
(7) & $(\lnot \mc{A}) \vdash (\mc{A} \implies (\lnot \mc{B})$ & $\implies I$, 6\\
(8) & $\Gamma, (\lnot \mc{A}) \vdash \curlywedge$ & $\lnot E$, 2, 7\\
(9) & $\Gamma \vdash (\lnot (\lnot \mc{A}))$ & $\lnot I_W$, 8\\
(10) & $\Gamma \vdash \mc{A}$ & $DN$ 9
\end{tabular}
\end{center}

\hrulefill

\textbf{Rule of $\land E_L$ (Conjunction Elimination - Left)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash (\mc{A}\land \mc{B})$}
\UnaryInfC{$\Gamma \vdash \mc{A}$}
\end{prooftree}

\hrulefill

\subsubsection*{Proof for Conjunction Introduction}

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{3cm} }
(1) & $\Gamma_1 \vdash \mc{A}$ & Prem\\
(2) & $\Gamma_2 \vdash \mc{B}$ & Prem\\
(3) & $(\mc{A} \implies (\lnot \mc{B})) \vdash (\mc{A} \implies (\lnot \mc{B}))$ & A\\
(4) & $\Gamma_1, (\mc{A} \implies (\lnot \mc{B})) \vdash (\lnot \mc{B})$ & $\implies E$, 1, 3\\
(5) & $\Gamma_1, \Gamma_2, (\mc{A} \implies (\lnot \mc{B})) \vdash \curlywedge$ & $\lnot E$, 2, 4\\
(6) & $\Gamma_1, \Gamma_2 \vdash (\lnot (\mc{A} \implies (\lnot \mc{B})))$ & $\lnot I_W$ 5\\
(7) & $\Gamma_1, \Gamma_2 \vdash (\mc{A} \land \mc{B})$ & $W$, 6
\end{tabular}
\end{center}

\hrulefill

\textbf{Rule of $\land I$ (Conjunction Introduction)}

\begin{prooftree}
\AxiomC{$\Gamma_1 \vdash \mc{A}$}
\AxiomC{$\Gamma_2 \vdash \mc{B}$}
\BinaryInfC{$\Gamma_1, \Gamma_2 \vdash (\mc{A} \land \mc{B})$}
\end{prooftree}

\hrulefill
\subsection{Disjunction Inference Rules}
\subsubsection*{Proof for Disjunction Elimination}

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{3cm} }
(1) & $\Gamma_1 \vdash (\mc{A} \lor \mc{B})$ & Prem\\
(2) & $\Gamma_2 \vdash (\mc{A} \implies \mc{C})$ & Prem\\
(3) & $\Gamma_3 \vdash (\mc{B} \implies \mc{C})$ & Prem\\
(4) & $\Gamma_1 \vdash ((\lnot \mc{A}) \implies \mc{B})$ & $W$, 1\\
(5) & $(\lnot \mc{C}) \vdash (\lnot \mc{C})$ & A\\
(6) & $\Gamma_2, (\lnot \mc{C}) \vdash (\lnot \mc{A})$ & $MT$, 2, 5\\
(7) & $\Gamma_1, \Gamma_2, (\lnot \mc{C}) \vdash \mc{B}$ & $\implies E$, 4, 6\\
(8) & $\Gamma_1, \Gamma_2, \Gamma_3, (\lnot \mc{C}) \vdash \mc{C}$ & $\implies E$, 3, 7\\
(9) & $\Gamma_1, \Gamma_2, \Gamma_3, (\lnot \mc{C}) \vdash \curlywedge$ & $\lnot E$, 5, 8\\
(10) & $\Gamma_1, \Gamma_2, \Gamma_3 \vdash (\lnot(\lnot \mc{C}))$ & $\lnot I_W$, 9\\
(11) & $\Gamma_1, \Gamma_2, \Gamma_3 \vdash \mc{C}$ & $DN$, 10
\end{tabular}
\end{center}

\subsubsection*{Proof for Left Disjunction Introduction}

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{3cm} }
(1) & $\Gamma \vdash \mc{A}$ & Prem\\
(2) & $(\lnot \mc{A}) \vdash (\lnot \mc{A})$ & A\\
(3) & $\Gamma, (\lnot \mc{A}) \vdash \curlywedge$ & $\lnot E$, 1, 2\\
(4) & $\Gamma/\{(\lnot \mc{B})\}, (\lnot \mc{A}) \vdash (\lnot (\lnot \mc{B}))$ & $\lnot I$, 3\\
(5) & $\Gamma/\{(\lnot \mc{B})\}, (\lnot \mc{A}) \vdash \mc{B}$ & $DN$, 4\\
(6) & $\Gamma/\{(\lnot \mc{B})\} \vdash ((\lnot \mc{A}) \implies \mc{B})$ & $\implies I_W$, 5\\
(7) & $\Gamma \vdash ((\lnot \mc{A}) \implies \mc{B})$ & $W$, 6\\
(8) & $\Gamma \vdash (\mc{A} \lor \mc{B})$ & $W$, 7
\end{tabular}
\end{center}


\subsubsection*{Proof for Right Disjunction Introduction}

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{3cm} }
(1) & $\Gamma \vdash \mc{B}$ & Prem\\
(2) & $\Gamma/\{(\lnot \mc{A})\} \vdash ((\lnot \mc{A}) \implies \mc{B})$ & $\implies I$, 1\\
(3) & $\Gamma \vdash ((\lnot \mc{A}) \implies \mc{B})$ & $W$, 2\\
(4) & $\Gamma \vdash (\mc{A} \lor \mc{B})$ & $W$, 3
\end{tabular}
\end{center}

With this we introduce the inference rules for disjunction:

\hrulefill

\textbf{Rule of $\lor E$ (Disjunction Elimination)}

\begin{prooftree}
\AxiomC{$\Gamma_1 \vdash (\mc{A}\lor \mc{B})$}
\AxiomC{$\Gamma_2 \vdash (\mc{A} \implies \mc{C})$}
\AxiomC{$\Gamma_3 \vdash (\mc{B} \implies \mc{C})$}
\TrinaryInfC{$\Gamma_1, \Gamma_2, \Gamma_3 \vdash \mc{C}$}
\end{prooftree}

\hrulefill

\textbf{Rule of $\lor I_L$ (Disjunction Introduction - Left)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash \mc{A}$}
\UnaryInfC{$\Gamma \vdash (\mc{A}\lor \mc{B})$}
\end{prooftree}

\hrulefill

\textbf{Rule of $\lor I_R$ (Disjunction Introduction - Right)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash \mc{B}$}
\UnaryInfC{$\Gamma \vdash (\mc{A}\lor \mc{B})$}
\end{prooftree}

\hrulefill

\subsection{Biconditional Inference Rules}
\subsubsection*{Proof for Biconditional Introduction}

\begin{center}
\begin{tabular}{ p{1cm} p{7cm} p{3cm} }
(1) & $\Gamma_1 \vdash (\mc{A} \implies \mc{B})$ & Prem\\
(2) & $\Gamma_2 \vdash (\mc{B} \implies \mc{A})$ & Prem\\
(3) & $\Gamma_1, \Gamma_2 \vdash ((\mc{A} \implies \mc{B}) \land (\mc{B}\implies \mc{A}))$ & $\land I$, 1, 2\\
(4) & $\Gamma_1, \Gamma_2 \vdash (\mc{A} \iff \mc{B})$ & $W$, 3
\end{tabular}
\end{center}

\subsubsection*{Proof for Left Biconditional Elimination}
\begin{center}
\begin{tabular}{ p{1cm} p{7cm} p{3cm} }
(1) & $\Gamma \vdash (\mc{A} \iff \mc{B})$ & Prem\\
(2) & $\Gamma \vdash ((\mc{A} \implies \mc{B}) \land (\mc{B} \implies \mc{A}))$ & $W$, 1\\
(3) & $\Gamma \vdash (\mc{A} \implies \mc{B})$ & $\land E_L$, 2\\
(4) & $\Gamma \vdash (\mc{B} \implies \mc{A})$ & $\land E_R$, 2
\end{tabular}
\end{center}

We get two new inference rules from the last two lines of this proof.
The inference rules for biconditional are

\hrulefill

\textbf{Rule of $\iff I$ (Biconditional Introduction)}

\begin{prooftree}
\AxiomC{$\Gamma_1 \vdash (\mc{A}\implies\mc{B})$}
\AxiomC{$\Gamma_2 \vdash (\mc{B} \implies \mc{A})$}
\BinaryInfC{$\Gamma_1, \Gamma_2 \vdash (\mc{A} \iff \mc{B})$}
\end{prooftree}

\hrulefill

\textbf{Rule of $\iff E_L$ (Biconditional Elimination - Left)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash (\mc{A} \iff \mc{B})$}
\UnaryInfC{$\Gamma \vdash (\mc{A} \implies \mc{B})$}
\end{prooftree}

\hrulefill

\textbf{Rule of $\iff E_R$ (Biconditional Elimination - Right)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash (\mc{A} \iff \mc{B})$}
\UnaryInfC{$\Gamma \vdash (\mc{B} \implies \mc{A})$}
\end{prooftree}

\hrulefill

\subsection{Inference Rules for Existence}

We now must derive the inference rule for existence introduction and elimination.
For reference I repeat the rules for universal introduction and elimination.

\hrulefill

\textbf{Rule of $\forall I$ (Universal Introduction/Generalization)}

\begin{prooftree}
\AxiomC{$_{\{t\}}\Gamma \vdash {_{[t/x], \{t\}}\mc{A}}[t/x]$}
\UnaryInfC{$\Gamma \vdash ((\forall x) \mc{A})$}
\end{prooftree}

\hrulefill

\textbf{Rule of $\forall E$ (Universal Elimination/Instantiation/Specification)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash ((\forall x)_{[t/x]}\mc{A})$}
\UnaryInfC{$\Gamma \vdash \mc{A}[t/x]$}
\end{prooftree}

\hrulefill

The rules for existential introduction and elimination will be

\hrulefill

\textbf{Rule of $\exists I$ (Existential Introduction/Generalization)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash {_{[t/x]}\mc{A}}[t/x]$}
\UnaryInfC{$\Gamma \vdash ((\exists x)\mc{A})$}
\end{prooftree}

\hrulefill

\textbf{Rule of $\exists E$ (Existential Elimination/Instantiation/Specification)}

\begin{prooftree}
\AxiomC{$\Gamma_1 \vdash ((\exists x)\mc{A})$}
\AxiomC{$_{\{t\}}\Gamma_2 \vdash ({_{[t/x], \{t\}}\mc{A}}[t/x] \implies {_{\{t\}}\mc{B}})$}
\BinaryInfC{$\Gamma_1, \Gamma_2 \vdash \mc{B}$}
\end{prooftree}

\hrulefill

\subsubsection*{Proof for Existential Introduction}

\begin{center}
\begin{tabular}{ p{1cm} p{7cm} p{3cm} }
(1) & $\Gamma \vdash {_{[t/x]}\mc{A}}[t/x]$ & Prem\\
(2) & $((\forall x)(\lnot \mc{A})) \vdash ((\forall x)(\lnot \mc{A}))$ & A\\
(3) & $((\forall x)(\lnot \mc{A})) \vdash ((\forall x)(\lnot _{[t/x]}\mc{A}))$ & $W$, 2\\
(4) & $((\forall x)(\lnot \mc{A})) \vdash ((\forall x)_{[t/x]}(\lnot \mc{A}))$ & $W$, 3\\
(5) & $((\forall x)(\lnot \mc{A})) \vdash (\lnot \mc{A})[t/x]$ & $\forall E$, 4\\
(6) & $((\forall x)(\lnot \mc{A})) \vdash (\lnot \mc{A}[t/x])$ & $W$, 5\\
(7) & $\Gamma, ((\forall x)(\lnot \mc{A})) \vdash \curlywedge$ & $\lnot E$, 1, 6\\
(8) & $\Gamma \vdash (\lnot ((\forall x) (\lnot \mc{A})))$ & $\lnot I_W$, 7\\
(9) & $\Gamma \vdash ((\exists x) \mc{A})$ & $W$, 8
\end{tabular}
\end{center}

This proof had some subtlety involving replacement and substitutability.
The inferences from line 1 to line 2 and from line 2 to line 3 follow from the rules for substitutability.
Note that these inferences were labeled as abbreviations.
This is because lines 2, 3, and 4 indicate the same sequents, just with different metalogical annotations.
Similarly, line 6 follows from line 5 by the rules for term replacement.

I repeat the proof above with the metalogical notation carried out implicitly.

\begin{center}
\begin{tabular}{ p{1cm} p{7cm} p{3cm} }
(1) & $\Gamma \vdash \mc{A}[t/x]$ & Prem\\
(2) & $((\forall x)(\lnot \mc{A})) \vdash ((\forall x)(\lnot \mc{A}))$ & A\\
(3) & $((\forall x)(\lnot \mc{A})) \vdash (\lnot \mc{A}[t/x])$ & $\forall E$, 2\\
(4) & $\Gamma, ((\forall x)(\lnot \mc{A})) \vdash \curlywedge$ & $\lnot E$, 1, 3\\
(5) & $\Gamma \vdash (\lnot ((\forall x) (\lnot \mc{A})))$ & $\lnot I_W$, 4\\
(6) & $\Gamma \vdash ((\exists x) \mc{A})$ & $W$, 5
\end{tabular}
\end{center}


\subsubsection*{Proof for Existential Elimination}

\begin{center}
\begin{tabular}{ p{1cm} p{7cm} p{3cm} }
(1) & $\Gamma_1 \vdash ((\exists x)\mc{A})$ & Prem\\
(2) & $_{\{t\}}\Gamma_2 \vdash ({_{[t/x], \{t\}}\mc{A}}[t/x] \implies {_{\{t\}}\mc{B}})$  & Prem\\
(3) & $_{\{t\}}\Gamma_2,  (\lnot {_{\{t\}}\mc{B}}) \vdash (\lnot {_{[t/x], \{t\}}\mc{A}}[t/x])$ & $MT2$, 2\\
(4) & $_{\{t\}} \Gamma_2, {_{\{t\}}(\lnot \mc{B})} \vdash {_{[t/x], \{t\}}(\lnot \mc{A})}[t/x]$ & $W$, 3\\
(5) & $\Gamma_2, (\lnot \mc{B}) \vdash ((\forall x)(\lnot \mc{A}))$ & $\forall I$, 4\\
(6) & $\Gamma_1 \vdash (\lnot ((\forall x)(\lnot \mc{A})))$ & $W$, 1\\
(7) & $\Gamma_1, \Gamma_2, (\lnot \mc{B}) \vdash \curlywedge$ & $\lnot E$, 5, 6\\
(8) & $\Gamma_1, \Gamma_2 \vdash (\lnot(\lnot \mc{B}))$ & $\lnot I_W$, 7\\
(9) & $\Gamma_1, \Gamma_2 \vdash \mc{B}$ & $DN$, 8
\end{tabular}
\end{center}

I repeat this proof with implicit metalogical notation.

\begin{center}
\begin{tabular}{ p{1cm} p{7cm} p{3cm} }
(1) & $\Gamma_1 \vdash ((\exists x)\mc{A})$ & Prem\\
(2) & $\Gamma_2 \vdash (\mc{A}[t/x] \implies \mc{B})$  & Prem\\
(3) & $\Gamma_2,  (\lnot \mc{B}) \vdash (\lnot \mc{A})[t/x]$ & $MT2$, 2\\
(4) & $\Gamma_2, (\lnot \mc{B}) \vdash ((\forall x)(\lnot \mc{A}))$ & $\forall I$, 3\\
(5) & $\Gamma_1 \vdash (\lnot ((\forall x)(\lnot \mc{A})))$ & $W$, 1\\
(6) & $\Gamma_1, \Gamma_2, (\lnot \mc{B}) \vdash \curlywedge$ & $\lnot E$, 4, 5\\
(7) & $\Gamma_1, \Gamma_2 \vdash (\lnot(\lnot \mc{B}))$ & $\lnot I_W$, 6\\
(8) & $\Gamma_1, \Gamma_2 \vdash \mc{B}$ & $DN$, 7
\end{tabular}
\end{center}

These proofs complete the derivation of the inference rules for the existence quantifier.

\section{Theories}

A logical theory $\mc{T}$ consists of a language $\mc{L}$, a set of closed Wffs $\Gamma = \{\gamma_1, \ldots, \gamma_N\}$ in the language $\mc{L}$ which are referred to as the axioms of the theory, and a set of inference rules $\mc{R}$ such as those given above.
A proof in $\mc{T}$ is any proof that begins with $N$ premises of the form $\Gamma \vdash \gamma_i$ and uses the inference rules $\mc{R}$.
If it is possible to prove $\Gamma \vdash \mc{A}$ under these circumstance then we write

$$
\Gamma \vdash_{\mc{T}} \mc{A}
$$

Though we often drop the $\mc{T}$ subscript when the context of the theory is clear.
We would like to collect all of the Wffs which are derivable from $\Gamma$ under the inference rules $\mc{R}$.
If $\Gamma \vdash_{\mc{T}} \mc{A}$ and $\mc{T}$ is the theory with axioms $\Gamma$ and inference rules $\mc{R}$ then we write

$$
\mc{A} \in \Phi_{\mc{T}}
$$

$\Phi_{\mc{T}}$ is the set of all Wffs derivable under theory $\mc{T}$.

\subsection*{Extension}
Suppose $\mc{T}_1$ and $\mc{T}_2$ are theories with languages $\mc{L}_1$ and $\mc{L}_2$, axioms $\Gamma_1$ and $\Gamma_2$ and inference rules $\mc{R}_1$ and $\mc{R}_2$. We say that $\mc{T}_2$ is an extension of $\mc{T}_1$ if $\mc{L}_1 \subset \mc{L}_2$ (this means that $\mc{L}_1$ and $\mc{L}_2$ share their syntax rules but $\mc{L}_2$ has the same, or additional predicate or function symbols compared to $\mc{L}_1$), $\mc{R}_2 = \mc{R}_1$, and $\Gamma_1 \subset \Gamma_2$.
So we see that $\mc{T}_2$ may have more logical symbols than $\mc{T}_1$ and also may have more axioms than $\mc{T}_1$.

\subsection*{Conservative Extension}
If $\mc{T}_2$ is an extension of $\mc{T}_1$ and $\Phi_{\mc{T}_2} \cap \mc{L}_1 = \Phi_{\mc{T}_1}$ then we say that $\mc{T}_2$ is a conservative extension of $\mc{T}_1$.
That is, every Wff which is provable in $\mc{T}_1$ is also provable and $\mc{T}_2$ and every Wff of $\mc{L}_1$ which is provable in $\mc{T}_2$ is also provable in $\mc{T}_1$.

\subsection*{Extension by Definition}
There are two ways we can extend a theory by definition.
We can either introduce a new predicate or new function symbol and a corresponding axiom.
The concept of extension by function definition requires equality $=$ to be included in the language.
We will not yet include equality so we will save the description of that for later.

\subsubsection*{New Predicate Symbol}

Suppose $\mc{T}_1$ is a theory with language $\mc{L}_1$, axioms $\Gamma_1$ and inference rules $\mc{R}$.
Suppose $P$ is not a symbol in $\mc{L}_1$.
Suppose $\mc{Q}$ is a Wff in $\mc{L}_1$ with $FV(\mc{Q}) = \{x_1, \ldots , x_N\}$.
Let $\mc{L}_2$ be the same as $\mc{L}_1$ but with the addition of the $n$-ary predicate symbol $P$.
Let $\nu\in \mc{L}_2$ be the Wff

\begin{align*}
\nu \equiv& ((\forall x_1) (\ldots (\forall x_N) (P(x_1, \ldots, x_N) \iff \mc{Q}) \ldots ))\\
\equiv& \forall x_1,\ldots,x_N (P(x_1, \ldots, x_N) \iff \mc{Q})
\end{align*}

Let $\mc{R}_2 = \mc{R}_1$ and $\Gamma_2 = \Gamma_1 \cup \{\nu\}$.
The theory $\mc{T}_2$ determined by $\mc{L}_2, \Gamma_2$ and $\mc{R}_2$ is an extension by predicate definition of $\mc{T}_1$.


\section{Wff Atom Replacement}

If $\mc{A}$ is a Wff, $\mc{X}$ is an atomic formula, and $\mc{P}$ is a Wff then we define the replacement of $\mc{X}$ by $\mc{P}$ in $\mc{A}$, notated as $\mc{A}[\mc{P}/\mc{X}]$, by:

\hrulefill
\begin{itemize}
\item{If $\mc{A}$ is atomic then if $\mc{A}\equiv \mc{X}$ then $\mc{A}[\mc{P}/\mc{X}] \equiv \mc{P}$. If $\mc{A}\not\equiv \mc{X}$ then $\mc{A}[\mc{P}/\mc{X}] \equiv \mc{A}$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ the $\mc{A}[\mc{P}/\mc{X}] \equiv (\lnot \mc{B}[\mc{P}/\mc{X}])$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $\mc{A}[\mc{P}/\mc{X}] \equiv (\mc{B}[\mc{P}/\mc{X}] \implies \mc{C}[\mc{P}/\mc{X}])$.}
\item{If $\mc{A} \equiv ((\forall x)\mc{B})$ then $\mc{A}[\mc{P}/\mc{X}] \equiv ((\forall x) (\mc{B}[\mc{P}/\mc{X}])$.}
\end{itemize}
\hrulefill

Note that $\mc{A}[\mc{P}/\mc{X}]$ is not necessarily a Wff due to constraints with respect to quantification.
For $\mc{A}[\mc{P}/\mc{X}]$ to be a Wff it is necessary that, if $\mc{P}$ has any quantifier $(\forall x)$ that $\mc{X}$ is not within the scope of any $(\forall x)$ quantifier within $\mc{A}$.

Similarly to term replacement, we must also define a concept of substitutability for atoms. We say $\mc{S}(\mc{A}, \mc{P}, \mc{X})$ to mean that $\mc{P}$ is substitutable for $\mc{X}$ in $\mc{A}$. $\mc{S}(\mc{A}, \mc{P}, \mc{X})$ if (1) if $(\forall x)$ appear in $\mc{P}$ that $\mc{X}$ is not within the scope of any $(\forall x)$ quantifier in $\mc{A}$ and (2) no

\section{Wff Breakdown Tree}

Consider a Wff $\mc{A}$.
In what follows we will be interested in identifying particular Wff substrings of $\mc{A}$ such as $\mc{P}$.
For example, we may be interested in the Wff $\mc{A}$ with one instance of $\mc{P}$ replaced by $\mc{Q}$.
Unfortunately if $\mc{A} \equiv (\mc{P} \implies \mc{P})$, for example, then this sentence is ambiguous.
To resolve this problem we will come up with a rigorous schematic to uniquely specify an instance of a substring Wff in a main Wff.

We first define the overall depth of a Wff which, in some way, encodes the complexity of the Wff.
We will often induct on the depth of a Wff to prove metalogical theorems.

\hrulefill
\subsubsection*{Wff Depth}

\begin{itemize}
\item{If $\mc{A}$ is an atomic formula then $D(\mc{A}) = 0$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ then $D(\mc{A}) = D(\mc{B}) + 1$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $D(\mc{A}) = \text{max}(D(\mc{B}) + D(\mc{C})) + 1$.}
\item{If $\mc{A} \equiv ((\forall x) \mc{B})$ then $D(\mc{A}) = D(\mc{B}) + 1$.}
\end{itemize}
\hrulefill

We now need the ability to break a Wff down into its constituent parts.
The constituent parts of a Wff are contained in its parsing tree $T(\mc{A})$.
We develop the parsing tree $T(\mc{A})$ by applying a recursive algorithm on the Wff.
For any Wff we can find it's children by noticing its main connective, or top-level structure.
The elements of the parsing tree will be 4-tuples.
The first element of the tuple is one of $\{\text{Wff}, A, \lnot, \implies, \forall\}$.
This first element indicates how the sub-part is brought into $\mc{A}$.
The second element is an integer indicating the depth of the part within the overall Wff.
The third element is an integer indicating the branch number of the part within the Wff.
And finally the fourth element is a Wff representing the part.

We extract the children of a top level Wff $\mc{A}$ via $C(\mc{A})$.
If $C(\mc{A})$ is the children tree for $\mc{A}$ then $C_{+n, +m}(\mc{A})$ is the same tree but with $n$ added to the second element of every tuple and $m$ added to the third element of every tuple.


\hrulefill
\begin{itemize}
\item{If $\mc{A}$ is atomic then $C(\mc{A}) = \emptyset$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ then $C(\mc{A}) = \{(\lnot, 1, 1, \mc{B})\} \cup C_{+1, +0}(\mc{B})$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$. Let $b$ equal the maximum branch number in $C(\mc{B})$. then $C(\mc{A}) = \{(\implies_L, 1, 1, \mc{B}), (\implies_R, 1, b+1, \mc{C})\} \cup C_{+1, +0}(\mc{B}) \cup C_{+1, +b}(\mc{C})$.}
\item{If $\mc{A} \equiv ((\forall x)\mc{B})$ then $C(\mc{A}) \equiv \{(\forall, 1, 1, \mc{B})\} \cup C_{+1, +0}(\mc{B})$.}
\end{itemize}
\hrulefill

Finally the total parsing tree for Wff $\mc{A}$ is

$$
T(\mc{A}) = \{(\text{Wff}, 0, 1, \mc{A})\} \cup C(\mc{A})
$$

\subsection{Parsing Tree Unique Numbers}

\subsubsection{Smallest Depth and Branch Number}
Here we will prove that $C(\mc{A})$ contains no depth or branch number less than 1. We prove this by induction on the depth of $\mc{A}$, $D(\mc{A})$.

If $D(\mc{A}) = 1$ the $\mc{A}$ is atomic so $C(\mc{A}) = \emptyset$. There are no depth and branch numbers appearing in $C(\mc{A})$ so the condition is satisfied.

Now for the induction hypothesis we assume that if $D(\mc{B}) < n$ then $C(\mc{A})$ contains no depth or branch numbers less than 1.

Consider $\mc{A}$ with $D(\mc{A}) = n > 1$. $\mc{A}$ must not be atomic since $D(\mc{A}) \neq 1$.

If $\mc{A} \equiv (\lnot \mc{B})$ then $C(\mc{A}) = \{(\lnot, 1, 1, \mc{B})\} \cup C_{+1, +0}(\mc{B})$. Since $D(\mc{B}) < D(\mc{A}) = n$ no depth or branch number smaller than 1 appear in $C(\mc{B})$ by the induction hypothesis. We see then that no depth or branch number smaller than 1 appears in $C(\mc{A})$.

If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $C(\mc{A}) = \{(\implies_L, 1, 1, \mc{B}), (\implies, 1, b+1, \mc{C})\} \cup C_{+1, +0}(\mc{B}) \cup C_{+1, +b}(\mc{C})$ with $b$ the maximum branch number in $C(\mc{B})$. Since $D(\mc{B}), D(\mc{C}) < D(\mc{A}) = n$ no depth or branch number smaller than 1 appears in either $C(\mc{B})$ or $C(\mc{C})$ by the induction hypothesis. We see then that no depth or branch number smaller than 1 appears in $C(\mc{A})$.

If $\mc{A} \equiv ((\forall x)\mc{B})$ then $C(\mc{A}) = \{(\forall, 1, 1, \mc{B})\} \cup C_{+1, +0}(\mc{B})$. Since $D(\mc{B}) < D(\mc{A}) = n$ no depth or branch number smaller than 1 appears in $C(\mc{B})$. We see therefore that no depth or branch number smaller than 1 appears in $C(\mc{A})$.

\subsubsection{Unique Depth and Branch Numbers}

We now prove that the tuple $(n, m)$ specifying the depth and branch number for every element of $C(\mc{A})$ is unique within $C(\mc{A})$.

If $D(\mc{A}) = 1$ then $\mc{A}$ is atomic so $C(\mc{A}) = \emptyset$. There are no elements in $C(\mc{A})$ so the condition is vacuously satisfied.

For the induction hypothesis we assume that if $D(\mc{B}) < n$ then the depth and branch number tuples in $C(\mc{B})$ are unique.

Suppose $D(\mc{A}) = n > 1$. $\mc{A}$ cannot be atomic.

If $\mc{A} \equiv (\lnot \mc{B})$ then $C(\mc{A}) = \{(\lnot, 1, 1, \mc{B})\} \cup C_{+1, +0}(\mc{B})$. Since $D(\mc{B}) < D(\mc{A}) = n$ all of the tuples in $C(\mc{B})$ are unique by the induction hypothesis. There is no depth number smaller than 1 in $C(\mc{B})$ by the previous theorem so there is no depth number in $C_{+1, +0}(\mc{B})$ smaller than 2. This means all of the tuples in $C(\mc{A})$ are unique since $C(\mc{A})$ only adds one tuple, with depth number 1, to $C_{+1, +0}(\mc{B})$.

If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $C(\mc{A}) = \{(\implies, 1, 1, \mc{B}), (\implies, 1, b+1, \mc{C})\} \cup C_{+1, +0}(\mc{B}) \cup C_{+1, +b}(\mc{C})$ with $b$ the maximum branch number in $C(\mc{B})$. Since $D(\mc{B}), D(\mc{C}) < D(\mc{A}) = n$ all of the tuples in $C(\mc{B})$ are unique and all of the tuples in $C(\mc{C})$ are unique by the induction hypothesis. However, there is not yet guarantee against duplication of tuples between $C(\mc{B})$ and $C(\mc{C})$. There is no depth number smaller than 1 in both $C(\mc{B})$ and $C(\mc{C})$ by the previous theorem. Therefore, there is no depth number smaller than 2 in either $C_{+1, +0}(\mc{B})$ or $C_{+1, +b}(\mc{C})$. In particular this means that the set of depth and branch numbers from $\{(\implies, 1, 1, \mc{B})\} \cup C_{+1, +0}$ are unique amongst each other and the set of depth and branch numbers from $\{(\implies, 1, b+1, \mc{C})\} \cup C_{+1, +b}(\mc{C})$ are also unique amongst each other. There is no branch number smaller than 1 appearing in $C(\mc{C})$ so the smallest branch number appearing in the latter set from the previous sentence is $b+1$. This means the branch numbers between the two sets are unique since the former has a largest branch number of $b$ and the latter has a smaller branch number of $b+1$. This concludes the proof that if $\mc{A} \equiv (\mc{B} \implies \mc{C})$ that all depth and branch tuples in $C(\mc{A})$ are unique.

If $\mc{A} \equiv ((\forall x)\mc{B})$ then $C(\mc{A}) = \{(\forall, 1, 1, \mc{B})\} \cup C_{+1, +0}(\mc{B})$. Since $D(\mc{B}) < D(\mc{A}) = n$ all of the tuples in $C(\mc{B})$ are unique by the induction hypothesis. There is no depth number smaller than 1 in $C(\mc{B})$ by the previous theorem so the smallest depth number in $C_{+1, +0}(\mc{B})$ is 2. This means all of the tuples in $C(\mc{A})$ are unique.

\subsubsection{Unique Depth and Branch Numbers for Full Tree}

Since no depth number smaller than 1 appears in $C(\mc{A})$, we have that the depth and branch numbers in

$$
T(\mc{A}) = \{(\text{Wff}, 0, 1, \mc{A})\} \cup C(\mc{A})
$$

are unique.


\subsubsection{Maximum Depth Number is Equal to Wff Depth}

If $\mc{A}$ is atomic then $D(\mc{A}) = 1$ and $C(\mc{A}) = \{(A, 1, 1, \mc{A})\}$ so we see the maximum depth number is 1 as needed.

For the induction hypothesis we assume that if $D(\mc{B}) < n$ then the maximum depth number in $C(\mc{B})$ is equal to $D(\mc{B})$.

If $\mc{A} \equiv (\lnot \mc{B})$ then $D(\mc{A}) = D(\mc{B}) + 1$ and $C(\mc{A}) = \{(\lnot, 1, 1, \mc{B})\} \cup C_{+1, +0}(\mc{B})$. The maximum depth number in $C(\mc{B})$ is $D(\mc{B})$ by induction hypothesis so the maximum depth number in $C(\mc{A})$ is $D(\mc{B}) + 1 = D(\mc{A})$ as needed.

If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $D(\mc{A}) = \text{max}(\mc{B}, \mc{C}) + 1$. The maximum depth number in $C(\mc{B})$ is $D(\mc{B})$ and the maximum depth number in $C(\mc{C})$ is $D(\mc{C})$ by the induction hypothesis. The maximum depth number in $C(\mc{A})$ is then $\text{max}(D(\mc{B}) + 1, D(\mc{C})+1) = \text{max}(D(\mc{B}), D(\mc{C})) + 1$ as needed.

If $\mc{A} \equiv ((\forall x)\mc{B})$ then $D(\mc{A}) = D(\mc{B}) + 1$. The maximum depth number in $C(\mc{B})$ is $D(\mc{B})$ by the induction hypothesis so the maximum depth number in $C(\mc{A})$ is $D(\mc{B}) + 1$ as needed.

It is clear than the maximum depth number in $T(\mc{A})$ is also equal to $D(\mc{A})$.

\newpage

\section{Proof for Equivalent Wff Replacement}

Consider the closed Wff $\mc{A}$.
Suppose the Wff $\mc{P}$ is a part of $\mc{A}$. By this we mean that $\mc{P}$ appears as the fourth element of one of the tuples listed in $T(\mc{A})$. In this case we say $\mc{P} \in T(\mc{A})$ (with a slight abuse of notation).
Suppose we have another Wff $\mc{Q}$ with $FV(\mc{Q}) \subset FV(\mc{P})$ and we have

$$
\nu \vdash ((\forall x_1)(\ldots((\forall x_n) (\mc{P} \iff \mc{Q})))).
$$

Let $\mc{A}' \equiv B\mc{Q}C$. I will now prove that $\nu \vdash (\mc{A} \iff \mc{A}')$.
For this I will need to define the depth of a Wff, $D(\mc{A})$.



We will prove the desired theorem by metalogical strong induction on the depth of the Wff $\mc{A}$.
That is, we will prove that the theorem holds in the case when $D(\mc{A}) = 1$.
Then, if $D(\mc{A}) = n$, we will assume the theorem holds for all integers $<n$ and the prove again that the theorem holds by $n$.
In this way we will be assured the theorem holds for all Wff depths.

\subsection*{Base Case}

If $D(\mc{A}) = 1$ then $\mc{A}$ is an atomic Wff.
Since $\mc{A}$ contains $\mc{P}$ as a substring $\mc{P}$ must also be atomic and we must have $\mc{A} \equiv \mc{P}$.
We then also have that $\mc{A}' \equiv \mc{Q}$.
Note that we must have $FV(\mc{P}) = FV(\mc{Q}) = \emptyset$ since $\mc{A}$ is an atomic closed Wff.
It is then the case that $(\mc{A} \iff \mc{A}') \equiv (\mc{P} \iff \mc{Q})$, but we already have that $\nu \vdash (\mc{P} \iff \mc{Q})$.

\subsection*{Induction Step}

Suppose $D(\mc{A}) = n$.
We have that
$$
\nu \vdash ((\forall x_1)(\ldots((\forall x_n) (\mc{P} \iff \mc{Q})))).
$$
We assume then that, for any Wff $\mc{B}$ with $D(\mc{B}) < n$ which contains $\mc{P}$ as a substring that we also can derive
$$
\nu \vdash (\mc{B} \iff \mc{B}')
$$
where $\mc{B}'$ is the result of replacing $\mc{P}$ by $\mc{Q}$ as above.




\section{Proof that Extension by Predicate Definition is a Conservative Extension}

Suppose $\mc{T}_2$ is an extension by predicate definition of $\mc{T}_1$ as in the previous section.
We will now prove that $\mc{T}_2$ is a conservative extension of $\mc{T}_1$.
For this we must prove that

$$
\Phi_{\mc{T}_2} \cap \mc{L}_1 = \Phi_{\mc{T}_1}
$$

Suppose $\mc{A} \in \Phi_{\mc{T}_1}$.
Then $\Gamma_1 \vdash_{\mc{T}_1} \mc{A}$.
We can apply the weakening inference rule ($W$) in $\mc{T}_2$ to then prove that $\Gamma_2 = \Gamma_1 \cup \{\nu\} \vdash_{\mc{T}_2} \mc{A}$.
This means $\mc{A} \in \Phi_{\mc{T}_2}$.
Since $\mc{A} \in \Phi_{\mc{T}_2}$ and $\mc{A} \in \mc{L}_1$ this means that

$$
\Phi_{\mc{T}_1} \subset \Phi_{\mc{T}_2} \cap \mc{L}_1
$$

The challenge for this proof is to prove that

$$
\Phi_{\mc{T}_2} \cap \mc{L}_1 \subset \Phi_{\mc{T}_1}
$$

Suppose $\mc{A} \in \Phi_{\mc{T}_2} \cap \mc{L}_1$.
This means that $\mc{A} \in \mc{L}_1$ and $\mc{A} \in \Phi_{\mc{T}_2}$.
Because $\mc{A} \in \Phi_{\mc{T}_2}$ so that there is a proof tree in $\mc{T}_2$ concluding $\Gamma_2 \vdash_{\mc{T}_2} \mc{A}$.
Our goal is to prove that $\Gamma_1 = \Gamma_2 \backslash \{\nu\} \vdash_{\mc{T}_1} \mc{A}$.

We will prove this in a few steps.
First we must define the concept of the translation of a Wff from $\mc{L}_2$ into $\mc{L}_1$.
Suppose $\mc{A} \in \mc{L}_2$.
We then define $\mc{A}^\# \in \mc{L}_1$ by induction.

\hrulefill
\subsubsection*{Wff Translation}
\begin{itemize}
\item{\textbf{Atomic Formulas:} If $\mc{A}$ is an atomic formula then if $\mc{A} \equiv Pt_1\ldots t_n$ then $\mc{A}^\# \equiv \mc{Q}$}
\end{itemize}
\hrulefill


We will accomplish this by metalogically deconstruction of the $\mc{T}_2$ proof of $\Gamma_2 \vdash_{\mc{T}_2} \mc{A}$.
This proof has length $N$.
We will prove by metalogical induction on proof length that if $\Gamma_2 \vdash_{\mc{T}_2} \mc{A}$ via a proof of length $N$, then it is possible to find a proof of $\Gamma_1 \vdash_{\mc{T}_1} \mc{A}$ (of unspecified length).

Suppose we have a proof that concludes $\Gamma \vdash_{\mc{T}} \mc{A}$.
We define the length of the proof to be the line number on which $\Gamma \vdash_{\mc{T}} \mc{A}$ appears minus the number of premises.
This means that if $\mc{A}$ is an axiom of $\mc{T}$ then it is possible to prove $\Gamma \vdash_{\mc{T}} \mc{A}$ in zero steps.

For induction we first work with a proof of length zero as our base case.
If $\Gamma \vdash_{\mc{T}_2} \mc{A}$ in zero steps then $\mc{A}$ is an axiom of $\mc{T}_2$.
This means that $\mc{A} \in \Gamma \cup \{\nu\}$

\end{document}


