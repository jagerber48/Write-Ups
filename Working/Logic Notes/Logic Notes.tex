\documentclass[12pt]{article}

\usepackage{amssymb, amsmath, amsfonts}
\usepackage{mathtools}
\usepackage{ND}
\usepackage{tcolorbox}
\usepackage{bbm}
\usepackage[utf8]{inputenc}
\usepackage{subfigure}%ngerman
%\usepackage[pdftex]{graphicx}
\usepackage{textcomp} 
\usepackage{color}
\usepackage[hidelinks]{hyperref}
\usepackage{anysize}
\usepackage{verbatim}
\usepackage{float}
\usepackage{braket}
\usepackage{xfrac}
\usepackage{array, booktabs} 
\usepackage{tabularx}
\usepackage{bussproofs}

\usepackage{amsthm}

\newtheoremstyle{break}% name
  {}%         Space above, empty = `usual value'
  {}%         Space below
  {\itshape}% Body font
  {}%         Indent amount (empty = no indent, \parindent = para indent)
  {\bfseries}% Thm head font
  {}%        Punctuation after thm head
  {\newline}% Space after thm head: \newline = linebreak
  {}%         Thm head spec
\theoremstyle{break}
\newtheorem{definition}{Definition}[section]
\theoremstyle{break}
\newtheorem{theorem}{Theorem}[section]
\theoremstyle{break}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{break}
\newtheorem{lemma}[theorem]{Lemma}


\newcommand{\bv}[1]{\boldsymbol{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\bc}[1]{\bv{\mc{#1}}}
\newcommand{\qq}[1]{``#1''}

\begin{document}
\title{Logic Notes}
\author{Justin Gerber}
\date{\today}
\maketitle

\section*{Old Introduction}

In this document I will try to get straight some of my thoughts on formal logic. At the moment the goal is to build up to a metalogic proof, in an extended Lemmon system of logic, that extensions by definition are conservative extensions to a formal theory. A lot of the difficulty comes from 1) the fact that the Lemmon system does not seem to be heavily used in the logic community so there are not really references; when syntactic (as opposed to semantic) proofs are given they often use the Hilbert system and 2) many references use semantic arguments to complete the proof (such as the completeness theorem and model theory generally) and I would like to see a purely syntactical proof.

I am following two references. ``Modern Logic: A Text in Elementary Symbolic Logic'' by Graehme Forbes and ``Lectures in Logic and Set Theory: Volume I'' by George Tourlakis. The former is my reference on Lemon logic and the rules therein while The Tourlakis book is more rigorous and does a cleaner job introducing ideas with the metalanguage or metatheory including the concept of inducting on formulas using the metalanguage. However, Tourlakis uses the Hilbert system and semantic proofs in cases which is why I need to adapt both approaches.

\section*{Preface}
The previous introduction was written around February 2019, the last time I took up investigations into formal logic. I'm revisiting formal logic again (Aug 2021). 
Typically I find myself looking into formal logic after the following sequence of events. 
1) I am interested in some physics topic, typically somewhat mathematically oriented in nature, 2) I look into the math supporting this topic, 3) I get curious about the deep mathematical definitions or theorems involved in the math topic 4) I get into very deep math subjects such as topology or the very definitions of functions 5) I finally find myself back facing formal logic.

Back in 2019 I believe I was interested in the relationship between classical and quantum random variables. 
This led me to look into mathematical formulations of probability theory which took me eventually to measure theory.
In learning measure theory I was attempting to learn about Borel sets and some basic topology again.
I believe I then got curious about the definition of infinite unions and this brought me down to formal logic.
I was then curious at the time about how to build from ZFC up to larger mathematical theories.

Such an undertaking requires the extension by definition of a logical theory.
Extension by definition of a logical theory involves introducing a new symbol to the language and adding a ``defining'' axiom for that logic into the theory.
For appropriate defining axioms the new symbol should not change the theory in the sense that the extension of the theory is conservative.
An extension of a theory is conservative if a formula of the new language, which is a valid formula of the old language, can be deduced in the new theory exactly when it can be deduced in the old theory.
The challenge I faced last time was trying to prove the claim that appropriate extensions by definition are conservative.

I learned a lot about formal logic and had some luck, but the approach I was taking ended up being too tedious.
Two references I utilized addressed the problem: \textit{Lectures in Logic and Set Theory: Volume I} by George Tourlakis and \textit{Introduction to Mathematical Logic} by Elliott Mendelson.
However, both of these books were based on Hilbert style formal logic.

At the time I found this frustrating.
My background in formal logic (from a course I took my freshman year in college in 2009) was from an introductory textbook called \textit{Modern Logic: A Text in Elementary Symbolic Logic} by Graehme Forbes.
This book used a natural deduction approach for proofs and visualized proofs using a Lemmon logic tabular proof style.
I found this latter style to be very natural, and, combined with self study on the Zermelo-Fraenkel axioms of set theory, I, at least subconsciously, understood that it would be possible to put all of mathematics into this mathematical formalism.

I found the axiomatic approaches in the Hilbert style logic to be unnatural and annoying.
A theory didn't seem elegant when all tautologies are assumed as axioms.
It was much more natural to me to have a theory which has no ``logical'' axioms, but from which tautologies could be derived via the rules of inference.
This is of course the same story as the origin of Gentzen's natural deduction.

I have recently revisited this problem.
The physics problem I was trying to understand is how to derive the 3D multipole vector fields describing electromagnetic radiation from multipole charge and current distributions.
This led me to try to understand some theorems about differential equations.
In learning about existence theorems for solutions to, for example, the Laplace equation, i was directed towards, simultaneously, it seems, complex analysis and the fundamental theorem of algebra. 
In proving the fundamental theorem of algebra I found myself visiting some familiar topics in multivariable calculus.
At this time I hit upon a confusion/frustration that has cropped its head up for me time and time again.
This frustration regards the notation for partial derivatives.

We often have, for example $\partial f/\partial x$.
It is implicitly understood that this means differentiation with respect to the `first parameter' of the function $f$.
In cases of nested functions the notation gets complicated, confusing, and sometimes ambiguous so I seek a rigorous definition. 
A long story short, this led me to try to understand a set theoretic definition of a multivariable function.
Finally, in the context of ZFC set theory, function notation arises from symbols which have been added to the theory via extensions by definition.
This brought me back to formal logic.

I walked over many of the tracks I had explored before.
One thing I understood then and I understand now is that much of my difficulty in developing the proof I was interested in was that I did not have a formal enough definition of the rules of inference.
I was relying on the rules of inference as they were laid out in the Graehme textbook.
Unfortunately the definitions there were too heavily tied to the Lemmon tabular proof structure making it very difficult to reason generally about proof structure.

Just recently I came to a more thorough understanding of the sequent calculus.
I believe that the sequent calculus provides rigorous enough definitions of inference rules for me to complete the proof I am interested in now.
Additionally, I've hit upon a reference which is close to the flavor of formal logic I am interested in.
This is \textit{Structural Proof Theory} by Sara Negri and Jan Von Plato.
In fact, I learned that exactly what I am trying to do is a topic in the field of structural proof theory.

One of the major difficulties I found when trying to proof extensions by definition are conservative in the past is the fact that natural deduction logic typically has many inference rules.
When developing an inductive metalogic proof it is necessary to induct over all of these different rules making the proof extremely tedious.
To this end I became very curious if any of the inference rules are redundant.
That is, it is known that some logical connectives can be written in terms of others.
For example: $\mc{A}\land\mc{B} \equiv \lnot(\mc{A}\implies \lnot\mc{B})$.
If the rules of inference involving $\land$ could be derived from the rules of inference for $\implies$ and $\lnot$ then it would be possible to dispense with the $\land$ symbol as a native part of the language and simply maintain it as an abbreviation for the above expression, also taking the corresponding inference rules as derived rules.
The would reduce the number of cases over which we need to induct for the metalogical proofs that follow.
I believe this program is possible and that will be part of this document.

Another difficulty I faced was that the rules of inference involving quantifiers sometimes involve various restrictions on the formulas which are being replaced.
Without a nice way to notate these restrictions I've found it difficult to work with these rules in metalogical proofs.
I hope that in this revisit I'll be able to overcome this difficulty.


\newpage
\section{Introduction}

\subsection{Formal Logic and Language}
All of traditional mathematics can be expressed in terms of mathematical set theory. 
Set theory itself is expressed in terms of a formal logic called first-order logic \textbf{FOL} which is expressed in the language of first-order logic \textbf{LFOL} .
The \textbf{LFOL} is a written formal language which means that it is composed of a clearly and well-defined set of symbols which constitutes the alphabet of that language and clear rules of syntax which identify `appropriate' ways in which the symbols can be combined to form well-formed formulas, or Wffs.
In addition to \textbf{LFOL}, \textbf{FOL} includes a deductive calculus based on inference rules which allows us to derive new Wffs from old Wffs in a way which will be made clear below.
\textbf{FOL} is fully captured by the language in which it is expressed, \textbf{LFOL}, and the inference rules which allow for deduction within the logic.

A formal theory within a formal logic begins with a set of axioms which are a subset of the Wffs of the language. 
It is then possible, using the inference rules, to derive new formulas from these axioms using the inference rules.
The main question we ask about a formal theory is which Wffs can be proven from the given axioms?

\subsection{Syntax and Semantics}
There are two important topics in formal logic: syntax and semantics.
Syntax pertains to the formal rules listed above such as which symbols are in the alphabet, how can they be combined to form Wffs, and what are the inference rules which allows us to manipulate Wffs to derive new ones.
Semantics is the task of assigning meaning to Wffs.
In its simplest form semantics is the assigning of a truth value to Wffs.

Let's consider an example.
Suppose we would like to formalize the statement: If $x$ is an integer and $x$ is odd then $x \div 2$ is an integer.

$$
((\forall x)((\in x \mathbb{N} \land Ox)\implies \in \div x 2 \mathbb{N}))
$$

The smallest element in our formal language is the \textit{term}.
A term is one of three things (1) a concrete object such as the number \qq{2}, (2) a variable into which concrete objects can be `plugged in' such as \qq{x}, or (3) an $n$-ary function of other terms such as \qq{$\div$}.
In fact, below we will express a concrete object as a 0-ary function.
The terms in the above expression are:

\begin{align}
& x \\
& \mathbb{N} \\
& x \\
& \div x 2 \\
& x \\
& 2 \\
& \mathbb{N}
\end{align}

Here I have listed every term as it appears from left to right.
We can see that some terms appear multiple times in the formula.
The only variable which appears is \qq{$x$}.
We see \qq{2} and \qq{$\mathbb{N}$} appearing as 0-ary functions.
We see \qq{$\div$} appearing as a 2-ary function with arguments \qq{$x$} and \qq{2} which are a variable and 0-ary function respectively.
Note that in our formal language we always use prefix notation for functions and predicates so we symbolize \qq{$\div x2$} rather than \qq{$x \div 2$} as we would see in typical infix notation.

The next larger element in our formal language is the \textit{atomic formula}.
An atomic formula is the combination of an $n$-ary predicate symbol and $n$ terms.
We have 2 predicate symbols in the above formula: \qq{$\in$} is a 2-ary predicate and \qq{$O$} is a 1-ary predicate symbol.
The atomic formulas are then

\begin{align}
\mc{A} \equiv& \in x \mathbb{N}\\
\mc{B} \equiv& Ox\\
\mc{C} \equiv& \in \div x2 \mathbb{N}
\end{align}

Here $\mc{A}$, $\mc{B}$, and $\mc{C}$ are metalanguage symbols which stand for the corresponding Wffs on the right hand sides of the $\equiv$ symbol.

Finally, the next larger object in our language is the Wff.
Atomic formulas themselves are already Wffs, but we can form larger more complex Wffs by stringing together smaller Wffs with connectives such as \qq{$\land$} and \qq{$\implies$} or quantifying with $(\forall x)$.
We see sub-Wffs above:

\begin{align}
& ((\forall x)((\in x \mathbb{N} \land Ox)\implies \in \div x 2 \mathbb{N})) \\
& ((\in x \mathbb{N} \land Ox)\implies \in \div x 2 \mathbb{N}) \\
& (\in x \mathbb{N} \land Ox)\\
& \in x \mathbb{N}\\
& Ox \\
&  \in \div x 2 \mathbb{N}
\end{align}

The above discussion has been a purely syntactic analysis of the given Wff.
That is, we broke down the Wff into smaller parts of the formal language.
We will see below that this Wff is, in fact, a Wff according to the formal rules of syntax.

Semantics comes in when we give an \textit{interpretation} to the various expressions above.
For example, when we \textit{interpret} the symbol \qq{2} as the number 2, the expression \qq{$\div x 2$} as the division of the variable $x$ by the number 2, or predicate \qq{$Ox$} to mean that the variable $x$ is odd, we are imposing semantics onto the expression.
We also are making a semantic interpretation any time we impose or deduce a truth value for a given Wff.
For example, on all of the usual interpretations, we would assign a value of false to the Wff above because the odd integers are in fact, not divisible by 2.

In this document I would like to follow a game formalist approach to formal logic.
This is an approach which holds that it is possible to describe set theory, and by extension, all of mathematics using only syntactic methods.
Or, at the very least, it asks the question how far one can get using such an approach.
To that end, I would like it to be clear that all definitions and arguments should be self-contained within purely syntactic analyses.
At a few times I may make reference to semantic ideas, but this is only to motivate the definition.
One could, instead, proceed by dropping semantics entirely from the narrative.
In this case some definitions may appear unmotivated, at least at first, but that is not, technically, a problem for the game formalist.

\subsection{Outline}

The goal of this work is to prove that extensions by definition to a formal theory are conservative extensions.

I will begin with a thorough outlining of \textbf{LFOL}, starting with the alphabet, and then moving onto syntax.
In addition, some useful metalanguage concepts such as term substitution will be defined.
The conventions chosen in my definitions for \textbf{LFOL} are chosen with the goal of easing the proof about definition by extension to follow.
For example, to minimize metalogical inductive cases, I restrict to using a small subset of connectives, rather than the full intuitive set sometimes chosen.

After \textbf{LFOL} is laid out we move into proof theory.
We being by describing sequents and follow with the concept of a formal proof and our corresponding rules of inference.
We then derive a number of derived inference rules, including inference rules for the abbreviated symbols which were not included natively in the language.

Next I will define a logical theory based on a set of axioms.
At this point I will then define a conservative extension to a theory, and an extension by definition to a theory.
There are two types of extensions by definition.
We either define new predicate symbols, or new function symbols.
In the remainder of this document I will prove that these two forms of extensions by definition to a logical theory are in fact conservative extensions.
These proofs will utilize metalogical induction.

\newpage
\section{The Language of First Order Logic (LFOL)}

At the basis of a formal language is a set of symbols which can be concatenated together to form strings.
If a string follows certain allowed construction rules, specified by the syntax of the language, then the string is said to be a well-formed formula, or Wff.

There are multiple classes of symbol objects, each of which plays a different role in the syntax of the language. 
We first distinguish between logical and non-logical symbols. 
Logical symbols are symbols which are related purely to the formal presentation of the language.
Logical symbols include things like logical connectives and parentheses.
The second main class of symbols is the class of non-logical symbols.
The non-logical symbols include the predicate and function symbols.
The symbols included in the non-logical symbols may vary from one application of the formal language to the next.


\subsection{The Lexicon of LFOL:}

\begin{definition}[Logical Symbols]
The logical symbols are defined as follows.
\begin{itemize}
\item{\textbf{Variables:} Variable symbols such as $x_1, x_2, \ldots, v_1, v_2, \ldots$. We will try to use lower case letters from near the end of the alphabet for variables. The set of all variables is denoted $\textbf{Var}$.}
\item{\textbf{Logical Operators:} The logical operators for implication: $\implies$, and negation: $\lnot$. The set of all logical operators is denoted $\textbf{Lop}$.}
\item{\textbf{Quantifiers:} The for all quantifier: $\forall$. The set of all quantifiers is denoted $\textbf{Quant}$.}
\item{\textbf{Punctuation Marks:} Parenthesis $($ and $)$. The set of all punctuation marks is denoted $\textbf{Punct}$.}
\end{itemize}


The set of all logical symbols is denoted $\textbf{LogSymb}$ and is the disjoint union of the variables, logical operators, quantifiers and punctuation marks: $\textbf{LogSymb} = \textbf{Var} \sqcup \textbf{Lop} \sqcup \textbf{Quant} \sqcup \textbf{Punct}$.
\end{definition}

We have chosen to use a restricted set of logical operators in the theory to the exclusion of the conjunction, $\land$, disjunction, $\lor$, and equivalence, $\iff$ connectives and the existential quantifier, $\exists$.
This reduced set of logical symbols will reduce the number of cases over which we need to induct for metalogical proofs. 
However, we will need to introduce the $\land$, $\lor$, and $\iff$ symbols as abbreviations and metalogically derive the appropriate corresponding rules of inference.

\begin{definition}[Non-Logical Symbols]
The non-logical symbols are defined as follows.
\begin{itemize}
\item{\textbf{Predicate Symbols:} for each arity $0 \le n \le N_{pred}$ predicate symbols such as $P, Q, R$. Examples are $\in, \subset, <$. These example all have arity 2 but other aritys are possible. The contradiction symbol, $\curlywedge$, is taken to be a 0-ary predicate in our language. The set of all $n$-ary predicate symbols is denoted $\textbf{Pred}_n$. The set of all predicate symbols is denoted $\textbf{Pred}$ and is the disjoint union of the sets of all $n$-ary predicate symbols: $\textbf{Pred} = \bigsqcup_{i=1}^{N_{\text{pred}}} \textbf{Pred}_i$.}
\item{\textbf{Function Symbols}: for each arity $0 \le n \le N_{\text{func}}$ function symbols such as $f, g, h$. Examples are $+$ and $\times$, these examples both have arity $2$. Note that function symbols of arity $0$ are the same as constants such as $a,b,c,\emptyset,4$. The set of all $n$-ary function symbols is denoted $\textbf{Func}_n$. The set of all function symbols is the disjoint union of the sets of all $n$-ary function symbols and is denoted $\textbf{Func} = \bigsqcup_{n=1}^{N_{\text{func}}} \textbf{Func}_i$.}
\end{itemize}

The set of all non-logical symbols is denoted $\textbf{NonLogSymb}$ and is the disjoint union of the predicate and function symbols: $\textbf{NonLogSymb} = \textbf{Pred} \sqcup \textbf{Func}$. If we are not worried about having an infinite number of distinct symbol types then we can let $N_{\text{pred}} = N_{\text{func}} = \infty$.
\end{definition}

\begin{definition}[The Set of All Symbols]
The set of all symbols in the language is denoted $\textbf{Symb}$ and is the disjoint union of the logical and non-logical symbols: $\textbf{Symb} = \textbf{LogSymb} \sqcup \textbf{NonLogSymb}$.
\end{definition}

\subsection{Strings}
\subsubsection{Definition of and Basic Facts about Lists}
The symbols in $\textbf{Symb}$ can be combined into collections of symbols called strings.
Formally, we will define strings as finite ordered lists of symbols.
Most of this section will be devoted to defining and describing finite ordered lists.

\begin{definition}[Natural Number Subset]
Let $[n]$ denote the (possibly empty) set of natural numbers (including 0) which are less than $n$
\end{definition}

As an example we have that $[5] = \{0, 1, 2, 3, 4\}$.
Note that $[0] = \{\} = \emptyset$.

\begin{definition}[Finite Ordered List]
If $\Sigma$ is any set of objects then a length $n$ finite ordered list of objects in $\Sigma$ is a functional mapping from $[n]$ to $\Sigma$.\footnote{When we use the term `list' below we always mean `finite ordered list'.}  The collection of all lists over a set of objects $\Sigma$ is denoted by $\Sigma^*$.
\end{definition}


An example of a list of elements of $\textbf{Symb}$ defined above is is
\begin{align*}
[&[0, x_1],\\
&[1, \lnot],\\
&[2, Q],\\
&[3, \curlywedge],\\
&[4, (]]
\end{align*}
This list could also be written as
$$
[[0, x_1], [1, \lnot], [2, Q], [3, \curlywedge], [4, (]]
$$
We can also abbreviate the list expression by excluding the explicit inclusion of the numbering:
$$
[x_1, \lnot, Q, \curlywedge, (]
$$

\begin{definition}[List Length]
If $\mc{L}$ is a mapping from $[n]$ into a set of objects $\Sigma$ then we say that $\mc{L}$ is a length $n$ list of elements of $\Sigma$.
We denote the length of the list by $|\mc{L}| = n$.
Note that $|\mc{L}| \ge 0$.
\end{definition}

The natural number length of a list is a fundamental part of its definition in this formal setting.

\begin{definition}[Symbol Equality]
If $\alpha, \beta \in \Sigma$ and $\alpha$ and $\beta$ represent the same object then we may write $\alpha \equiv \beta$.
Otherwise, if $\alpha$ and $\beta$ represent different objects we may write $\alpha \not \equiv \beta$.
\end{definition}

\begin{definition}[List Element Extraction]
Suppose $\mc{L}$ is a list with $|\mathcal{L}| = n$.
For all $i$ with $0 \le i < n$ we denote the $i^{\text{th}}$ element of $\mc{L}$ (with 0-indexing) by $\mc{L}[i]$.
\end{definition}

For example, in the list above we have $\mc{L}[2] \equiv Q$.

\begin{definition}[List Equality]
If $\mc{A}$ and $\mc{B}$ are two lists which satisfy $|\mc{A}| = |\mc{B}| = n$ and for all $0 \le i < n$ we have $\mc{A}[i] \equiv \mc{B}[i]$ then we say that $\mc{A}$ and $\mc{B}$ are the same list and we may write $\mc{A} \equiv \mc{B}$.
\end{definition}

Recall that a list can be a mapping from $[0] = \emptyset$.

\begin{definition}[Empty Lists]
If $\mc{L}$ is a mapping from the empty set $[0] = \emptyset$ then we say $\mc{L}$ is an empty list.
We can see that if $\mc{L}$ is an empty list then $|\mc{L}| = 0$.
An empty list could be written as $[]$.
\end{definition}

\begin{theorem}[The Empty List is Unique]
Suppose $\mc{E}$ and $\mc{E}'$ are both empty lists.
We have that $|\mc{E}| = |\mc{E}'| = 0 $.
Since there is no $i$ satisfying $0\le i < 0$ we need not check equality for any elements of these empty lists.
Therefore $\mc{E} \equiv \mc{E}'$.
\end{theorem}

\begin{definition}[The Empty List]
Because of the uniqueness of the empty list we are justified in defining a notation for it. The empty list can be expressed as $[]$ and we define $\mc{E} \equiv []$.
\end{definition}

\subsubsection{List Concatenation}
If we are given two lists, $\mc{A}$ and $\mc{B}$, we can define a third list by concatenating these two lists.

\begin{definition}[List Concatenation]
Suppose $\mc{A}$, $\mc{B}$, and $\mc{C}$ are lists.
If $\mc{C}$ satisfies
\begin{enumerate}
\item{$|\mc{C}| = |\mc{A}| + |\mc{B}|$}
\item{for all $i$ with $0 \le i < |\mc{A}|$ we have $\mc{C}[i] \equiv \mc{A}[i]$}
\item{for all $i$ with $0 \le i < |\mc{B}|$ we have $\mc{C}[|\mc{A}| + i] \equiv \mc{B}[i]$}
\end{enumerate}
Then we say $\mc{C}$ is the concatenation of $\mc{A}$ and $\mc{B}$ and we write
$$
\mc{C} \equiv \mc{A}\circ \mc{B}
$$
We often abbreviate $\mc{A}\circ\mc{B}$ as $\mc{A}\mc{B}$.
Note that such a $\mc{C}$ always exists.
\end{definition}

\begin{theorem}[Equality of Concatenation with list implies Equality of Lists]
Suppose $\mc{A}\mc{B} \equiv \mc{A}\mc{B}'$. 
Then $\mc{B} \equiv \mc{B}'$.
First, $|\mc{A}\mc{B}| = |\mc{A}| + |\mc{B}| = |\mc{A}\mc{B}'| = |\mc{A}| + |\mc{B}'|$ so $|\mc{B}| = |\mc{B}'| = n$.
Choose $0\le i < n$.
By the definition of concatenation we have $\mc{A}\mc{B}[|\mc{A}| + i] \equiv \mc{A}\mc{B}'[|\mc{A}| + i] \equiv \mc{B}[i] \equiv \mc{B}'[i]$.
Since this holds for $0\le i < n$ we have $\mc{B} \equiv \mc{B}'$.

Now suppose $\mc{A}\mc{B}\equiv \mc{A}'\mc{B}$. Then $\mc{A}\equiv \mc{A}'$.
First $|\mc{A}\mc{B}| \equiv |\mc{A}| + |\mc{B}| = |\mc{A}'\mc{B}| = |\mc{A}'\mc{B}| = |\mc{A}'| + |\mc{B}|$ so $|\mc{A}| = |\mc{A}'| = n$. Consider $0\le i < n$.
By the definition of concatenation we have $\mc{A}\mc{B}[i] \equiv \mc{A}'\mc{B}[i] \equiv \mc{A}[i] \equiv \mc{A}'[i]$.
Since this holds for $0 \le i < n$ we have $\mc{A} \equiv \mc{A}'$.


\end{theorem}

\begin{theorem}[Concatenation with the Empty List]
Here we prove that $\mc{E}\mc{B} \equiv \mc{B} \mc{E} \equiv \mc{B}$.

For the first case, suppose $|\mc{B}|= 0$ so that $\mc{B}\equiv \mc{E}$.
We can check that the set $\mc{E}$ satisfies the conditions to be the concatenation $\mc{E}\mc{E}$. 
First $|\mc{E}| = 0 = 0 + 0 = |\mc{E}| + |\mc{E}|$. 
Next, there is no $i$ satisfying $0 \le i < |\mc{B}|$ or $0 \le i < |\mc{E}|$ so the two element equality conditions for concatenation are vacuously satisfied. 
Therefore $\mc{E}\mc{E} \equiv \mc{E}\mc{B} \equiv \mc{B}\mc{E} \equiv \mc{E}$.

Consider $|\mc{B}|>0$ .

Let $\mc{C} \equiv \mc{E}\mc{B}$. 
First we see that $|\mc{C}| = |\mc{E}| + |\mc{B}| = |\mc{B}|$.
By the definition of concatenation we have that for $0 \le i < |\mc{B}|$ that $\mc{C}[|\mc{E}| + i] \equiv \mc{C}[i] \equiv \mc{B}[i]$. 
This means that $\mc{E}\mc{B}\equiv\mc{B}$.

Let $\mc{C} \equiv \mc{B}\mc{E}$. 
Again $|\mc{C}| \equiv |\mc{B}|+|\mc{E}| = |\mc{B}|$. 
By the definition of concatenation we have that for $0 \le i < |\mc{B}|$ that $\mc{C}[i] \equiv \mc{B}[i]$. 
This means $\mc{B}\mc{E} \equiv \mc{B}$.
\end{theorem}


\begin{theorem}[List Concatenation is Associative]
\label{thm:concatassoc}
Here we will prove that $\Braket{\mc{A}\circ \mc{B}}\circ \mc{C} \equiv \mc{A} \circ \Braket{\mc{B} \circ \mc{C}}$.
The angle brackets here denote the order of operations for the multiple concatenations.

First, if $\mc{A} \equiv \mc{E}$ then $\Braket{\mc{A}\mc{B}}\mc{C} \equiv \mc{B}\mc{C} \equiv \mc{A}\Braket{\mc{B}\mc{C}}$. 
If $\mc{B}\equiv \mc{E}$ then $\Braket{\mc{A}\mc{B}}\mc{C} \equiv \mc{A}\mc{C} \equiv \mc{A}\Braket{\mc{B}\mc{C}}$. 
If $\mc{C}\equiv \mc{E}$ then $\Braket{\mc{A}\mc{B}}\mc{C} \equiv \mc{A}\mc{B} \equiv \mc{A}\Braket{\mc{B}\mc{C}}$. So the theorem holds if any of $\mc{A}$, $\mc{B}$ or $\mc{C}$ is empty.

Now suppose $\mc{A}$, $\mc{B}$, and $\mc{C}$ are all nonempty so that $|\mc{A}|, |\mc{B}|, |\mc{C}| > 0$.
First we have that
$$
|\Braket{\mc{A}\mc{B}}\mc{C}| = |\mc{A}\mc{B}| + |\mc{C}| = |\mc{A}| + |\mc{B}| + |\mc{C}| = |\mc{A}| + |\mc{B}\mc{C}| = |\mc{A}\Braket{\mc{B}\mc{C}}|.
$$

Consider $0 \le i < |\mc{A}| + |\mc{B}| + |\mc{C}|$.
We must show
$$
\Braket{\Braket{\mc{A}\mc{B}}\mc{C}}[i] \equiv \Braket{\mc{A}\Braket{\mc{B}\mc{C}}}[i]
$$
There are three cases
\begin{itemize}
\item{
Suppose $0 \le i < |\mc{A}|$. 
Consider $\Braket{\mc{A}\mc{B}}\mc{C}$. 
Since $0 \le i < |\mc{A}| \le |\mc{A}\mc{B}|$ we have that $\Braket{\Braket{\mc{A}\mc{B}}\mc{C}}[i] \equiv \Braket{\mc{A}\mc{B}}[i]$. 
Again, since $0\le i < |\mc{A}|$, we have $\Braket{\mc{A}\mc{B}}[i] \equiv \mc{A}[i]$ so that $\Braket{\Braket{\mc{A}\mc{B}}\mc{C}}[i] \equiv \mc{A}[i]$. 
On the other hand, consider $\mc{A}\Braket{\mc{B}\mc{C}}$. 
Since $0 \le i < |\mc{A}|$ we have $\Braket{\mc{A}\Braket{\mc{B}\mc{C}}} \equiv \mc{A}[i]$. 
So in this case $\Braket{\Braket{\mc{A}\mc{B}}\mc{C}}[i] \equiv \Braket{\mc{A}\Braket{\mc{B}\mc{C}}}[i]$.
}
\item{
Now suppose $|\mc{A}| \le i < |\mc{A}| + |\mc{B}|$. 
Define $j = i - |\mc{A}|$ so that $0 \le j < |\mc{B}|$. 
Consider $\Braket{\mc{A}\mc{B}}\mc{C}$. 
Because $0 \le |\mc{A}| \le i < |\mc{A}| + |\mc{B}| = |\mc{A}\mc{B}|$ we have $\Braket{\Braket{\mc{A}\mc{B}}\mc{C}}[i] \equiv \Braket{\mc{A}\mc{B}}[i] \equiv \Braket{\mc{A}\mc{B}}[|\mc{A}| + j]$. Because $0 \le j < |\mc{B}|$ we have $\Braket{\mc{A}\mc{B}}[|\mc{A}|+j] = \mc{B}[j] \equiv \mc{B}[i - |\mc{A}|]$. 
On the other hand, consider $\mc{A}\Braket{\mc{B}\mc{C}}$. 
Because $0 \le j < |\mc{B}| \le |\mc{B}\mc{C}|$ we have $\Braket{\mc{A}\Braket{\mc{B}\mc{C}}}[i] \equiv \Braket{\mc{A}\Braket{\mc{B}\mc{C}}}[|\mc{A}| + j] \equiv \Braket{\mc{B}\mc{C}}[j]$. But, since $0 \le j < |\mc{B}|$ we have $\Braket{\mc{B}\mc{C}}[j] \equiv \mc{B}[j] \equiv \mc{B}[i - |\mc{A}|]$. So in this case $\Braket{\Braket{\mc{A}\mc{B}}\mc{C}}[i] \equiv \Braket{\mc{A}\Braket{\mc{B}\mc{C}}}[i]$.
}
\item{
Now suppose $|\mc{A}| + |\mc{B}| \le i < |\mc{A}| + |\mc{B}| + |\mc{C}|$. 
Define $j = i - |\mc{A}| - |\mc{B}|$ so that $0 \le j < |\mc{C}|$. Consider $\langle\mc{A}\mc{B}\rangle\mc{C}$. 
Because $0 \le j < |\mc{C}|$ we have $\Braket{\Braket{\mc{A}\mc{B}}\mc{C}}[i] \equiv \langle\langle\mc{A}\mc{B}\rangle\mc{C}\rangle[|\mc{A}| + |\mc{B}| + j] \equiv \langle\langle\mc{A}\mc{B}\rangle\mc{C}\rangle[|\mc{A}\mc{B}| + j] \equiv \mc{C}[j] \equiv \mc{C}[i - |\mc{A}| - |\mc{B}|]$. 
On the other hand, consider $\mc{A}\Braket{\mc{B}\mc{C}}$. 
We have $\Braket{\mc{A}\Braket{\mc{B}\mc{C}}}[i] \equiv \Braket{\mc{A}\Braket{\mc{B}\mc{C}}}[|\mc{A}| + |\mc{B}| + j]$.
We have that $0 \le |\mc{B}| \le |\mc{B}| + j < |\mc{B}| + |\mc{C}| = |\mc{B}\mc{C}|$ so $\Braket{\mc{A}\Braket{\mc{B}\mc{C}}}[|\mc{A}| + |\mc{B}| + j] \equiv \Braket{\mc{B}\mc{C}}[|\mc{B}| + j]$. 
But, $0 \le j < |\mc{C}|$ so $\Braket{\mc{B}\mc{C}}[|\mc{B}|+j] \equiv \mc{C}[j] \equiv \mc{C}[i - |\mc{A}|-|\mc{B}|]$. 
So in this case $\langle\langle\mc{A}\mc{B}\rangle\mc{C}\rangle[i] \equiv \langle\mc{A}\langle\mc{B}\mc{C}\rangle\rangle[i].$
}

In all cases we have $\langle\langle\mc{A}\mc{B}\rangle\mc{C}\rangle[i] \equiv \langle\mc{A}\langle\mc{B}\mc{C}\rangle\rangle[i]$.
This means that $\langle\mc{A}\mc{B}\rangle\mc{C} \equiv \mc{A}\langle\mc{B}\mc{C}\rangle$. 
\end{itemize}
\end{theorem}

\begin{definition}[Multiple Concatenation]
Here we define the set of $n$-concatenations over a list of lists. 
Note that lists of lists are different than lists which are themselves lists of objects.
If $S$ is a list of $n$ lists, i.e. $S\in \textbf{Str}^*$ and $|S| = n$, then $\circ_n(S)$ is defined to be the set of all lists which arise from concatenations of the $n$ lists in $S$.
We define $\circ_n$ recursively.

\begin{itemize}
\item{We define $\circ_1([\mc{A}]) = \{\mc{A}\}$}
\item{$\circ_n([\mc{A}_0,\ldots, \mc{A}_{n-1}]) = \bigcup_{i=1}^{n-1} \{\mc{X}\circ\mc{Y}:\mc{X} \in \circ_i([\mc{A}_0,\ldots \mc{A}_{i-1}]) \text{ and } \mc{Y}\in \circ_{n-i}([\mc{A}_{i}, \ldots, \mc{A}_{n-1}])\}$}
\end{itemize}
We say that $\circ_n(S)$ is the set of all parenthesizations of the concatenations of the $n$ lists in $S$.

\end{definition}

As an example consider $\circ_4([\mc{A}, \mc{B}, \mc{C}, \mc{D}])$. This will be equal to
\begin{align*}
\circ_4(\mc{A}, \mc{B}, \mc{C}, \mc{D}) = \{&\Braket{\mc{A}\circ\Braket{\mc{B}\circ\Braket{\mc{C}\circ\mc{D}}}},\\
&\Braket{\mc{A}\circ\Braket{\Braket{\mc{B}\circ\mc{C}}\circ\mc{D}}},\\
&\Braket{\Braket{\mc{A}\circ\mc{B}}\circ\Braket{\mc{C}\circ\mc{D}}},\\
&\Braket{\Braket{\mc{A}\circ\Braket{\mc{B}\circ\mc{C}}}\circ\mc{D}},\\
&\Braket{\Braket{\Braket{\mc{A}\circ\mc{B}}\circ\mc{C}}\circ\mc{D}}\}
\end{align*}


\begin{theorem}[Generalized Associativity of Multiple Concatenation]
The generalized associativity property for multiple concatenation states that, for $S\in\textbf{Str}^*$ with $S=[\mc{A}_0,\ldots,A_{n-1}]$ that $\circ_n(S)$ has only one element.
That is, if $\mc{A}, \mc{B}\in\circ_n(S)$ then $\mc{A}\equiv \mc{B}$.
We may denote this unique element as $\mc{A}_0\circ\ldots\circ\mc{A}_{n-1}$, which can be abbreviated as $\mc{A}_0\ldots\mc{A}_{n-1}$.
We proceed by induction on $n$.

If $n=1$ then $|S|=1$ and $S = [\mc{A}]$.
We then have $\circ_1(S) = \{\mc{A}\}$.
This set only has one element so we are done.

The induction hypothesis is that for all $m<n$ that $\circ_m([\mc{A}_0, \ldots, \mc{A}_{m-1}])$ contains a single unique element.
This element is denoted by $\mc{A}_0\ldots\mc{A}_{m-1}$.

We have the following useful result.
Suppose $1 \le l \le {m-1}$.
Since $l < m < n$ we have, by the induction hypothesis, that $\circ_l([\mc{A}_0, \ldots, \mc{A}_{l-1}])$ has a single unique element denoted by $\mc{A}_0\ldots\mc{A}_{l-1}$ (or simply $\mc{A}_0 \equiv \mc{A}_{l-1}$ if $l=1$).
Since $m-l < m < n$ we also have, again by the induction hypothesis, that $\circ_{m-l}([\mc{A}_l, \ldots, \mc{A}_{m-1}])$ has a single unique element denoted by $\mc{A}_l\ldots\mc{A}_{m-1}$ (or simply $\mc{A}_l \equiv \mc{A}_{m-1}$ if $l=m-1$).
This means that
$$
\mc{A}_0\ldots\mc{A}_{l-1} \circ \mc{A}_l\ldots\mc{A}_{m-1} \in \circ_m([\mc{A}_0, \ldots, \mc{A}_{m-1}])
$$
But, by the induction hypothesis, we know this latter set has a single unique element which means
$$
\mc{A}_0\ldots\mc{A}_{m-1} \equiv \mc{A}_0\ldots\mc{A}_{l-1}\circ \mc{A}_l \ldots \mc{A}_{m-1}
$$
We will use this result below.

Consider now
\begin{align*}
\circ_n([\mc{A}_0,\ldots, \mc{A}_{n-1}]) = \bigcup_{i=1}^{n-1} \{\mc{X}\circ\mc{Y}:\mc{X} \in \circ_i([\mc{A}_0,\ldots \mc{A}_{i-1}]) \text{ and } \mc{Y}\in \circ_{n-i}([\mc{A}_{i}, \ldots, \mc{A}_{n-1}])\}
\end{align*}
Because $i<n$ we know, by the induction hypothesis, that $\circ_i([\mc{A}_0, \ldots, \mc{A}_{i-1}])$ has a single unique element denoted by $\mc{A}_0\ldots\mc{A}_{i-1}$.
Likewise, because $n-i < n$, we know, again by the induction hypothesis, that $\circ_{n-i}([\mc{A}_i, \ldots, \mc{A}_{n-1}])$ has a single unique element denoted by $\mc{A}_i \ldots \mc{A}_{n-1}$.
We then have
\begin{align}
\circ_n([\mc{A}_0, \ldots, \mc{A}_{n-1}]) = \bigcup_{i=1}^{n-1} \{\mc{A}_0\ldots\mc{A}_{i-1}\circ\mc{A}_i\ldots\mc{A}_{n-1}\}
\end{align}

Now suppose $\mc{X}, \mc{Y} \in \circ([\mc{A}_0, \ldots, \mc{A}{n-1}])$.
This means there exists $j, k$ with $1 \le j \le n-1$ and $1 \le k \le n-1$ with
\begin{align}
\mc{X} \equiv& \mc{A}_0\ldots\mc{A}_{j-1} \circ \mc{A}_j \ldots \mc{A}_{n-1}\\
\mc{Y} \equiv& \mc{A}_0\ldots\mc{A}_{k-1} \circ \mc{A}_k \ldots \mc{A}_{n-1} 
\end{align}
We will now show $\mc{X} \equiv \mc{Y}$.

If $j=k$ then clearly $\mc{X}\equiv \mc{Y}$.
We now suppose, without loss of generality, that $k>j$.
By the intermediate result on the induction hypothesis above, and $n-j < n$ and $k<n$, we can write this as
\begin{align}
\mc{X} \equiv& \mc{A}_0\ldots\mc{A}_{j-1} \circ \langle\mc{A}_j\ldots\mc{A}_{k-1} \circ \mc{A}_k\ldots\mc{A}_{n-1}\rangle\\
\mc{Y} \equiv& \langle \mc{A}_0\ldots\mc{A}_{j-1}\circ\mc{A}_j\ldots\mc{A}_{k-1}\rangle\circ\mc{A}_k\ldots\mc{A}_{n-1}
\end{align}
But, we can see by Theorem \ref{thm:concatassoc} that $\mc{X}\equiv \mc{Y}$.
This implies that $\circ_n(\mc{A}_0,\ldots, \mc{A}_{n-1})$ has a single unique element which we may denote as
\begin{align*}
\mc{A}_0\ldots\mc{A}_{n-1}
\end{align*}

\end{theorem}

\begin{theorem}[Length of Multiple Concatenations]
\label{thm:multconcat}
Suppose $\mc{A} \equiv \mc{A}_0 \ldots \mc{A}_{N-1}$.
We will prove by induction on $N$ that $|\mc{A}| = \sum_{k=0}^{N-1} |\mc{A}|_k$.

Suppose $N=1$.
Then $\mc{A} \equiv \mc{A}_0$ so $|\mc{A}| = |\mc{A}_0|$.

Now suppose the theorem holds for any $m<N$. 
Let $m=N-1$.
We can write
$$
\mc{A} \equiv \Braket{\mc{A}_0 \ldots \mc{A}_{m-1}} \circ \mc{A}_{N-1}
$$
Because the term in the angle brackets is a concatenation of $m<N$ strings we have that 
$$
|\mc{A}_0\ldots \mc{A}_{m-1}| = \sum_{k=0}^{m-1} |\mc{A}_k|
$$
By the definition of concatenation, the length of $\mc{A}$ must then be
$$
|\mc{A}| = \sum_{k=0}^{N-2} |\mc{A}_k| + |\mc{A}_{N-1}| = \sum_{k=0}^{N-1}|\mc{A}_k|
$$
\end{theorem}

\begin{theorem}[Multiple Concatenation Element Extraction]
\label{thm:multconcatextract}
Suppose $\mc{A} \equiv \mc{A}_0\ldots\mc{A}_{N-1}$.
Now choose $0\le k \le N-1$ and let $L_k = \sum_{j=0}^{k-1} |\mc{A}_j|$, noting that $L_0=0$.
I will prove by induction on $N$ that if $\mc{A}_k$ is non-empty, then for $0 \le i < |\mc{A}_k|$ that $\mc{A}[L_k + i] \equiv \mc{A}_k[i]$.

Suppose $N=1$.
Then $\mc{A} \equiv \mc{A}_0$ and we must choose $k=0$ with $L_0=0$ and $0 \le i < |\mc{A}[0]|$.
In this case $\mc{A}[i]$ and $\mc{A}_0[i]$ are both defined and $\mc{A}[i] \equiv \mc{A}_0[i]$.

As our induction hypothesis we suppose that for $m = N-1 < N$ that, if $\mc{B}\equiv \mc{A}_0 \ldots \mc{A}_{m-1}$ that for $0\le k \le m-1$ if $\mc{A}_k$ is non-empty and $0 \le i < |\mc{A}_k|$ then $\mc{B}[L_k+i] \equiv \mc{A}_k[i]$.

Consider $\mc{A} \equiv \mc{A}_0 \ldots \mc{A}_{N-1}$.
We write $\mc{A}$ as
\begin{align*}
\mc{A} \equiv \Braket{\mc{A}_0\ldots \mc{A}_{m-1}} \circ \mc{A}_{N-1} \equiv \mc{B}\mc{A}_{N-1}
\end{align*}
where I've defined $\mc{B} \equiv \mc{A}_0 \ldots \mc{A}_{m-1}$.
Note that by the Theorem \ref{thm:multconcat} we have $|\mc{B}| = L_m$.
Choose $k$ with $0 \le k \le N-1$ with $\mc{A}_k$ non-empty and choose $i$ with $0 \le i < |\mc{A}_k|$.

If $k=m=N-1$ then we have $0\le i < |\mc{A}_{N-1}|$ which means, by the definition of concatenation, that
$$
\mc{A}[|\mc{B}|+i] \equiv \mc{A}[L_k+i] \equiv \mc{A}_{N-1}[i] \equiv \mc{A}_k[i]
$$

If $k < m$ then $k+1 \le m$ so $L_{k+1} \le L_m = |\mc{B}|$.
Consider $j = L_k + i$.
We have 
$$
0\le L_k \le j < L_k + |\mc{A}_k| = L_{k+1} \le |\mc{B}|
$$
This means, by the definition of concatenation, that
$$
\mc{A}[j] \equiv \mc{B}[j] \equiv \mc{B}[L_k + i]
$$
But, the induction hypothesis holds for $\mc{B}$, which means $\mc{B}[L_k+i] \equiv \mc{A}_k[i]$.
This means $\mc{A}[L_k+i] \equiv \mc{A}_k[i]$.

\end{theorem}

\subsubsection{List Slicing and Sublists}
In addition to combining lists to form larger lists, we can also slice lists to form smaller lists.

\begin{definition}[List Slicing]
Suppose $\mc{A}$ is a list.
We define $\mc{A}[i:j]$ recursively on the difference $j-i$. Suppose $0 \le i \le j \le |\mc{A}|$.

\begin{itemize}
\item{If $j-i=0$ then $\mc{A}[i:j] \equiv \mc{E}$.}
\item{If $j-i=1$ then $\mc{A}[i:j] \equiv [\mc{A}[i]]$.\footnote{Note the difference between $[\mc{A}[i]]\in \Sigma^*$, the list containing the object $\mc{A}[i]$ and the object $\mc{A}[i]\in \Sigma$ itself.}}
\item{If $j-i>1$ then $\mc{A}[i:j] \equiv \mc{A}[i:j-1]\circ\mc{A}[j-1:j]$}
\end{itemize}
\end{definition}

\begin{definition}[Sublists and Initial Parts]
Suppose $\mc{A}$ is a list, $0\le i \le j \le |\mc{A}|$ and $\mc{B} \equiv \mc{A}[i:j]$.
\begin{itemize}
\item{We say $\mc{B}$ is a sublist of $\mc{A}$.}
\item{If $i>0$ or $j<|\mc{A}|$ then we say $\mc{B}$ is a proper sublist of $\mc{A}$.}
\item{If $i=0$ then we say $\mc{B}$ is an initial part of $\mc{A}$.}
\item{If $i=0$ and $j < |\mc{A}|$ then we say $\mc{B}$ is a proper initial part of $\mc{A}$.}
Clearly if $\mc{B}$ is a proper initial part of $\mc{A}$ then it is an initial part of $\mc{A}$, if $\mc{B}$ is an initial part of $\mc{A}$ it is a sublist of $\mc{A}$, and if $\mc{B}$ is a proper sublist of $\mc{A}$ then it is a sublist of $\mc{A}$.
\end{itemize}
\end{definition}

\begin{theorem}[The Empty List is an Initial Part of Every List]
\label{thm:emptylistinitialpart}
If $\mc{A}$ is a list then $\mc{A}[0:0] \equiv \mc{E}$ so $\mc{E}$ is an initial part of $\mc{A}$.
\end{theorem}

\begin{theorem}[Length of Sublist]

Suppose $\mc{A}$ is a list and $0 \le i \le j \le |\mc{A}|$. Here we will prove that $|\mc{A}[i:j]| = j-i$ by induction on $j-i$.

\begin{itemize}
\item{If $j-i=0$ then $\mc{A}[i:j] \equiv \mc{E}$ and $|\mc{A}[i:j]|=0 = j-i$.}
\item{If $j-i=1$ then $\mc{A}[i:j] = [\mc{A}[i]]$ so $|\mc{A}[i:j]| = 1 = j-i$.}
\item{Now consider $j-i > 1$.
Suppose that if $j'-i' < j-i$ that $|\mc{A}[i':j']| = j'-i'$.
Consider $\mc{A}[i:j] \equiv \mc{A}[i:j-1] \circ \mc{A}[j-1:j]$.
We have, from the definition of concatenation, that $|\mc{A}[i:j]| = |\mc{A}[i:j-1]| + |\mc{A}[j-1:j]|$.
But, $(j-1) - i < j-i$ and $j - (j-1) = 1 < j-i$ so $|\mc{A}[i:j-1]| + |\mc{A}[j-1:j]| = (j-1)-i + j - (j-1) = j-i$.}
\end{itemize}
\end{theorem}

\begin{theorem}[Sublist Element Extraction]
\label{thm:sublistextraction}
Suppose $\mc{A}$ is non-empty and $0 \le i < j \le |\mc{A}|$ and $0 \le k < j-i$.
We will prove that $\mc{A}[i:j][k] \equiv \mc{A}[k + i]$. We will induct on $j$. 

If $j=1$ then $i=k=0$. $\mc{A}[0:1][0] \equiv [\mc{A}[0]][0] \equiv \mc{A}[0]$. Now suppose that $j>1$ but for any $j'<j$ and $0 \le i < j'$ and $0 \le k < j'-i$ that $\mc{A}[i:j'][k]\equiv \mc{A}[k+i]$. Suppose $0 \le i < j \le |\mc{A}|$ and $0 \le k < j-i$ and consider $\mc{A}[i:j][k] \equiv (\mc{A}[i:j-1]\circ\mc{A}[j-1:j])[k]$. Note that $|\mc{A}[i:j-1]| = j-1-i$. If $0 \le k < j-1-i$ then $\mc{A}[i:j][k] \equiv \mc{A}[i:j-1][k]$. Since $j-1<j$ this means $\mc{A}[i:j][k] \equiv \mc{A}[k+i]$. If $k=j-i-1$ then $\mc{A}[i:j][k] \equiv \mc{A}[i:j][j-i-1 + 0] \equiv \mc{A}[j-1:j][0] \equiv [\mc{A}[j-1]][0] \equiv \mc{A}[j-1] = \mc{A}[k+i]$. Therefore $\mc{A}[i:j][k+i] \equiv \mc{A}[k+i]$.
\end{theorem}

\begin{corollary}[A List is a Sublist of Itself]
Taking $i=0$ and $j = |\mc{A}|$ we see that $\mc{A}[0:|\mc{A}|] \equiv \mc{A}$, therefore $\mc{A}$ is a sublist of itself.
\end{corollary}

\begin{theorem}[Mutual Initial Parts are Equal]
Suppose $\mc{A}$ is an initial part of $\mc{A}'$ and $\mc{A}'$ is an initial part of $\mc{A}$. 
This means $\mc{A} \equiv \mc{A}'[0:j]$ so $|\mc{A}| = j\le|\mc{A}'|$ and $\mc{A}'\equiv \mc{A}[0:j']$ so $|\mc{A}'| = j'\le |\mc{A}|$. 
Since $|\mc{A}|\le |\mc{A}|'$ and $|\mc{A}'| \le |\mc{A}|$ we have $|\mc{A}| = |\mc{A}'| = j = j'$.
So $\mc{A}' \equiv \mc{A}[0:|\mc{A}|] \equiv \mc{A}$.
\end{theorem}

The following theorem will be critical for proving facts about strings within the formal language.

\begin{theorem}[Corresponding Intermediate Parts]
\label{thm:corrintpart}
We will prove that if $\mc{A}\equiv \mc{B}\mc{C}\mc{D} \equiv \mc{B}\mc{C}'\mc{D}'$ with $|\mc{C}| \le |\mc{C}'|$ that either $\mc{C}$ is an initial part of $\mc{C}'$.
We will show that $\mc{C} \equiv \mc{C}'[0:|\mc{C}|]$.

If $\mc{C}\equiv\mc{E}$ then $\mc{C}$ is a sublist of $\mc{C}'$ by Theorem \ref{thm:emptylistinitialpart}.

If $\mc{C}$ is non-empty, consider $0 \le j < |\mc{C}| \le |\mc{C}'|$.
We have $\mc{A} \equiv (\mc{B}\mc{C})\mc{D} \equiv (\mc{B}\mc{C}')\mc{D}'$.
Let $i = j + |\mc{B}|$ so that $0 \le i < |\mc{B}|+|\mc{C}| =|\mc{B}\mc{C}| \le |\mc{B}|+|\mc{C}'| = |\mc{B}\mc{C}'|$ so we have that $\mc{A}[i] \equiv (\mc{B}\mc{C})[i] \equiv (\mc{B}\mc{C}')[i]$. 

We rewrite this as $(\mc{B}\mc{C})[|\mc{B}|+j] \equiv (\mc{B}\mc{C}')[|\mc{B}|+j] \equiv \mc{C}[j] \equiv \mc{C}'[j] \equiv \mc{C}'[0:|\mc{C}|][j]$ by Theorem \ref{thm:sublistextraction}.
Since this holds for all $0 \le j < |\mc{C}|$ we have that $\mc{C} \equiv \mc{C}'[0:|\mc{C}|]$.

This means that $\mc{C}$ is an initial part of $\mc{C}'$.
\end{theorem}

\begin{corollary}{Corresponding Intermediate Parts}
\label{corr:intpart}
If $\mc{A} \equiv \mc{B}\mc{C}\mc{D} \equiv \mc{B}\mc{C}'\mc{D}'$ then either $\mc{C}$ is an initial part of $\mc{C}'$ of $\mc{C}'$ is an initial part of $\mc{C}$.
This follows from the fact that either $|\mc{C}| \le |\mc{C}'|$ or $|\mc{C}'| \le |\mc{C}|$ and Theorem \ref{thm:corrintpart}.
\end{corollary}

\subsubsection{Strings from Lists}
Up until now we have been discussing lists of over a general set of objects $\Sigma$.

\begin{definition}[String]
A string is an element of $\textbf{Str} = \textbf{Symb}^*$.
In other words, every string is a finite ordered list of symbols and any finite ordered list of symbols is a string.
\end{definition}

The only distinctive feature about strings compared to general lists is that if we have a list of symbols such as 

$$
[x_1, \lnot, Q, \curlywedge, (]
$$

We will typically abbreviate it as

$$
x_1\lnot Q\curlywedge (
$$

dropping the separating commas and enclosing square brackets.
This abbreviation is generally ok.
The main ambiguity in this abbreviation is that when we write down, for example, the symbol $\lnot$ it is not clear if we are talking about the symbol $\lnot$ or the list containing the symbol $[\lnot] \equiv [[0, \lnot]]$.
In practice, this distinction is typically not important so the ambiguity is harmless.
If we say, for example, that $\alpha \in \textbf{Str}$ where $\alpha$ is a symbol we mean $[\alpha] \in \textbf{Str}$.
For strings we replace the terms `sublist' and `proper sublist' with `substring' and `proper substring'.


\subsection{The Syntax of LFOL}

The smallest syntactic elements of \textbf{LFOL} are the terms defined as follows.

\begin{definition}[Terms]

$t \in \textbf{Term}$ iff $t \in \textbf{Str}$ and satisfies one of
\begin{itemize}
\item{\textbf{Variables:} $t\equiv [x]$ with $x \in \textbf{Var}$.}
\item{\textbf{Functions:} $t \equiv ft_1 \ldots t_n$ with $f \in \textbf{Func}_n$ and $t_1, \ldots, t_n \in \textbf{Term}$. Often we will abuse notation and write this as $f(t_1, \ldots, t_n)$ or $f(\bv{t})$. Note that if $f$ is a 0-ary function symbol then $f$ is a term representing a constant.}
\end{itemize}
\end{definition}

Semantically, if the expressions of the formal language are making statements, then terms are the subjects of those statements.
0-ary functions represent constants or concrete objects while variables represent place-holders into which other terms can be `plugged in'.

We define the set of free variables $FV(t)$ of a term $t$ recursively.

\begin{definition}[Free Variables in Terms]
If $t\in \textbf{Term}$ then $t$ we define the set $FV(t)$ as follows.
\begin{itemize}
\item{\textbf{Variables:} If $t\equiv [x]$ with $FV = \{x\}$}
\item{\textbf{Functions:} If $t\equiv ft_1\ldots t_n$ then $FV(t) = FV(t_1)\cup\ldots \cup FV(t_n)$.}
\end{itemize}
\end{definition}

If $FV(t)$ is empty then we say that $t$ is closed, otherwise we say that $t$ is open.
Closed terms can be thought of as concrete objects while open terms can be thought of as placeholders into which concrete objects can be `plugged in'.

The next larger syntactic elements in \textbf{LFOL} are the atomic formulas defined as follows.

\begin{definition}[Atomic Formulas]
$\mc{A} \in \textbf{Atom}$ iff $\mc{A} \in \textbf{Str}$ and satisfies

\begin{itemize}
\item{\textbf{Atomic Formulas:} $\mc{A} \equiv Pt_1\ldots t_n$ with $P\in \textbf{Pred}_n$ and $t_1, \ldots, t_n \in \textbf{Term}$. Again we will abuse notation and write this as $P(t_1,t_2,\ldots,t_n)$ or $P(\bv{t})$. In our notation $\curlywedge$ is an atomic formula.}
\end{itemize}
\end{definition}

Semantically, an atomic formula express a boolean statement about the terms within its scope.
If the terms are all closed then we can, semantically, think of the atomic formula as having a truth value.
If any of the terms are open then the atomic formula would only have a truth value once closed terms are `plugged in' for all variables.

The final layer of syntax is the formula, sometimes called a well-formed formula  (Wff).
Formulas combine multiple atomic formulas into more complex expressions using the logical operators.
We will define formulas in two phases.
We will first present \textit{permissive} formulas, then we will present \textit{strict} formulas. 
permissive formulas and strict formulas will differ in their treatment of quantifiers.
In short, permissive formulas will allow repeated quantification (the binding of a variable $x$ within the scope of a quantifier already binding over $x$) and redundant quantification (the quantification over a variable $x$ over an expression which does not include $x$). 
Both repeated and redundant quantification are in some-sense unnatural, in that they don't translate well into English example sentences, and that the quantification feels either confusing (in the case of repeated quantification) or wasteful (in the case of redundant quantification).

Strict formulas will explicitly forbid both repeated and redundant quantification.
The downside, however, is that it will be much more difficult to prove metalogical theorems about strict formulas.
This is because care will need to be taken at each step to ensure that each expression satisfies the provisos necessary for strict formulas.
It is typically the case that logic references do allow repeated and redundant quantification for precisely this reason.
In fact, one might worry that semantically different theorems could be derived using permissive formulas compared to strict formulas, but this is in fact not the case.

Nonetheless, in this document we will mainly be using strict formulas, with permissive formulas only as a temporary stepping stone to help more easily define strict formulas.
Moving forward `formula' will refer to `strict formula' unless otherwise specified.

\begin{definition}[Permissive Formulas]
$\mc{A} \in \textbf{pForm}$ iff $\mc{A} \in \textbf{Str}$ and satisfies one of 

\begin{itemize}
\item{\textbf{Atomic Formulas:} $\mc{A}$ is an atomic formula.}
\item{\textbf{Implication:} $\mc{A}\equiv (\mc{B}\implies \mc{C})$ and $\mc{B}, \mc{C} \in \textbf{pForm}$.}
\item{\textbf{Negation:} $\mc{A}\equiv (\lnot \mc{B})$ and $\mc{B} \in \textbf{pForm}$.}
\item{\textbf{Quantification:} $\mc{A}\equiv ((\forall x)\mc{B})$ and $x\in \textbf{Var}$, $\mc{B}\in \textbf{pForm}$. We say all substrings of $\mc{B}$ (including $\mc{B}$) appear in the scope of $(\forall x)$ in $\mc{A}$.} 
\end{itemize}
\end{definition}

We define the free variables, $FV(\mc{A})$ in a formula $\mc{A}$ similarly to the free variables of a term.

\begin{definition}[Free Variables in Permissive Formulas]
If $\mc{A}$ is a permissive formula then we define $FV(\mc{A})$ recursively.
\begin{itemize}
\item{if $\mc{A} \equiv Pt_1\ldots t_n$ for $n$-ary predicate $P$ then $FV(\mc{A}) = FV(t_1)\cup\ldots\cup FV(t_n)$. Note that this implies $FV(P)=\emptyset$ if $P$ is a 0-ary predicate such as $\curlywedge$.}
\item{if $\mc{A} \equiv (\mc{B}\implies \mc{C})$ for Wffs $\mc{B}$ and $\mc{C}$ then $FV(\mc{A}) = FV(\mc{B})\cup FV(\mc{C})$.}
\item{if $\mc{A} \equiv (\lnot\mc{B})$ then $FV(\mc{A}) = FV(\mc{B})$.}
\item{if $\mc{A} \equiv ((\forall x)\mc{B})$ then $FV(\mc{A}) = FV(\mc{B}) - \{x\}$.}
\item{If $FV(\mc{A}) = \emptyset$ then we say $\mc{A}$ is closed.}
\end{itemize}
\end{definition}


Semantically, like atomic formulas, formulas express, now more complex, statements about terms.
Note that, $((\forall x) \mc{B})$ is a permissive formula even if $x\not \in FV(\mc{B})$.

\subsection{Unique Readability}

\begin{theorem}[No Proper Initial Part of a Term is a Term]
\label{thm:termnopropinit}
I will prove that if $t\in\textbf{Term}$ then if $t'$ is a proper initial part of $t$ then $t'\not \in \textbf{Term}$. I will induct on $|t|$.

$|t|$ must be greater than zero because if $|t| = 0$ then $t \equiv \mc{E}$ and $\mc{E}\not \in \textbf{Term}$.

If $|t| = 1$ then the only proper initial part of $t$ is $t[0:0] = \mc{E}$ and $\mc{E} \not \in \textbf{Term}$.

Now suppose $|t|>1$. 
Suppose, as the induction hypothesis, that for any $s\in\textbf{Term}$ with $|s|<|t|$ that if $s'$ is a proper initial part of $s$ that $s'\not \in \textbf{Term}$. 
Suppose, for contradiction, that $t'$ is a proper initial part of $t$ and that $t'$ is a term. 
We then have that $t \equiv ft_1\ldots t_n$ and $t' \equiv f't_1'\ldots t_n'$ With $t_1, \ldots t_n, t_1', \ldots t_n' \in \textbf{Term}$. 
We must have $t' \equiv t[0:j]$ for some $0 < j < |t|$ so $t'[0] \equiv t[0]$ by Theorem \ref{thm:sublistextraction}.
This means $f' \equiv f$ so that $t' \equiv ft_1'\ldots t_n'$. 
By Corollary \ref{corr:intpart} it must be the case that either $t_1'$ is an initial part of $t_1$ or $t_1$ is an initial part of $t_1'$. But, $|t_1|, |t_1'| < |t|$ which means that neither $t_1$ nor $t_1'$ can have a proper initial part that is a term which means that either $t_1$ is a non-proper initial part of $t_1$ or $t_1$ is a non-proper initial part of $t_1'$, in both cases, we get that $t_1\equiv t_1'$.
This argument can be repeated up to $t_n$ showing that $t_k \equiv t_k'$ for $1 \le k \le n$ which means that $t \equiv t'$ contradicting the assumption that $t'$ is a proper initial substring of $t$.
\end{theorem}

\begin{theorem}[Initial Symbols in a Term]
\label{thm:initsymbterm}
Suppose $t$ is a term. If $|t|=1$ then either $t[0] \in \textbf{Var}$ or $t[0] \in \textbf{Func}_0$.
If $|t|>1$ then $t\equiv ft_1\ldots t_n$ with $f\in\textbf{Func}$ and $t_1\ldots t_n\in\textbf{Term}$ so $t[0]\equiv f \in \textbf{Func}$.
This means that for any term $t$ that $t[0] \in \textbf{Var}$ or $t[0]\in\textbf{Func}$.
\end{theorem}

\begin{theorem}[Initial Symbols in a Permissive Formula]
\label{thm:initsymbform}
Suppose $\mc{A}\in \textbf{pForm}$.
We will use Theorem \ref{thm:multconcatextract} throughout.

If $\mc{A}$ is atomic then $\mc{A}[0] \in \textbf{Pred}$.
If $\mc{A}$ is an implication, negation, or quantification, then $\mc{A}[0] \equiv ($. So for every permissive formula $\mc{A}[0]$ is in $\textbf{Pred}$ or equal to $($.

Suppose $|\mc{A}| > 1$.
If $\mc{A}$ is atomic then $\mc{A}\equiv Pt_1\ldots t_n$ with $P\in\textbf{Pred}$ and $t_1\ldots t_n\in\textbf{Term}$.
This means $\mc{A}[1]\equiv t_1[0]$ and, by Theorem \ref{thm:initsymbterm} $t_1[0]\in\textbf{Var}$ or $t_1[0]\in\textbf{Func}$.

If $\mc{A}$ is an implication then $\mc{A}\equiv (\mc{B}\implies \mc{C})$ for $\mc{B},\mc{C}\in\textbf{pForm}$.
We see that $\mc{A}[1]\equiv \mc{B}[0]$ and from above, $\mc{B}[0]$ is either in $\textbf{Pred}$ or equal to $($.

If $\mc{A}$ is a negation then $\mc{A}\equiv (\lnot \mc{B})$ and $\mc{A}[1] \equiv \lnot$.

If $\mc{A}$ is a quantification then $\mc{A}\equiv ((\forall x)\mc{B}0$ and $\mc{A}[1] \equiv ($.

This all means that if $|\mc{A}|>1$ that $\mc{A}[1]$ is either in $\textbf{Var}$, in $\textbf{Func}$, in $\textbf{Pred}$, equal to $($, or equal to $\lnot$.
\end{theorem}

\begin{theorem}[Unique Range of Permissive Formula Construction Rules]
\label{thm:uniquerange}
Suppose $\mc{A}\in\textbf{pForm}$.
We will use Theorem \ref{thm:multconcatextract} throughout.

Suppose $\mc{A}$ is atomic.
Then $\mc{A}[0] \in \textbf{Pred}$.
If $\mc{A}$ were either an implication, negation, or quantification we would have $\mc{A}[0]\equiv (\not\in \textbf{Pred}$ so $\mc{A}$ is not an implication negation or quantification.



Suppose $\mc{A}$ is an implication so that $\mc{A} \equiv (\mc{B}\implies \mc{C})$ with $\mc{B},\mc{C}\in\textbf{pForm}$.
We know from above that $\mc{A}$ is not atomic.

We see that $\mc{A}[1]\equiv \mc{B}[0]$ so by Theorem \ref{thm:initsymbform}, $\mc{A}[1]\equiv\mc{B}[0]$ is in $\textbf{Pred}$ or equal to $($. 
If $\mc{A}$ were also a negation then we would have $\mc{A}[1]\equiv \lnot$, so $\mc{A}$ is not a negation.

If $|\mc{B}| = 1$ then $\mc{A}[2]\equiv \implies$.
If $|\mc{B}| > 1$ then $\mc{A}[2] \equiv \mc{B}[1]$ which, by Theorem \ref{thm:initsymbform}, is either in $\textbf{Var}$, in $\textbf{Func}$, in $\textbf{Pred}$, equal to $($, or equal to $\lnot$. 
If $\mc{A}$ were a quantification then we would have $\mc{A}[2] \equiv \forall$, so $\mc{A}$ is not a quantification.

Suppose $\mc{A}$ is a negation so that $\mc{A} \equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$.
We know from above that $\mc{A}$ is not atomic and $\mc{A}$ is not an implication.
$\mc{A}[2] \equiv \mc{B}[0]$ which must either be in $\textbf{Pred}$ or equal to $($.
If $\mc{A}$ were a quantification then we would have $\mc{A}[2] \equiv \forall$, so $\mc{A}$ is not a quantification.

If $\mc{A}$ were a quantification then we know from above that it is not atomic, not an implication, and not a negation.

Therefore if $\mc{A}\in\textbf{pForm}$ then it exactly one of atomic, implication, negation, or quantification.
\end{theorem}

\begin{theorem}[Unique Readability of Atomic Formulas]
\label{thm:atomunique}
Suppose $\mc{A}\in \textbf{pForm}$ and $\mc{A}$ is atomic.
Then $\mc{A} \equiv Pt_1\ldots t_n$ with $P\in\textbf{Pred}_n$ and $t_1,\ldots,t_n\in\textbf{Term}$. 
We know from \ref{thm:uniquerange} that $\mc{A}$ is not an implication, negation, or quantification.
Suppose that $\mc{A} \equiv P't_1'\ldots t_k'$ with $P'\in\textbf{Pred}_k$ and $t_1',\ldots, t_k'\in\textbf{Term}$.
We see that $\mc{A}[0] \equiv P \equiv P'$ which also means $k=n$.
We then have $\mc{A}\equiv P t_1'\ldots t_k'$.
We can see by Corollary \ref{corr:intpart} that $t_1$ must be an initial part of $t_1'$ or $t_1'$ must be an initial part of $t_1$, but because of Theorem \ref{thm:termnopropinit} no term can have a proper initial part, so we must have $t_1\equiv t_1'$.
This same argument is repeated for $t_2$ through $t_n$ so that $t_i \equiv t_i'$ for all $1 \le i \le n$.
\end{theorem}

\begin{theorem}[No Proper Initial Part of a Permissive Formula is a Permissive Formula]
\label{thm:permpropinit}
Suppose $\mc{A}\in\textbf{pForm}$.
We proceed by induction on $|\mc{A}|$ to show that $\mc{A}$ has no proper initial part that is a permissive formula.

If $|\mc{A}| = 1$ then the only proper initial part of $\mc{A}$ is $\mc{E} \not \in \textbf{pForm}$.

Suppose that $|\mc{A}| = n>1$. 
The induction hypothesis is that for any $\mc{B}$ with $|\mc{B}|<n$ that $\mc{B}$ has no proper initial part that is a permissive formula.
We need one case for each type of permissive formula.

Suppose $\mc{A}$ is atomic and suppose $\mc{A}'\in \textbf{pForm}$ and is an initial part of $\mc{A}$.
We have $\mc{A} \equiv Pt_1\ldots t_n$ with $P\in \textbf{Pred}$ and $t_1,\ldots,t_n\in\textbf{Term}$.
We can see that $\mc{A}'[0] \equiv \mc{A}[0] \equiv  P$ so we see that $\mc{A}'$ is atomic.
We have $\mc{A}' \equiv Pt_1'\ldots t_n'$.
By the same argument as in Theorem \ref{thm:atomunique} we conclude that $t_i' \equiv t_i$ for all $1 \le i \le n$.
Therefore $\mc{A}' \equiv \mc{A}$ so that $\mc{A}'$ cannot be a proper initial part of $\mc{A}$.
Note that this case does not require the induction hypothesis.

If $\mc{A}$ is an implication then we have $\mc{A} \equiv (\mc{B} \implies \mc{C})$ with $\mc{B}, \mc{C} \in \textbf{pForm}$.
Suppose, for contradiction, that $\mc{A}'$ is a proper initial part of $\mc{A}$ and $\mc{A}\in\textbf{pForm}$.
If $|\mc{A}'| = 0$ then $\mc{A}' \equiv \mc{E}$ and so $\mc{A}' \not \in \textbf{pForm}$.
If $|\mc{A}'| = 1$ then $\mc{A}' \equiv [(]$ and $\mc{A}' \not\in \textbf{pForm}$.
Suppose $|\mc{A}'|>1$.
Then $\mc{A}'[1] \equiv \mc{A}[1] \equiv \mc{B}[0]$ which is either in $\textbf{Pred}$ or equal to $($. This means that $\mc{A}'$ is not a negation.
If $|\mc{A}'| < 3$ then $\mc{A}'$ is not a quantification.
If $|\mc{A}'| \ge 3$ then $\mc{A}'[2] \equiv \mc{A}[2] \equiv \mc{B}[1]$ which is either in $\textbf{Var}$, $\textbf{Func}$, or $\textbf{Pred}$ or equal to $($ or $\lnot$. But if $\mc{A}'$ were a quantification then we would have $\mc{A}'[2] \equiv \forall$, so $\mc{A}'$ is not a quantification.
So $\mc{A}'$ is an implication with $\mc{A}' \equiv (\mc{B}' \implies \mc{C}')$ with $\mc{B}', \mc{C}' \in \textbf{pForm}$.
We can see by Corollary \ref{corr:intpart} that $\mc{B}$ must be an initial part of $\mc{B'}$ or $\mc{B}'$ must be an initial part of $\mc{B}$.
But $|\mc{B}|$ and $|\mc{B}'|$ are less than $n$, so, by the induction hypothesis, neither can have a proper initial part that is also a permissive formula.
Therefore $\mc{B} \equiv \mc{B}'$ and $\mc{A}' \equiv (\mc{B} \implies \mc{C}')$.
By the same argument we find $\mc{C} \equiv \mc{C}'$.
This means $\mc{A} \equiv \mc{A}'$, contradicting the assumption that $\mc{A}'$ is a proper initial part of $\mc{A}$.

If $\mc{A}$ is a negation then $\mc{A} \equiv (\lnot \mc{B})$ with $\mc{B} \in \textbf{pForm}$.
Suppose that $\mc{A}'$ is a proper initial part of $\mc{A}$ and $\mc{A}'\in\textbf{pForm}$.
Since $\mc{A}'[0] \equiv \mc{A}[0] \equiv ($ we know that $\mc{A}'$ is not atomic.
If $\mc{A}'$ were an implication then we would have $\mc{A}' \equiv (\mc{B}' \implies \mc{C}')$ with $\mc{B}', \mc{C}' \in \textbf{pForm}$. 
In this case we would have $\mc{A}[1] \equiv \mc{B}[0]$ which can be either in $\textbf{Pred}$ or equal to $($, but $\mc{A}[1] \equiv \lnot$ so $\mc{A}'$ is not an implication.
If $\mc{A}'$ were a quantification we would have $\mc{A}' \equiv ((\forall x) \mc{B}')$ so that $\mc{A}'[2] \equiv \forall$. But $\mc{A}'[2] \equiv \mc{B}[0]$ which is either in $\textbf{Pred}$ or equal to $($ so $\mc{A}'$ is not a quantification.
$\mc{A}'$ must be a negation so that $\mc{A}' \equiv (\lnot \mc{B}')$ with $\mc{B}' \in \textbf{pForm}$.
But, by Corollary \ref{corr:intpart} we have $\mc{B}$ must be an initial part of $\mc{B}'$ or $\mc{B}'$ must be an initial part of $\mc{B}$.
But, $|\mc{B}|$ and $|\mc{B}'|$ are both less than $n$, so by the induction hypothesis, neither can have a proper initial part that is also a permissive formula so we must have $\mc{B}' \equiv \mc{B}$.
This means $\mc{A}' \equiv \mc{A}$, contradicting the assumption that $\mc{A}'$ is a proper initial part of $\mc{A}$.

If $\mc{A}$ is a quantification then $\mc{A} \equiv ((\forall x) \mc{B})$ with $x \in \textbf{Var}$ and $\mc{B} \in \textbf{pForm}$.
Suppose $\mc{A}'$ is a proper initial part of $\mc{A}$ and $\mc{A}' \in \textbf{pForm}$.
$\mc{A}'[0] \equiv \mc{A}[0] \equiv ($ so $\mc{A}'$ is not atomic.
Also, $\mc{A}'[1] \equiv \mc{A}[1] \equiv \lnot$ so $\mc{A}'$ is not a negation.
If $\mc{A}'$ were an implication then we would have $\mc{A}' \equiv (\mc{B}' \implies \mc{C}')$ with $\mc{B}', \mc{C}' \in \textbf{pForm}$.
$\mc{A}'[2]$ would be $\mc{B}'[1]$ which is either in $\textbf{Var}$, $\textbf{Func}$, $\textbf{Pred}$, or equal to $($ or $\lnot$, but $\mc{A}'[2] \equiv \mc{A}[2] \equiv \forall$, so $\mc{A}'$ is not an implication.
$\mc{A}'$ must be a quantification with $\mc{A}' \equiv ((\forall x') \mc{B}')$ with $x' \in \textbf{Var}$ and $\mc{B}' \in \textbf{pForm}$.
$\mc{A}'[3] \equiv x' \equiv \mc{A}[3] \equiv x$ so $x' \equiv x$.
We can then see, by an application of Corollary \ref{corr:intpart}, that $\mc{B}$ must be an initial part of $\mc{B}'$ or $\mc{B}'$ must be an initial part of $\mc{B}$.
But since $|\mc{B}|$ and $|\mc{B}'| $ are less than $n$ neither can have a proper initial part that is also a permissive formula, so we must have $\mc{B} \equiv \mc{B}'$.
This means $\mc{A}' \equiv \mc{A}$ contradicting the assumption that $\mc{A}'$ is a proper initial part of $\mc{A}$.
\end{theorem}

\begin{corollary}[Unique Readability of Permissive Formulas]
If $\mc{A}$ is atomic with $\mc{A}\equiv Pt_1\ldots t_n$ with $P\in \textbf{Pred}$ and $t_1, \ldots, t_n \in \textbf{Term}$ then we know from Theorem \ref{thm:uniquerange} that $\mc{A}$ is not an implication, negation, or quantification.
We also know from Theorem \ref{thm:atomunique} that if $\mc{A} \equiv P't_1'\ldots t_n'$ with $P'\in\textbf{Pred}$ and $t_1',\ldots, t_n'\in\textbf{Term}$ that $P' \equiv P$ and $t_i' \equiv t_i$ for all $1 \le i \le n$.

Suppose $\mc{A}$ is an implication with $\mc{A} \equiv (\mc{B} \implies \mc{C})$.
We know from Theorem \ref{thm:uniquerange} that $\mc{A}$ is not atomic, a negation, or a quantification.
Suppose $\mc{A} \equiv (\mc{B}'\implies \mc{C}')$.
Corollary \ref{corr:intpart} implies that $\mc{B}'$ is an initial part of $\mc{B}$ or $\mc{B}$ is an initial part of $\mc{B}'$, but, by Theorem \ref{thm:permpropinit}, no permissive formula can have a proper initial part that is a permissive formula so we must have $\mc{B} \equiv \mc{B}'$. 
The same argument follows to conclude $\mc{C} \equiv \mc{C}'$.

Suppose $\mc{A}$ is a negation with $\mc{A} \equiv (\lnot \mc{B})$.
We know from Theorem \ref{thm:uniquerange} that $\mc{A}$ is not atomic, an implication, or a quantification.
Suppose $\mc{A} \equiv (\lnot \mc{B}')$.
Corollary \ref{corr:intpart} implies that $\mc{B}'$ is an initial part of $\mc{B}$ or $\mc{B}$ is an initial part of $\mc{B}'$, but, by Theorem \ref{thm:permpropinit}, no permissive formula can have a proper initial part that is a permissive formula so we must have $\mc{B} \equiv \mc{B}'$. 

Suppose $\mc{A}$ is a quantification with $\mc{A} \equiv ((\forall x) \mc{B})$ with $x \in \textbf{Var}$ and $\mc{B}\in\textbf{pForm}$.
We know from \ref{thm:uniquerange} that $\mc{A}$ is not atomic, an implication, or a negation.
Suppose $\mc{A} \equiv ((\forall x') \mc{B}')$.
Since $\mc{A}[3] \equiv x' \equiv x$ we have $x' \equiv x$.
Corollary \ref{corr:intpart} implies that $\mc{B}'$ is an initial part of $\mc{B}$ or $\mc{B}$ is an initial part of $\mc{B}'$, but, by Theorem \ref{thm:permpropinit}, no permissive formula can have a proper initial part that is a permissive formula so we must have $\mc{B} \equiv \mc{B}'$. 
\end{corollary}

\subsection{Free and Bound Variable Identification}

In this section we will define the free and bound variables in a permissive formula $\mc{A}$.


\begin{definition}[Free Variables in Terms]
If $t\in \textbf{Term}$ then $t$ we define the set $FV(t)$ as follows.
\begin{itemize}
\item{\textbf{Variables:} If $t\equiv [x]$ with $FV = \{x\}$}
\item{\textbf{Functions:} If $t\equiv ft_1\ldots t_n$ then $FV(t) = FV(t_1)\cup\ldots \cup FV(t_n)$.}
\end{itemize}
\end{definition}

\begin{definition}[Free Variables in Permissive Formulas]
If $\mc{A}$ is a permissive formula then we define $FV(\mc{A})$ recursively.
\begin{itemize}
\item{if $\mc{A} \equiv Pt_1\ldots t_n$ for $n$-ary predicate $P$ then $FV(\mc{A}) = FV(t_1)\cup\ldots\cup FV(t_n)$. Note that this implies $FV(P)=\emptyset$ if $P$ is a 0-ary predicate such as $\curlywedge$.}
\item{if $\mc{A} \equiv (\mc{B}\implies \mc{C})$ for Wffs $\mc{B}$ and $\mc{C}$ then $FV(\mc{A}) = FV(\mc{B})\cup FV(\mc{C})$.}
\item{if $\mc{A} \equiv (\lnot\mc{B})$ then $FV(\mc{A}) = FV(\mc{B})$.}
\item{if $\mc{A} \equiv ((\forall x)\mc{B})$ then $FV(\mc{A}) = FV(\mc{B}) - \{x\}$.}
\item{If $FV(\mc{A}) = \emptyset$ then we say $\mc{A}$ is closed.}
\end{itemize}
\end{definition}

Note that $((\forall x) \mc{B})$ is a permissive formula even if $x\not \in FV(\mc{B})$.
In such cases $\mc{A}$ exhibits redundant quantification.

\begin{definition}[Quantified Variables in Permissive Formulas]
If $\mc{A}$ is a permissive formula then we define $QV(\mc{A}$ recursively.

\begin{itemize}
\item{If $\mc{A}$ is an atomic formula then $QV(\mc{A}) = \emptyset$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$ then $QV(\mc{A}) \equiv QV(\mc{B})$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ with $\mc{B}, \mc{C}\in\textbf{pForm}$ then $QV(\mc{A}) = QV(\mc{B}) \cup QV(\mc{C})$.}
\item{If $\mc{A} \equiv ((\forall x) \mc{B})$ with $x\in\textbf{Var}$ and $\mc{B}\in\textbf{pForm}$ then $QV(\mc{A}) = QV(\mc{B}) \cup \{x\}$.}
\end{itemize}
\end{definition}

Note that $((\forall x)\mc{B})$ is a permissive formula even if $x\in QV(\mc{B})$.
In such cases $\mc{A}$ exhibits repeated quantification.

Note also that it is possible to have $x\in FV(\mc{A})$ and $x\in QV(\mc{A})$.
For example
\begin{align}
\mc{A} \equiv (Px \implies ((\forall x) Qy))
\end{align}
We have $FV(\mc{A}) = \{x, y\}$ and $QV(\mc{A}) = \{x\}$.

\subsection{Strict Formulas}

We are now in a position to define strict formulas.
\begin{definition}[Strict Formulas]
$\mc{A}\in\textbf{sForm} = \textbf{Form}$ iff $\mc{A} \in \textbf{pForm}$ and satisfies one of
\begin{itemize}
\item{\textbf{Atomic Formulas}: $\mc{A}$ is an atomic formula.}
\item{\textbf{Implication}: $\mc{A} \equiv (\mc{B} \implies \mc{C})$ and $\mc{B}, \mc{C} \in \textbf{Form}$.}
\item{\textbf{Negation}: $\mc{A} \equiv (\lnot \mc{B})$ and $\mc{B}\in\textbf{Form}$.}
\item{\textbf{Quantification}: $\mc{A} \equiv ((\forall x)\mc{B})$ with $\mc{x}\in\textbf{Var}$, $\mc{B}\in\textbf{Form}$, and $x\in FV(\mc{B})$ and $x\not\in QV(\mc{B})$.}
\end{itemize}
\end{definition}



AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA

\subsection{Strict Wffs}
We will now use pWffs to define free and quantified variables within a pWff $\mc{A}$.
These concepts will, in turn, be used to define sWffs.

\noindent\fbox{\parbox{\textwidth}{
\textbf{Free Variables}
We define inductively the set of free variables in a term $t$ or pWff $\mc{A}$, $FV(\mc{A})$.

\begin{itemize}
\item{if $t\equiv x$ is a variable then $FV(t) = \{x\}$.}
\item{if $t \equiv ft_1\ldots t_n$ then $FV(t) = FV(t_1)\cup\ldots\cup FV(t_n)$. Note that this implies $FV(f)=\emptyset$ if $f$ is a 0-ary function.}
\item{if $\mc{A} \equiv Pt_1\ldots t_n$ for $n$-ary predicate $P$ then $FV(\mc{A}) = FV(t_1)\cup\ldots\cup FV(t_n)$. Note that this implies $FV(P)=\emptyset$ if $P$ is a 0-ary predicate such as $\curlywedge$.}
\item{if $\mc{A} \equiv (\mc{B}\implies \mc{C})$ for Wffs $\mc{B}$ and $\mc{C}$ then $FV(\mc{A}) = FV(\mc{B})\cup FV(\mc{C})$.}
\item{if $\mc{A} \equiv (\lnot\mc{B})$ then $FV(\mc{A}) = FV(\mc{B})$.}
\item{if $\mc{A} \equiv ((\forall x)\mc{B})$ then $FV(\mc{A}) = FV(\mc{B}) - \{x\}$. Recall that $\mc{B}$ must contain $x$ as a substring.}
\item{If $FV(\mc{A}) = \emptyset$ then we say $\mc{A}$ is closed.}
\end{itemize}
}}

\noindent\fbox{\parbox{\textwidth}{
\textbf{Quantified Variables}
We define inductively the set of quantified variables in a pWff $\mc{A}$, $QV(\mc{A})$.

\begin{itemize}
\item{If $\mc{A}$ is an atomic formula then $QV(\mc{A}) = \emptyset$.}
\item{If $\mc{A} \equiv (\mc{B}\implies \mc{C})$ then $QV(\mc{A}) = QV(\mc{B}) \cup QV(\mc{C})$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ then $QV(\mc{A}) = QV(\mc{B})$.}
\item{If $\mc{A} \equiv ((\forall x) \mc{B})$ then $QV(\mc{A}) = QV(\mc{B}) \cup \{x\}$.}
\end{itemize}

}}

We are now in a position to define sWffs or Wffs.

\noindent\fbox{\parbox{\textwidth}{
\textbf{Strict Well-Formed Formulas}
$\mc{A} \in \textbf{sWff} = \textbf{Wff}$ iff $\mc{A} \in \textbf{pWff}\subset\textbf{Str}$ and satisfies one of

\begin{itemize}
\item{\textbf{Atomic Formulas:} $\mc{A}$ is an atomic formula}
\item{\textbf{Implication:} $\mc{A} \equiv (\mc{B}\implies \mc{C})$ and $\mc{B}, \mc{C} \in \textbf{sWff}$.}
\item{\textbf{Negation:} $\mc{A} \equiv (\lnot \mc{B})$ and $\mc{B}\in \textbf{sWff}$.}
\item{\textbf{Quantification:} $\mc{A} \equiv ((\forall x)\mc{B})$ and $x\in \textbf{Var}$, $\mc{B}\in \textbf{sWff}$, $x\in FV(\mc{B})$ and $x\not\in QV(\mc{B})$.}
\end{itemize}
}}

Note that $FV(\mc{A})$ and $QV(\mc{A})$ are defined the same for pWff and sWff.
This is fine because $\textbf{sWff}\subset\textbf{pWff}$.



The formation rules are all straightforward with the exception of quantification.
For the quantification formation rule we include 2 provisos.
The first proviso is that $\mc{B}$ must contain the variable $x$ as a substring.
We will see below that this is equivalent to requiring the variable $x$ to appear in the set of free variables of $\mc{B}$.
This proviso allows us to avoid so-called redundant quantification such as in the expression $((\forall x)((\forall y) P_y))$.
Here the $\forall x$ quantifier does nothing to change the meaning of the expression, and, when rendered into English, the expression doesn't quite make sense.
The second proviso is that we can not quantify twice over the same variable.
This would be double quantification.
This allows us to avoid an expression like $((\forall x)((\forall x)(P_{xx})))$.
It is possible to develop formal logic in ways to allow redundant and double quantification.
However, in the interest of preserving `naturalness' of the language, we restrict to avoid them.
This may have the consequence of making some metalogical proofs and definitions more complicated.

We introduce a number of logical abbreviations to allow us to express the usual full range of logical connectives.

\noindent\fbox{\parbox{\textwidth}{ 
\textbf{Logical Abbreviations}

For Wffs $\mc{A}$ and $\mc{B}$
\begin{itemize}
\item{\textbf{Conjunction}: We let $(\mc{A} \land \mc{B})$ abbreviate $(\lnot(\mc{A} \implies (\lnot \mc{B})))$}
\item{\textbf{Disjunction}: We let $(\mc{A} \lor \mc{B})$ abbreviate $((\lnot \mc{A}) \implies \mc{B})$}
\item{\textbf{Equivalence}: We let $(\mc{A} \iff \mc{B})$ abbreviate $((\mc{A} \implies \mc{B}) \land (\mc{B} \implies \mc{A}))$}
\item{\textbf{Existential Quantification}: We let $((\exists x) \mc{A})$ abbreviate $(\lnot((\forall x)(\lnot \mc{A})))$. .}
\end{itemize}
}}

We also introduce abbreviations for removing parentheses. 
This abbreviation language is copied directly from Tourlakis.

\noindent\fbox{\parbox{\textwidth}{
\begin{itemize}
\item{\textbf{Parentheses:} ``To minimize the use of brackets in the metanotation we adopt standard \textit{priorities} of connectives: $\forall, \exists,$ and $\lnot$ have the highest, and then we have (in decreasing order of priority) $\land, \lor, \implies, \iff$, and we agree not to use outermost brackets. all \textit{associativities} are \textit{right} - that is if we write $\mc{A} \implies \mc{B} \implies \mc{C}$, then it is a (sloppy) counterpart for $(\mc{A} \implies ( \mc{B} \implies \mc{C}))$.''}
\end{itemize}
}}

\subsection{Metalanguage Concepts}

There are a number of metalanguage concepts and manipulations which will be important to the final proof that extensions by definitions are conservative.

The first of these is the concept of the \textit{depth} of a Wff, with the idea that more complex Wffs have a larger depth.
The depth, or complexity, of a Wff will be an integer quantifying the number of steps used to build up that Wff.
Induction on the depth of a Wff will be an important metalogical technique.

\noindent\fbox{\parbox{\textwidth}{
\textbf{Wff Depth}

We define inductively the depth of a pWff $\mc{A}$.

\begin{itemize}
\item{If $\mc{A}$ is an atomic formula then $D(\mc{A}) = 0$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ then $D(\mc{A}) = D(\mc{B}) + 1$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $D(\mc{A}) = \text{max}(D(\mc{B}) + D(\mc{C})) + 1$.}
\item{If $\mc{A} \equiv ((\forall x) \mc{B})$ then $D(\mc{A}) = D(\mc{B}) + 1$.}
\end{itemize}
}}

We can see plainly (and could prove by induction) that the depth of a Wff is equivalent to the total number of logical operators and quantifiers in that Wff.

\newpage
The next metalogical concept is the distinction between free and bound variables.
The terminology developed here will help us discuss the concept of term substitution.

\noindent\fbox{\parbox{\textwidth}{
\textbf{Free Variables}
We define inductively the set of free variables in a term or Wff $\mc{A}$, $FV(\mc{A})$.

\begin{itemize}
\item{if $\mc{A}\equiv x$ is a variable then $FV(\mc{A}) = \{x\}$.}
\item{if $\mc{A} \equiv ft_1\ldots t_n$ then $FV(\mc{A}) = FV(t_1)\cup\ldots\cup FV(t_n)$. Note that this implies $FV(f)=\emptyset$ if $f$ is a 0-ary function.}
\item{if $\mc{A} \equiv Pt_1\ldots t_n$ for $n$-ary predicate $P$ then $FV(\mc{A}) = FV(t_1)\cup\ldots\cup FV(t_n)$. Note that this implies $FV(P)=\emptyset$ if $P$ is a 0-ary predicate such as $\curlywedge$.}
\item{if $\mc{A} \equiv (\mc{B}\implies \mc{C})$ for Wffs $\mc{B}$ and $\mc{C}$ then $FV(\mc{A}) = FV(\mc{B})\cup FV(\mc{C})$.}
\item{if $\mc{A} \equiv (\lnot\mc{B})$ then $FV(\mc{A}) = FV(\mc{B})$.}
\item{if $\mc{A} \equiv ((\forall x)\mc{B})$ then $FV(\mc{A}) = FV(\mc{B}) - \{x\}$. Recall that $\mc{B}$ must contain $x$ as a substring.}
\end{itemize}
}}

If $FV(\mc{A}) = \{x_1, \ldots, x_n\}$ then we write $\mc{A} = \mc{A}(x_1, \ldots, x_n) = \mc{A}(\bv{x})$.
We say that $x_1, \ldots, x_n$ appear free in $\mc{A}$.
If a Wff contains any free variables then we say that it is \textit{open}.
Otherwise we say the Wff is \textit{closed}.
A closed Wff has had functions or constants plugged in for all variables.

Recall that an instance of a variable $x$ is \textit{bound} within a Wff $\mc{A}$ if it appears in the scope of a quantifier $(\forall x)$.
If an instance of a variable $x$ does not appear within the scope of a quantifier with $\mc{A}$ then we can see that $x$ appears free in $\mc{A}$.

Note that a variable can appear both free and bound in a Wff.
For example, $x$ appears both free and bound in the Wff $(Px \land ((\forall x)Qxy))$.
While this is a valid Wff, it would be less confusing if we instead had $(Px \land ((\forall z)Qzy))$.
These two expressions can be shown to be logically equivalent using the inference rules which will follow.

In terms of semantics, it is always possible to assign a truth value to a closed Wff.
Open Wffs however are trickier.
For example, while it is clear that the sentence `$3=5$' can have a truth value, the sentence `$x=5$' cannot have a truth value because the truth value depends on what value is plugged in for $x$.

\newpage
We are now in a position to carefully define term replacement.

\noindent\fbox{\parbox{\textwidth}{
\textbf{Term Replacement}

Suppose $s, t\in \textbf{Term}$ and $x\in \textbf{Var}$.
\begin{itemize}
\item{If $s \in \textbf{Var}$ then if $s\equiv x$ $s[t/ x] \equiv t$. If $s \not\equiv x$ then $s[t/ x] \equiv s$}
\item{If $s \equiv fs_1\ldots s_n$ with $f\in \textbf{Func}_n$ and $s_1\ldots s_n \in \textbf{Term}$ then \\ $s[t/ x] \equiv fs_1[t/ x]\ldots s_n[t/ x]$. This definition implies that if $f\in \textbf{Func}_0$ then $f[t/ x] \equiv f$.}
\end{itemize}

Suppose $\mc{A}\in \textbf{Wff}$.
\begin{itemize}
\item{If $\mc{A}$ is atomic then $\mc{A}\equiv Ps_1\ldots s_n$ with $P\in \textbf{Pred}_n$ and $s_1\ldots s_n\in \textbf{Term}$. $\mc{A}[t/ x] \equiv Ps_1[t/ x]\ldots s_n[t/ x]$. This definition implies that if $P \in \textbf{Pred}_0$ then $P[t/ x] \equiv P$.}
\item{If $\mc{A} \equiv (\mc{B}\implies \mc{C})$ then $\mc{A}[t/ x] \equiv (\mc{B}[t/ x] \implies \mc{C}[t/ x])$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ then $\mc{A}[t/ x] \equiv (\lnot \mc{B}[t/ x])$.}
\item{If $\mc{A} \equiv ((\forall y)\mc{B})$ then if $y\not \equiv x$ then $\mc{A}[t/ x] \equiv ((\forall y)\mc{B}[t/ x])$. If $y\equiv x$ then $\mc{A}[t/ x] \equiv \mc{A}$.}
\end{itemize}
}}

With some minor work, we can see that if $s$ is a term that $s[t/x]$ is also a term and that if $\mc{A}$ is atomic that $\mc{A}[t/x]$ is also atomic.
We can then perform induction on Wff depth to prove that if $\mc{A}$ is a Wff that $\mc{A}[t/x]$ is also a Wff.

Our rule for replacement in a quantifier Wff says to replace all free occurrences of $x$ by $t$.
We would typically expect the replacement of free variables to have no effect on the semantics of the formula in question.
Unfortunately our definition of term replacement is slightly too general and simple to ensure this.
Consider the expression $\mc{A} \equiv ((\forall y)Pxy)$.
We can construct $\mc{A}[y/x] \equiv ((\forall y)Pyy)$.
We see that the replacement is valid and allowed because $y\not \equiv x$, but unfortunately we can see that $\mc{A}$ and $\mc{A}[y/x]$ are not logically equivalent.
What has happened here is that the variable $y$, which appeared in the replacement term, was captured by the quantifier $(\forall y)$ when replaced into $\mc{A}$.
When we define the rules of inference we will have to take special care to ensure no capture of replacement terms occurs.

An alternative approach would be to add a proviso to the term replacement rule that no replacement occurs if $y$ appears in $t$.
However, we will take the philosophical view that replacement should be a simple operation on Wffs independent of the anticipated logical manipulation.
Instead we will define the concept of substitutability.

\newpage
The idea for term substitutability is that if construction $\mc{A}[t/x]$ results in no capture of any variables then we would like to say $t$ is substitutable for $x$ in $\mc{A}$.
In this case we write $\mc{S}(\mc{A}, t, x)$.
If it is the case that $\mc{S}(\mc{A}, t, x)$ then we notationally indicate $\mc{A} \equiv _{[t/x]}\mc{A}$.
We define substitutability as follows.

\noindent\fbox{\parbox{\textwidth}{
\textbf{Term Substitutability}

\begin{itemize}
\item{If $\mc{A}$ is an atomic formula then $\mc{S}(\mc{A}, t, x)$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $\mc{S}(\mc{A}, t, x)$ if $\mc{S}(\mc{B}, t, x)$ and $\mc{S}(\mc{C}, t, x)$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ then $\mc{S}(\mc{A}, t, x)$ if $\mc{S}(\mc{B}, t, x)$.}
\item{If $\mc{A} \equiv ((\forall y)\mc{B})$ then $\mc{S}(\mc{A}, t, x)$ if $\mc{S}(\mc{B}, t, x)$ and $y\not\in FV(t)$ or $x\not \in FV(\mc{B})$.}
\end{itemize}
}}

The first proviso on substitutability in a quantifier expression ensures that if a replacement is made that there is no capture of a variable $y$ that might appear in $t$.
In particular, note that if $t$ is a closed term then we always have $\mc{S}(\mc{A}, t, x)$

The second proviso on substitutability in a quantifier expression allows for the substitution to take place if $x \not\in FV(\mc{B})$ even if $y\in FV(t)$.
This is allowed because the substitution will not change the overall expression $\mc{A}$ since $x$ does not appear in $\mc{B}$.

Finally, it will also be valuable for us to introduce notation for when a term does not appear in an expression.
If $t$ does not appear in $\mc{A}$ we write $_{\{t\}}\mc{A} \equiv \mc{A}$.
If $t$ does not appear in $\mc{A}$ and $t$ is substitutable for $x$ in $\mc{A}$ then we write $_{[t/x], \{t\}}\mc{A}$.

The next (final?) metalanguage concept we will introduce is that of sub-Wff replacement.
This will allow us to perform the operation of replacing an atomic formula, or sub-Wff within a Wff, by another Wff.

\noindent\fbox{\parbox{\textwidth}{
\textbf{Wff Substitution}

If $\mc{A}$ is a Wff, $\mc{X}$ is an atomic formula and $\mc{P}$ is a Wff then we define the replacement of $\mc{X}$ by $\mc{P}$ in $\mc{A}$, notated as $\mc{A}[\mc{P}/\mc{X}]$ by:

\begin{itemize}
\item{If $\mc{A}$ is atomic then if $\mc{A}\equiv \mc{X}$ then $\mc{A}[\mc{P}/\mc{X}] \equiv \mc{P}$. If $\mc{A}\not\equiv \mc{X}$ then $\mc{A}[\mc{P}/\mc{X}] \equiv \mc{A}$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ the $\mc{A}[\mc{P}/\mc{X}] \equiv (\lnot \mc{B}[\mc{P}/\mc{X}])$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $\mc{A}[\mc{P}/\mc{X}] \equiv (\mc{B}[\mc{P}/\mc{X}] \implies \mc{C}[\mc{P}/\mc{X}])$.}
\item{If $\mc{A} \equiv ((\forall x)\mc{B})$ then $\mc{A}[\mc{P}/\mc{X}] \equiv ((\forall x) (\mc{B}[\mc{P}/\mc{X}])$.}
\end{itemize}
}}

Above, we saw that in term substitution we always ended up with either a term or a Wff after the term substitution took place.
However, it was necessary to define the concept of substitutability to highlight substitutions that did or didn't fundamentally change the structure of a Wff.
In particular, it was important to be able to rule out substitution which would result in variable capture.

The case for Wff substitution is even more drastic.
Wff substitution as defined above does not even always result in a Wff.
There are three ways $\mc{A}[\mc{P}/\mc{X}]$ can be problematic.
The first one results in redundant quantification, the second results in repeated quantification, and the third results in variable capture.

\begin{itemize}
\item{If $\mc{X}$ is within the scope of a quantifier $(\forall x)$ but $x\not \in FV(\mc{P})$, then it is possible that after substitution of $\mc{P}$ for $\mc{X}$ that the quantifier $(\forall x)$ will not quantify over any variable $x$ resulting in redundant quantification. For example, let $\mc{X} \equiv Q_x$, $\mc{P} \equiv P$ and $\mc{A} \equiv ((\forall x) Q_x)$. Then $\mc{A}[\mc{P}/\mc{X}] \equiv ((\forall x) P)$. We can see this is not a valid Wff because $(\forall x)$ is a redundant quantifier. Note however that, even though $FV(\mc{X})$ contains an element not in $FV(\mc{P})$ it is not always the case that replacement of $\mc{X}$ by $\mc{P}$ will result in redundant quantification. For example if $\mc{A} \equiv ((\forall x)(Q_x \land S_x))$ then $\mc{A}[\mc{P}/\mc{X}] \equiv ((\forall x)(P\land S_x))$ which is in fact a Wff with no redundant quantification.}
\item{If $\mc{X}$ appears in $\mc{A}$ within the scope of some quantifier $(\forall x)$ and $\mc{P}$ quantifies over that same variable then $\mc{A}[\mc{P}/\mc{X}]$ will have repeated quantification. For example if $\mc{X} \equiv Q$, $\mc{P} \equiv ((\forall y) P_y)$ and $\mc{A} \equiv ((\forall y) (Q\land S_y))$ then $\mc{A}[\mc{P}/\mc{X}] \equiv ((\forall y)(((\forall y)P_y)\land S_y))$ which we can see is not a Wff because we have repeated quantification of $(\forall y)$.}
\item{Finally, if $\mc{X}$ is within the scope of some quantifier $(\forall x)$ but $x\not \in FV(\mc{X})$ but $x\in FV(\mc{P})$ then the free occurrences of $x$ in $\mc{P}$ will become captured after substitution. For example if $\mc{X} \equiv Q$, $\mc{P} \equiv P_x$ and $\mc{A} \equiv ((\forall x) (Q\land R_x))$ then $\mc{A} \equiv ((\forall x)(P_x\land R_x))$.}
\end{itemize}

We can always avoid the first and final issues by requiring that $FV(\mc{P}) = FV(\mc{X})$ when performing atomic formula substitution.
The second issue can be avoided by noting (as we will prove later) that any Wff $\mc{P}$ is equivalent to one in which all bound variables are replaced by new, fresh, bound variables which do not appear in $\mc{A}$.

We saw above that in term substitution it was necessary to define the concept of substitutability on top of substitution.
In the case of term substitution it was always the case that term substitution resulted in either a term or a Wff after substitution

Similar to the case 



Note that $\mc{A}[\mc{P}/\mc{X}]$ is only a Wff if none of the quantifiers with $\mc{P}$ result in repeated quantification after substitution in $\mc{A}$.
For example consider

$$
\mc{A} \equiv ((\forall x) (Q_x \land \mc{X}))
$$

with $Q_x$ a 1-ary predicate and $\chi$ a 0-ary predicate.
Let $\mc{P} \equiv ((\forall x) S_x)$.
We see that $FV(\mc{X}) = FV(\mc{P}) = \emptyset$ so $\mc{A}[\mc{P}/\mc{X}]$ is define as above by

$$
\mc{A}[\mc{P}/\mc{X}] \equiv ((\forall x)(Q_x \land ((\forall x) S_x)))
$$

But we can see that this is not a Wff because of the repeated quantification over $x$.
This issue 

Similarly to term replacement, we must also define a concept of substitutability for atoms. We say $\mc{S}(\mc{A}, \mc{P}, \mc{X})$ to mean that $\mc{P}$ is substitutable for $\mc{X}$ in $\mc{A}$. $\mc{S}(\mc{A}, \mc{P}, \mc{X})$ if (1) if $(\forall x)$ appear in $\mc{P}$ that $\mc{X}$ is not within the scope of any $(\forall x)$ quantifier in $\mc{A}$ and (2) no  

\subsubsection{Draft}

Bound Variable Substitution 

\noindent\fbox{\parbox{\textwidth}{
\textbf{Bound Variable Substitution}

If $\mc{A}$, $\mc{B}$, and $\mc{C}$ are Wffs and $x$ and $y$ are variables then we define $\mc{A}\{y/x\}$ as

\begin{itemize}
\item{If $\mc{A}$ is an atomic formula then $\mc{A}\{y/x\} \equiv \mc{A}$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $\mc{A}\{y/x\} \equiv (\mc{B}\{y/x\} \implies \mc{C}\{y/x\})$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ then $\mc{A}\{y/x\} \equiv (\lnot \mc{B}\{y/x\})$.}
\item{If $\mc{A} \equiv ((\forall z) \mc{B})$ then if $x \equiv z$ then $\mc{A}\{y/x\} \equiv ((\forall y) \mc{B}[y/z])$ and if $x\not \equiv z$ then $\mc{A}\{y/x\} \equiv \mc{A}$.}
\end{itemize}
}}

\noindent\fbox{\parbox{\textwidth}{
\textbf{Bound Variable Substitutability}

If $\mc{A}$, $\mc{B}$, and $\mc{C}$ are Wffs and $x$ and $y$ are variables then we define $\mc{A}\{y/x\}$ as

\begin{itemize}
\item{If $\mc{A}$ is an atomic formula then $S_B(\mc{A}, y, x)$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $S_B(\mc{A}, y, x)$ if $S_B(\mc{B}, y, x)$ and $S_B(\mc{C}, y, x)$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ then $S_B(\mc{A}, y, x)$ if $S_B(\mc{B}, y, x)$.}
\item{If $\mc{A} \equiv ((\forall z) \mc{B})$ then if $x \equiv z$ then $S_B(\mc{A}, y, x)$ if $S_F(\mc{B}, y, z)$. If $x\not \equiv z$ then $S_B(\mc{A}, y, x)$.}
\end{itemize}
}}

Proof that if $S_B(\mc{A}, y, x)$ then $\mc{A}\{y/x\}$ is a Wff. We induct on $D(\mc{A})$

Base case: If $D(\mc{A}) = 0$ then $\mc{A}$ is an atomic formula. Then $\mc{A}\{y/x\}$ is also an atomic formula so $\mc{A}\{y/x\}$ is a Wff.

Induction: Assume that, for any Wff $\mc{B}$, that if $D(\mc{B}) < n$ then $\mc{B}\{y/x\}$ is a Wff. Now assume $D(\mc{A}) = n$

If $\mc{A} \equiv (\lnot \mc{B})$ then $D(\mc{B}) = D(\mc{A}) - 1 < n$. $\mc{A}\{y/x\} \equiv (\lnot \mc{B}\{y/x\})$ but $\mc{B}\{y/x\}$ is a Wff so $\mc{A}\{y/x\}$ is also a Wff.

If $\mc{A} \equiv (\mc{B}\implies \mc{C})$ then $D(\mc{A}) = D(\mc{B}) + d(\mc{C}) + 1$ so $D(\mc{B}), D(\mc{C}) < \mc{A}$ so $\mc{B}\{y/x\}$ and $\mc{C}\{y/x\}$ are Wffs by induction hypothesis. $\mc{A}\{y/x\} \equiv (\mc{B}\{y/x\} \implies \mc{C}\{y/x\})$. Since $\mc{B}\{y/x\}$ and $\mc{C}\{y/x\}$ are Wffs we have that $\mc{A}\{y/x\}$ is a Wff.

Now suppose $\mc{A} \equiv ((\forall z)\mc{B})$. if $x\equiv z$ then $\mc{A}\{y/x\} \equiv ((\forall y)\mc{B}[y/x])$. For this to be a Wff we must have that $\mc{B}[y/z]\equiv$ is a Wff, $\mc{B}[y/z]$ contains $y$, but does not contain $(\forall y)$. 

$\mc{B}[y/z]$ is a Wff because $\mc{B}$ is a Wff. <<need some proofs about term substitution to prove that since $z$ appears in $\mc{B}$ (because it's in the scope of $\forall z$ in $\mc{A}$) that $y$ appears in $\mc{B}[y/z]$. I think this will proceed by showing that $z \in FV(\mc{B})$ and that $FV(\mc{B}[y/z]) = FV(\mc{B})\backslash \{z\} \cup \{y\}$ so that $y\in FV(\mc{B}[y/z])$ and if $y\in FV(\mc{B}[y/z])$ then $y$ appears in $\mc{B}[y/z]$>>

But, because $S_B(\mc{A}, y, x)$, we have that $S_F(\mc{B}, y, z)$.


\newpage

\section{Formal Proof}
\subsection{Sequents}
Syntactically, we represent the idea that Wff $\mc{A}$ logically follows from a set of Wffs $\Gamma$ using a sequent, or judgment, which we express as

$$\Gamma \vdash \mc{A}$$.

The symbol in the middle, $\vdash$, is the turnstile symbol.
The Wffs in the set $\Gamma$ on the left hand side are called the antecedents of the sequent and the Wff $\mc{A}$ on the right hand side is called the subsequent.
We say the subsequent $\mc{A}$ is derivable from the antecedents $\Gamma$.
A more concrete example of a sequent is 

$$
\{(\mc{A}\implies \mc{B}), \mc{A}\} \vdash \mc{B}.
$$

We will use a shorthand notation for the antecedents of sequents.
For sets of Wffs labeled $\Gamma_i$ and for Wffs $\mc{A}_i$ we can write

\begin{align}
\Gamma_1, \ldots, \Gamma_n, \mc{A}_1, \ldots, \mc{A}_n \vdash \mc{B}
\end{align}

as an abbreviation for

\begin{align}
\Gamma_1 \cup \ldots \cup \Gamma_n \cup \{\mc{A}_1\} \cup \ldots \cup \{\mc{A}_n\} \vdash \mc{B}
\end{align}

That is Wffs are replaced by singleton sets containing those Wffs and commas are replaced by the set union operation.

In some of what follows it will be important to indicate that a particular term, $t$, doesn't appear free in any of a set of Wffs. If $\Gamma = \{\mc{A}_1, \ldots , \mc{A}_n\}$ where $t$ doesn't appear free in any of $\mc{A}_i$ (that is $_{\{t\}}\mc{A}_i$) then we let $\Gamma \equiv _{\{t\}}\Gamma$.

$\Gamma_1 \backslash \Gamma_2$ denotes the subtraction of set $\Gamma_2$ from $\Gamma_1$.

In what follows, we will develop our proof theoretic framework which is essentially a calculus of the $\vdash$ symbol.
The framework will be based around rules of inference which allow us to derive new sequents from old sequents.

\subsection{Proof Table}

In logical proofs we begin with a set of premises, and, through logical manipulations, we draw new conclusions.
In the context of syntactic proof theory the premises serve as the starting point for a logical symbol game and rules of inference allows us to infer new logical statements dependent on the premises and other derived statements.

I will present proofs in a tabular format with premises listed at the top of the table.
The steps of the proof will be written as new lines in the table, where each line must be justified by some rule of inference which may involve earlier lines. 

One of the simplest example of a logical proof would be

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{2cm} }
(1) & $\Gamma \vdash (\mc{A} \land \mc{B})$ & Prem\\
(2) & $\Gamma \vdash \mc{A}$ & $\land E$, 1\\
\end{tabular}
\end{center}

We see that each line of the proof is numbered at the far left.
In the center of each line we see a sequent with a set of antecedents and a subsequent.
At the right we see the logical justification for writing down each line.
For the first line in the proof above the logical justification is that we take that sequent as a premise.
In this case no further logical justification is needed.
On the second line we have leveraged the rule of $\land E$, shorthand for `conjunction elimination' and applied it to the first line.
This is an example of one of the inference rules which we be described shortly.

All of the premises for a proof must be written as the first lines in the proof.
If we have any proof table which obeys the rules of inference faithfully, then for any line of the proof after the premises, we say that the proof table constitutes a proof of the sequent on that line from the set of premise sequents.

If a sequent appear on line $(n)$ of a proof that has $k$ premises then we say the proof table constitutes a proof of length $n-k$ of the sequent on line $(n)$ from the $k$ premises.

In what follows we will demand that all of the all of the Wffs in our proofs are closed.
That is to say, semantically, we would like to deal with Wffs which could have truth values.
Again, semantically, a proof would say something like: If the sets of statements $\Gamma$ are true then some other statement $\mc{A}$ will be true.
Recall that only closed statements can be easily assigned clear truth values.
This does slightly limit our proof structure.
For example, we can't prove $((x>5) \implies (x>3))$.
Instead we must prove $((\forall x)((x>5) \implies (x>3))$.

\subsection{Inference Rules}

Below I will write down all of the native inference rules for this system of natural deduction.
Each of these rules can be used in a formal proof as described above.

As explained, above, each line of our proof will express a sequent.
The proof will then derive new sequents from old sequents.
Alternatively, it is possible to develop a natural deduction in which the lines of the proofs are Wffs rather than sequents.
The advantage of deriving new sequents from old sequents, as opposed to simply deriving new Wffs from old Wffs, is that it is natural to keep track of on which premises any particular conclusion relies.

The rules of inference will be notated as one set of sequents above a line and one sequent below the line with the idea that the set of sequents above the line (the premises) logically allow the deduction of the sequent below the line (the conclusion). 

\newpage

\hrulefill

\textbf{Rule of Prem (Premise)}

\begin{prooftree}
\AxiomC{}
\UnaryInfC{$\Gamma \vdash \mc{A}$}
\end{prooftree}

We can start a proof with any (closed) sequent as a premise. Premises must appear at the top lines of a proof.

\hrulefill

\textbf{Rule of A (Assumption)}

\begin{prooftree}
\AxiomC{}
\UnaryInfC{$\mc{A}\vdash \mc{A}$}
\end{prooftree}

We can always assume that a (closed) Wff $\mc{A}$ derives itself from no premises.

\hrulefill

\textbf{Rule of W (Weakening)}

\begin{prooftree}
\AxiomC{$\Gamma_1 \vdash \mc{A}$}
\UnaryInfC{$\Gamma_1, \Gamma_2 \vdash \mc{A}$}
\end{prooftree}

Note that $\Gamma_2$ may be the empty set. This allows us to derive a sequent that has already been derived on a later line. This will be useful to allow to rewrite the same sequent with the inclusion or exclusions of convenient abbreviations. ($\Gamma_2$ must be closed)

\hrulefill

\textbf{Rule of $\implies I$ (Implication Introduction)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash \mc{B}$}
\UnaryInfC{$\Gamma\backslash \{\mc{A}\} \vdash (\mc{A} \implies \mc{B})$}
\end{prooftree}

Note that $\mc{A}$ need not appear in $\Gamma$ for this inference rule. ($\mc{A}$ must be closed).

\hrulefill

\textbf{Rule of $\implies E$ (Implication Elimination)}

\begin{prooftree}
\AxiomC{$\Gamma_1 \vdash (\mc{A} \implies \mc{B})$}
\AxiomC{$\Gamma_2 \vdash \mc{A}$}
\BinaryInfC{$\Gamma_1, \Gamma_2 \vdash \mc{B}$}
\end{prooftree}

\hrulefill

\textbf{Rule of $\lnot E$ (Negation Elimination)}

\begin{prooftree}
\AxiomC{$\Gamma_1 \vdash \mc{A}$}
\AxiomC{$\Gamma_2 \vdash (\lnot \mc{A})$}
\BinaryInfC{$\Gamma_1, \Gamma_2 \vdash \curlywedge$}
\end{prooftree}

\hrulefill

\textbf{Rule of $\lnot I$ (Negation Introduction)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash \curlywedge$}
\UnaryInfC{$\Gamma \backslash \{\mc{A}\} \vdash (\lnot \mc{A})$}
\end{prooftree}

Note that $\mc{A}$ need not appear in $\Gamma$. ($\mc{A}$ must be closed.)

\hrulefill

\textbf{Rule of $DN$ (Double Negation)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash (\lnot(\lnot \mc{A}))$}
\UnaryInfC{$\Gamma \vdash \mc{A}$}
\end{prooftree}

\hrulefill

\textbf{Rule of $\forall I$ (Universal Introduction/Generalization)}

\begin{prooftree}
\AxiomC{$_{\{t\}}\Gamma \vdash \left(_{[t/x], \{t\}}\mc{A}\right)[t/x]$}
\UnaryInfC{$\Gamma \vdash ((\forall x) \mc{A})$}
\end{prooftree}

$x$ must be in $FV(\mc{A})$.

\hrulefill

\textbf{Rule of $\forall E$ (Universal Elimination/Instantiation/Specification)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash ((\forall x)_{[t/x]}\mc{A})$}
\UnaryInfC{$\Gamma \vdash \mc{A}[t/x]$}
\end{prooftree}

\hrulefill

Recall that when we have a proof table we say can say that we have proven the sequent appearing on any line (typically the last line) from the premises.
Whenever we prove one sequent from some set of sequents we can introduce a new derived inference rule.
Specifically, if we have $n$ premises of the form $\Gamma_i \vdash \mc{A}_i$ and we derive $\Theta \vdash \mc{B}$ then we can introduce a new derived inference rule:

\hrulefill

\textbf{Rule of [Der] (Derived Inference Rule)}

\begin{prooftree}
\AxiomC{$\Gamma_1 \vdash \mc{A}_1, \ldots, \Gamma_n \vdash \mc{A}_n$}
\UnaryInfC{$\Theta \vdash \mc{B}$}
\end{prooftree}

\hrulefill

A derived inference rule can be thought of as an abbreviation for the proof that justifies the introduction of that derived rule.
That is, rather than repeat the proof of the derived inference in a new proof, one can skip the proof and simply use the derived inference rule.

\newpage

\section{Deriving Inference Rules for Abbreviated Logical Symbols}

We will now derive the inference rules for the abbreviation logical symbols $\land, \lor, \iff$ and $\exists$.
We will first introduce some convenience inference rules.

\subsection{Convenience Inference Rules}

\subsubsection*{Weaker $\implies I$}

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{2cm} }
(1) & $\Gamma, \mc{A} \vdash \mc{B}$ & Prem\\
(2) & $\Gamma/\{\mc{A}\} \vdash (\mc{A} \implies \mc{B})$ & $\implies I$, 1\\
(3) & $\Gamma \vdash (\mc{A} \implies \mc{B})$ & $W$, 2
\end{tabular}
\end{center}

\subsubsection*{Weaker $\lnot I$}

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{2cm} }
(1) & $\Gamma, \mc{A} \vdash \curlywedge$ & Prem\\
(2) & $\Gamma/\{\mc{A}\} \vdash (\lnot \mc{A})$ & $\lnot I$, 1\\
(3) & $\Gamma \vdash (\lnot \mc{A})$ & $W$, 2
\end{tabular}
\end{center}

These gives us the weaker, but useful, versions of the inference rules.
On the surface these inference rules look similar, but they are different in the case that $\mc{A} \in \Gamma$.
This is because $(\Gamma \cup \{\mc{A}\})/\{\mc{A}\} = \Gamma / \{\mc{A}\}$ which is not necessarily equal to $\Gamma$.

\hrulefill

\textbf{Rule of $\implies I_W$ (Implication Introduction - Weak)}

\begin{prooftree}
\AxiomC{$\Gamma, \mc{A} \vdash \mc{B}$}
\UnaryInfC{$\Gamma \vdash (\mc{A} \implies \mc{B})$}
\end{prooftree}

\hrulefill

\textbf{Rule of $\lnot I_W$ (Negation Introduction - Weak)}

\begin{prooftree}
\AxiomC{$\Gamma, \mc{A} \vdash \curlywedge$}
\UnaryInfC{$\Gamma \vdash (\lnot \mc{A})$}
\end{prooftree}

\hrulefill

\subsection{Modus Tollens}

I'll prove three forms of Modus Tollens and give three corresponding derived rules.

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{3cm} }
(1) & $\Gamma_1 \vdash (\mc{A} \implies \mc{B})$ & Prem\\
(2) & $\Gamma_2 \vdash (\lnot \mc{B})$ & Prem\\
(3) & $\mc{A} \vdash \mc{A}$ & A\\
(4) & $\Gamma_1, \mc{A} \vdash \mc{B}$ & $\implies E$, 1, 3\\
(5) & $\Gamma_1, \Gamma_2, \mc{A} \vdash \curlywedge$ & $\lnot E$, 2, 4\\
(6) & $\Gamma_1, \Gamma_2 \vdash (\lnot \mc{A})$ & $\lnot I_W$, 5
\end{tabular}
\end{center}

From this we get our first form of the Modus Tollens derived inference rule.

\hrulefill

\textbf{Rule of $MT1$ (Modus Tollens)}

\begin{prooftree}
\AxiomC{$\Gamma_1 \vdash (\mc{A} \implies \mc{B})$}
\AxiomC{$\Gamma_2 \vdash (\lnot \mc{B})$}
\BinaryInfC{$\Gamma_1, \Gamma_2 \vdash (\lnot \mc{A})$}
\end{prooftree}

\hrulefill

We can use the derived inference rule to derive two closely related forms of the Modus Tollens inference rule.

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{3cm} }
(1) & $\Gamma \vdash (\mc{A}\implies \mc{B})$ & Prem\\
(2) & $(\lnot \mc{B}) \vdash (\lnot \mc{B})$ & A\\
(3) & $\Gamma, (\lnot \mc{B}) \vdash (\lnot \mc{A})$ & $MT1$, 1, 2\\
(4) & $\Gamma \vdash ((\lnot \mc{B}) \implies (\lnot \mc{A}))$ & $\implies I_W$, 3
\end{tabular}
\end{center}

\hrulefill

\textbf{Rule of $MT2$ (Modus Tollens)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash (\mc{A} \implies \mc{B})$}
\UnaryInfC{$\Gamma, (\lnot \mc{B}) \vdash (\lnot \mc{A})$}
\end{prooftree}

\hrulefill

\textbf{Rule of $MT3$ (Modus Tollens)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash (\mc{A} \implies \mc{B})$}
\UnaryInfC{$\Gamma \vdash ((\lnot \mc{B}) \implies (\lnot \mc{A}))$}
\end{prooftree}

\hrulefill

\subsection{Conjunction Inference Rules}
\subsubsection*{Proof for Right Conjunction Elimination}

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{2cm} }
(1) & $\Gamma \vdash (\mc{A}\land\mc{B})$ & Prem\\
(2) & $\Gamma \vdash (\lnot(\mc{A}\implies(\lnot\mc{B})))$ & $W$, 1\\
(3) & $(\lnot\mc{B}) \vdash (\lnot\mc{B})$ & $A$ \\
(4) & $(\lnot \mc{B}) \vdash (\mc{A} \implies (\lnot \mc{B}))$ & $\implies I$, 3\\
(5) & $\Gamma, (\lnot \mc{B})\vdash \curlywedge$ & $\lnot E$, 2, 4\\
(6) & $\Gamma \vdash (\lnot(\lnot \mc{B}))$ & $\lnot I_W$, 5\\
(7) & $\Gamma \vdash \mc{B}$ & $DN$, 6
\end{tabular}
\end{center}

\hrulefill

\textbf{Rule of $\land E_R$ (Conjunction Elimination - Right)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash (\mc{A} \land \mc{B})$}
\UnaryInfC{$\Gamma \vdash \mc{B}$}
\end{prooftree}


\hrulefill

\subsubsection*{Proof for Left Conjunction Elimination}

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{2cm} }
(1) & $\Gamma \vdash (\mc{A}\land\mc{B})$ & Prem\\
(2) & $\Gamma \vdash (\lnot(\mc{A}\implies(\lnot\mc{B})))$ & $W$, 1\\
(3) & $(\lnot\mc{A})\vdash (\mc{\lnot A})$ & A\\
(4) & $\mc{A} \vdash \mc{A}$ & A\\
(5) & $\mc{A}, (\lnot \mc{A}) \vdash \curlywedge$ & $\lnot E$, 3, 4\\
(6) & $\mc{A}, (\lnot \mc{A}) \vdash (\lnot \mc{B})$ & $\lnot I$, 5\\
(7) & $(\lnot \mc{A}) \vdash (\mc{A} \implies (\lnot \mc{B})$ & $\implies I$, 6\\
(8) & $\Gamma, (\lnot \mc{A}) \vdash \curlywedge$ & $\lnot E$, 2, 7\\
(9) & $\Gamma \vdash (\lnot (\lnot \mc{A}))$ & $\lnot I_W$, 8\\
(10) & $\Gamma \vdash \mc{A}$ & $DN$ 9
\end{tabular}
\end{center}

\hrulefill

\textbf{Rule of $\land E_L$ (Conjunction Elimination - Left)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash (\mc{A}\land \mc{B})$}
\UnaryInfC{$\Gamma \vdash \mc{A}$}
\end{prooftree}

\hrulefill

\subsubsection*{Proof for Conjunction Introduction}

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{3cm} }
(1) & $\Gamma_1 \vdash \mc{A}$ & Prem\\
(2) & $\Gamma_2 \vdash \mc{B}$ & Prem\\
(3) & $(\mc{A} \implies (\lnot \mc{B})) \vdash (\mc{A} \implies (\lnot \mc{B}))$ & A\\
(4) & $\Gamma_1, (\mc{A} \implies (\lnot \mc{B})) \vdash (\lnot \mc{B})$ & $\implies E$, 1, 3\\
(5) & $\Gamma_1, \Gamma_2, (\mc{A} \implies (\lnot \mc{B})) \vdash \curlywedge$ & $\lnot E$, 2, 4\\
(6) & $\Gamma_1, \Gamma_2 \vdash (\lnot (\mc{A} \implies (\lnot \mc{B})))$ & $\lnot I_W$ 5\\
(7) & $\Gamma_1, \Gamma_2 \vdash (\mc{A} \land \mc{B})$ & $W$, 6
\end{tabular}
\end{center}

\hrulefill

\textbf{Rule of $\land I$ (Conjunction Introduction)}

\begin{prooftree}
\AxiomC{$\Gamma_1 \vdash \mc{A}$}
\AxiomC{$\Gamma_2 \vdash \mc{B}$}
\BinaryInfC{$\Gamma_1, \Gamma_2 \vdash (\mc{A} \land \mc{B})$}
\end{prooftree}

\hrulefill
\subsection{Disjunction Inference Rules}
\subsubsection*{Proof for Disjunction Elimination}

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{3cm} }
(1) & $\Gamma_1 \vdash (\mc{A} \lor \mc{B})$ & Prem\\
(2) & $\Gamma_2 \vdash (\mc{A} \implies \mc{C})$ & Prem\\
(3) & $\Gamma_3 \vdash (\mc{B} \implies \mc{C})$ & Prem\\
(4) & $\Gamma_1 \vdash ((\lnot \mc{A}) \implies \mc{B})$ & $W$, 1\\
(5) & $(\lnot \mc{C}) \vdash (\lnot \mc{C})$ & A\\
(6) & $\Gamma_2, (\lnot \mc{C}) \vdash (\lnot \mc{A})$ & $MT$, 2, 5\\
(7) & $\Gamma_1, \Gamma_2, (\lnot \mc{C}) \vdash \mc{B}$ & $\implies E$, 4, 6\\
(8) & $\Gamma_1, \Gamma_2, \Gamma_3, (\lnot \mc{C}) \vdash \mc{C}$ & $\implies E$, 3, 7\\
(9) & $\Gamma_1, \Gamma_2, \Gamma_3, (\lnot \mc{C}) \vdash \curlywedge$ & $\lnot E$, 5, 8\\
(10) & $\Gamma_1, \Gamma_2, \Gamma_3 \vdash (\lnot(\lnot \mc{C}))$ & $\lnot I_W$, 9\\
(11) & $\Gamma_1, \Gamma_2, \Gamma_3 \vdash \mc{C}$ & $DN$, 10
\end{tabular}
\end{center}

\subsubsection*{Proof for Left Disjunction Introduction}

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{3cm} }
(1) & $\Gamma \vdash \mc{A}$ & Prem\\
(2) & $(\lnot \mc{A}) \vdash (\lnot \mc{A})$ & A\\
(3) & $\Gamma, (\lnot \mc{A}) \vdash \curlywedge$ & $\lnot E$, 1, 2\\
(4) & $\Gamma/\{(\lnot \mc{B})\}, (\lnot \mc{A}) \vdash (\lnot (\lnot \mc{B}))$ & $\lnot I$, 3\\
(5) & $\Gamma/\{(\lnot \mc{B})\}, (\lnot \mc{A}) \vdash \mc{B}$ & $DN$, 4\\
(6) & $\Gamma/\{(\lnot \mc{B})\} \vdash ((\lnot \mc{A}) \implies \mc{B})$ & $\implies I_W$, 5\\
(7) & $\Gamma \vdash ((\lnot \mc{A}) \implies \mc{B})$ & $W$, 6\\
(8) & $\Gamma \vdash (\mc{A} \lor \mc{B})$ & $W$, 7
\end{tabular}
\end{center}


\subsubsection*{Proof for Right Disjunction Introduction}

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{3cm} }
(1) & $\Gamma \vdash \mc{B}$ & Prem\\
(2) & $\Gamma/\{(\lnot \mc{A})\} \vdash ((\lnot \mc{A}) \implies \mc{B})$ & $\implies I$, 1\\
(3) & $\Gamma \vdash ((\lnot \mc{A}) \implies \mc{B})$ & $W$, 2\\
(4) & $\Gamma \vdash (\mc{A} \lor \mc{B})$ & $W$, 3
\end{tabular}
\end{center}

With this we introduce the inference rules for disjunction:

\hrulefill

\textbf{Rule of $\lor E$ (Disjunction Elimination)}

\begin{prooftree}
\AxiomC{$\Gamma_1 \vdash (\mc{A}\lor \mc{B})$}
\AxiomC{$\Gamma_2 \vdash (\mc{A} \implies \mc{C})$}
\AxiomC{$\Gamma_3 \vdash (\mc{B} \implies \mc{C})$}
\TrinaryInfC{$\Gamma_1, \Gamma_2, \Gamma_3 \vdash \mc{C}$}
\end{prooftree}

\hrulefill

\textbf{Rule of $\lor I_L$ (Disjunction Introduction - Left)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash \mc{A}$}
\UnaryInfC{$\Gamma \vdash (\mc{A}\lor \mc{B})$}
\end{prooftree}

\hrulefill

\textbf{Rule of $\lor I_R$ (Disjunction Introduction - Right)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash \mc{B}$}
\UnaryInfC{$\Gamma \vdash (\mc{A}\lor \mc{B})$}
\end{prooftree}

\hrulefill

\subsection{Biconditional Inference Rules}
\subsubsection*{Proof for Biconditional Introduction}

\begin{center}
\begin{tabular}{ p{1cm} p{7cm} p{3cm} }
(1) & $\Gamma_1 \vdash (\mc{A} \implies \mc{B})$ & Prem\\
(2) & $\Gamma_2 \vdash (\mc{B} \implies \mc{A})$ & Prem\\
(3) & $\Gamma_1, \Gamma_2 \vdash ((\mc{A} \implies \mc{B}) \land (\mc{B}\implies \mc{A}))$ & $\land I$, 1, 2\\
(4) & $\Gamma_1, \Gamma_2 \vdash (\mc{A} \iff \mc{B})$ & $W$, 3
\end{tabular}
\end{center}

\subsubsection*{Proof for Left Biconditional Elimination}
\begin{center}
\begin{tabular}{ p{1cm} p{7cm} p{3cm} }
(1) & $\Gamma \vdash (\mc{A} \iff \mc{B})$ & Prem\\
(2) & $\Gamma \vdash ((\mc{A} \implies \mc{B}) \land (\mc{B} \implies \mc{A}))$ & $W$, 1\\
(3) & $\Gamma \vdash (\mc{A} \implies \mc{B})$ & $\land E_L$, 2\\
(4) & $\Gamma \vdash (\mc{B} \implies \mc{A})$ & $\land E_R$, 2
\end{tabular}
\end{center}

We get two new inference rules from the last two lines of this proof.
The inference rules for biconditional are

\hrulefill

\textbf{Rule of $\iff I$ (Biconditional Introduction)}

\begin{prooftree}
\AxiomC{$\Gamma_1 \vdash (\mc{A}\implies\mc{B})$}
\AxiomC{$\Gamma_2 \vdash (\mc{B} \implies \mc{A})$}
\BinaryInfC{$\Gamma_1, \Gamma_2 \vdash (\mc{A} \iff \mc{B})$}
\end{prooftree}

\hrulefill

\textbf{Rule of $\iff E_L$ (Biconditional Elimination - Left)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash (\mc{A} \iff \mc{B})$}
\UnaryInfC{$\Gamma \vdash (\mc{A} \implies \mc{B})$}
\end{prooftree}

\hrulefill

\textbf{Rule of $\iff E_R$ (Biconditional Elimination - Right)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash (\mc{A} \iff \mc{B})$}
\UnaryInfC{$\Gamma \vdash (\mc{B} \implies \mc{A})$}
\end{prooftree}

\hrulefill

\subsection{Inference Rules for Existence}

We now must derive the inference rule for existence introduction and elimination.
For reference I repeat the rules for universal introduction and elimination.

\hrulefill

\textbf{Rule of $\forall I$ (Universal Introduction/Generalization)}

\begin{prooftree}
\AxiomC{$_{\{t\}}\Gamma \vdash {_{[t/x], \{t\}}\mc{A}}[t/x]$}
\UnaryInfC{$\Gamma \vdash ((\forall x) \mc{A})$}
\end{prooftree}

\hrulefill

\textbf{Rule of $\forall E$ (Universal Elimination/Instantiation/Specification)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash ((\forall x)_{[t/x]}\mc{A})$}
\UnaryInfC{$\Gamma \vdash \mc{A}[t/x]$}
\end{prooftree}

\hrulefill

The rules for existential introduction and elimination will be

\hrulefill

\textbf{Rule of $\exists I$ (Existential Introduction/Generalization)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash {_{[t/x]}\mc{A}}[t/x]$}
\UnaryInfC{$\Gamma \vdash ((\exists x)\mc{A})$}
\end{prooftree} 

\hrulefill

\textbf{Rule of $\exists E$ (Existential Elimination/Instantiation/Specification)}

\begin{prooftree}
\AxiomC{$\Gamma_1 \vdash ((\exists x)\mc{A})$}
\AxiomC{$_{\{t\}}\Gamma_2 \vdash ({_{[t/x], \{t\}}\mc{A}}[t/x] \implies {_{\{t\}}\mc{B}})$}
\BinaryInfC{$\Gamma_1, \Gamma_2 \vdash \mc{B}$}
\end{prooftree}

\hrulefill

\subsubsection*{Proof for Existential Introduction}

\begin{center}
\begin{tabular}{ p{1cm} p{7cm} p{3cm} }
(1) & $\Gamma \vdash {_{[t/x]}\mc{A}}[t/x]$ & Prem\\
(2) & $((\forall x)(\lnot \mc{A})) \vdash ((\forall x)(\lnot \mc{A}))$ & A\\
(3) & $((\forall x)(\lnot \mc{A})) \vdash ((\forall x)(\lnot _{[t/x]}\mc{A}))$ & $W$, 2\\
(4) & $((\forall x)(\lnot \mc{A})) \vdash ((\forall x)_{[t/x]}(\lnot \mc{A}))$ & $W$, 3\\
(5) & $((\forall x)(\lnot \mc{A})) \vdash (\lnot \mc{A})[t/x]$ & $\forall E$, 4\\
(6) & $((\forall x)(\lnot \mc{A})) \vdash (\lnot \mc{A}[t/x])$ & $W$, 5\\
(7) & $\Gamma, ((\forall x)(\lnot \mc{A})) \vdash \curlywedge$ & $\lnot E$, 1, 6\\
(8) & $\Gamma \vdash (\lnot ((\forall x) (\lnot \mc{A})))$ & $\lnot I_W$, 7\\
(9) & $\Gamma \vdash ((\exists x) \mc{A})$ & $W$, 8
\end{tabular}
\end{center}

This proof had some subtlety involving replacement and substitutability.
The inferences from line 1 to line 2 and from line 2 to line 3 follow from the rules for substitutability.
Note that these inferences were labeled as abbreviations.
This is because lines 2, 3, and 4 indicate the same sequents, just with different metalogical annotations.
Similarly, line 6 follows from line 5 by the rules for term replacement.

I repeat the proof above with the metalogical notation carried out implicitly.

\begin{center}
\begin{tabular}{ p{1cm} p{7cm} p{3cm} }
(1) & $\Gamma \vdash \mc{A}[t/x]$ & Prem\\
(2) & $((\forall x)(\lnot \mc{A})) \vdash ((\forall x)(\lnot \mc{A}))$ & A\\
(3) & $((\forall x)(\lnot \mc{A})) \vdash (\lnot \mc{A}[t/x])$ & $\forall E$, 2\\
(4) & $\Gamma, ((\forall x)(\lnot \mc{A})) \vdash \curlywedge$ & $\lnot E$, 1, 3\\
(5) & $\Gamma \vdash (\lnot ((\forall x) (\lnot \mc{A})))$ & $\lnot I_W$, 4\\
(6) & $\Gamma \vdash ((\exists x) \mc{A})$ & $W$, 5
\end{tabular}
\end{center}


\subsubsection*{Proof for Existential Elimination}

\begin{center}
\begin{tabular}{ p{1cm} p{7cm} p{3cm} }
(1) & $\Gamma_1 \vdash ((\exists x)\mc{A})$ & Prem\\
(2) & $_{\{t\}}\Gamma_2 \vdash ({_{[t/x], \{t\}}\mc{A}}[t/x] \implies {_{\{t\}}\mc{B}})$  & Prem\\
(3) & $_{\{t\}}\Gamma_2,  (\lnot {_{\{t\}}\mc{B}}) \vdash (\lnot {_{[t/x], \{t\}}\mc{A}}[t/x])$ & $MT2$, 2\\
(4) & $_{\{t\}} \Gamma_2, {_{\{t\}}(\lnot \mc{B})} \vdash {_{[t/x], \{t\}}(\lnot \mc{A})}[t/x]$ & $W$, 3\\
(5) & $\Gamma_2, (\lnot \mc{B}) \vdash ((\forall x)(\lnot \mc{A}))$ & $\forall I$, 4\\
(6) & $\Gamma_1 \vdash (\lnot ((\forall x)(\lnot \mc{A})))$ & $W$, 1\\
(7) & $\Gamma_1, \Gamma_2, (\lnot \mc{B}) \vdash \curlywedge$ & $\lnot E$, 5, 6\\
(8) & $\Gamma_1, \Gamma_2 \vdash (\lnot(\lnot \mc{B}))$ & $\lnot I_W$, 7\\
(9) & $\Gamma_1, \Gamma_2 \vdash \mc{B}$ & $DN$, 8
\end{tabular}
\end{center}

I repeat this proof with implicit metalogical notation.

\begin{center}
\begin{tabular}{ p{1cm} p{7cm} p{3cm} }
(1) & $\Gamma_1 \vdash ((\exists x)\mc{A})$ & Prem\\
(2) & $\Gamma_2 \vdash (\mc{A}[t/x] \implies \mc{B})$  & Prem\\
(3) & $\Gamma_2,  (\lnot \mc{B}) \vdash (\lnot \mc{A})[t/x]$ & $MT2$, 2\\
(4) & $\Gamma_2, (\lnot \mc{B}) \vdash ((\forall x)(\lnot \mc{A}))$ & $\forall I$, 3\\
(5) & $\Gamma_1 \vdash (\lnot ((\forall x)(\lnot \mc{A})))$ & $W$, 1\\
(6) & $\Gamma_1, \Gamma_2, (\lnot \mc{B}) \vdash \curlywedge$ & $\lnot E$, 4, 5\\
(7) & $\Gamma_1, \Gamma_2 \vdash (\lnot(\lnot \mc{B}))$ & $\lnot I_W$, 6\\
(8) & $\Gamma_1, \Gamma_2 \vdash \mc{B}$ & $DN$, 7
\end{tabular}
\end{center}

These proofs complete the derivation of the inference rules for the existence quantifier.

\section{Theories}

A logical theory $\mc{T}$ consists of a language $\mc{L}$, a set of closed Wffs $\Gamma = \{\gamma_1, \ldots, \gamma_N\}$ in the language $\mc{L}$ which are referred to as the axioms of the theory, and a set of inference rules $\mc{R}$ such as those given above.
A proof in $\mc{T}$ is any proof that begins with $N$ premises of the form $\Gamma \vdash \gamma_i$ and uses the inference rules $\mc{R}$.
If it is possible to prove $\Gamma \vdash \mc{A}$ under these circumstance then we write

$$
\Gamma \vdash_{\mc{T}} \mc{A}
$$

Though we often drop the $\mc{T}$ subscript when the context of the theory is clear.
We would like to collect all of the Wffs which are derivable from $\Gamma$ under the inference rules $\mc{R}$.
If $\Gamma \vdash_{\mc{T}} \mc{A}$ and $\mc{T}$ is the theory with axioms $\Gamma$ and inference rules $\mc{R}$ then we write

$$
\mc{A} \in \Phi_{\mc{T}}
$$

$\Phi_{\mc{T}}$ is the set of all Wffs derivable under theory $\mc{T}$.

\subsection*{Extension}
Suppose $\mc{T}_1$ and $\mc{T}_2$ are theories with languages $\mc{L}_1$ and $\mc{L}_2$, axioms $\Gamma_1$ and $\Gamma_2$ and inference rules $\mc{R}_1$ and $\mc{R}_2$. We say that $\mc{T}_2$ is an extension of $\mc{T}_1$ if $\mc{L}_1 \subset \mc{L}_2$ (this means that $\mc{L}_1$ and $\mc{L}_2$ share their syntax rules but $\mc{L}_2$ has the same, or additional predicate or function symbols compared to $\mc{L}_1$), $\mc{R}_2 = \mc{R}_1$, and $\Gamma_1 \subset \Gamma_2$.
So we see that $\mc{T}_2$ may have more logical symbols than $\mc{T}_1$ and also may have more axioms than $\mc{T}_1$.

\subsection*{Conservative Extension}
If $\mc{T}_2$ is an extension of $\mc{T}_1$ and $\Phi_{\mc{T}_2} \cap \mc{L}_1 = \Phi_{\mc{T}_1}$ then we say that $\mc{T}_2$ is a conservative extension of $\mc{T}_1$.
That is, every Wff which is provable in $\mc{T}_1$ is also provable and $\mc{T}_2$ and every Wff of $\mc{L}_1$ which is provable in $\mc{T}_2$ is also provable in $\mc{T}_1$.

\subsection*{Extension by Definition}
There are two ways we can extend a theory by definition. 
We can either introduce a new predicate or new function symbol and a corresponding axiom.
The concept of extension by function definition requires equality $=$ to be included in the language.
We will not yet include equality so we will save the description of that for later.

\subsubsection*{New Predicate Symbol}

Suppose $\mc{T}_1$ is a theory with language $\mc{L}_1$, axioms $\Gamma_1$ and inference rules $\mc{R}$.
Suppose $P$ is not a symbol in $\mc{L}_1$.
Suppose $\mc{Q}$ is a Wff in $\mc{L}_1$ with $FV(\mc{Q}) = \{x_1, \ldots , x_N\}$.
Let $\mc{L}_2$ be the same as $\mc{L}_1$ but with the addition of the $n$-ary predicate symbol $P$.
Let $\nu\in \mc{L}_2$ be the Wff

\begin{align*}
\nu \equiv& ((\forall x_1) (\ldots (\forall x_N) (P(x_1, \ldots, x_N) \iff \mc{Q}) \ldots ))\\
\equiv& \forall x_1,\ldots,x_N (P(x_1, \ldots, x_N) \iff \mc{Q})
\end{align*}

Let $\mc{R}_2 = \mc{R}_1$ and $\Gamma_2 = \Gamma_1 \cup \{\nu\}$.
The theory $\mc{T}_2$ determined by $\mc{L}_2, \Gamma_2$ and $\mc{R}_2$ is an extension by predicate definition of $\mc{T}_1$.

\section{Temporary section on predicate definition without quantifiers}

Above we defined term replacement.
In addition to replacing terms, it will be valuable for us to be able to replace entire sub-Wffs within a Wff by another Wff.
To this end we define Wff replacement.


If $\mc{A}$ is a Wff, $\mc{X}$ is an atomic formula, and $\mc{P}$ is a Wff then we define the replacement of $\mc{X}$ by $\mc{P}$ in $\mc{A}$, notated as $\mc{A}[\mc{P}/\mc{X}]$, by:

\hrulefill
\begin{itemize}
\item{If $\mc{A}$ is atomic then if $\mc{A}\equiv \mc{X}$ then $\mc{A}[\mc{P}/\mc{X}] \equiv \mc{P}$. If $\mc{A}\not\equiv \mc{X}$ then $\mc{A}[\mc{P}/\mc{X}] \equiv \mc{A}$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ the $\mc{A}[\mc{P}/\mc{X}] \equiv (\lnot \mc{B}[\mc{P}/\mc{X}])$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $\mc{A}[\mc{P}/\mc{X}] \equiv (\mc{B}[\mc{P}/\mc{X}] \implies \mc{C}[\mc{P}/\mc{X}])$.}
\end{itemize}
\hrulefill

It can be easily proven that if $\mc{A}$

\section{Wff Atom Replacement}

If $\mc{A}$ is a Wff, $\mc{X}$ is an atomic formula, and $\mc{P}$ is a Wff then we define the replacement of $\mc{X}$ by $\mc{P}$ in $\mc{A}$, notated as $\mc{A}[\mc{P}/\mc{X}]$, by:

\hrulefill
\begin{itemize}
\item{If $\mc{A}$ is atomic then if $\mc{A}\equiv \mc{X}$ then $\mc{A}[\mc{P}/\mc{X}] \equiv \mc{P}$. If $\mc{A}\not\equiv \mc{X}$ then $\mc{A}[\mc{P}/\mc{X}] \equiv \mc{A}$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ the $\mc{A}[\mc{P}/\mc{X}] \equiv (\lnot \mc{B}[\mc{P}/\mc{X}])$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $\mc{A}[\mc{P}/\mc{X}] \equiv (\mc{B}[\mc{P}/\mc{X}] \implies \mc{C}[\mc{P}/\mc{X}])$.}
\item{If $\mc{A} \equiv ((\forall x)\mc{B})$ then $\mc{A}[\mc{P}/\mc{X}] \equiv ((\forall x) (\mc{B}[\mc{P}/\mc{X}])$.}
\end{itemize}
\hrulefill

Note that $\mc{A}[\mc{P}/\mc{X}]$ is not necessarily a Wff due to constraints with respect to quantification.
For $\mc{A}[\mc{P}/\mc{X}]$ to be a Wff it is necessary that, if $\mc{P}$ has any quantifier $(\forall x)$ that $\mc{X}$ is not within the scope of any $(\forall x)$ quantifier within $\mc{A}$.

Similarly to term replacement, we must also define a concept of substitutability for atoms. We say $\mc{S}(\mc{A}, \mc{P}, \mc{X})$ to mean that $\mc{P}$ is substitutable for $\mc{X}$ in $\mc{A}$. $\mc{S}(\mc{A}, \mc{P}, \mc{X})$ if (1) if $(\forall x)$ appear in $\mc{P}$ that $\mc{X}$ is not within the scope of any $(\forall x)$ quantifier in $\mc{A}$ and (2) no  

\section{Wff Breakdown Tree}

Consider a Wff $\mc{A}$.
In what follows we will be interested in identifying particular Wff substrings of $\mc{A}$ such as $\mc{P}$.
For example, we may be interested in the Wff $\mc{A}$ with one instance of $\mc{P}$ replaced by $\mc{Q}$.
Unfortunately if $\mc{A} \equiv (\mc{P} \implies \mc{P})$, for example, then this sentence is ambiguous. 
To resolve this problem we will come up with a rigorous schematic to uniquely specify an instance of a substring Wff in a main Wff.

We first define the overall depth of a Wff which, in some way, encodes the complexity of the Wff.
We will often induct on the depth of a Wff to prove metalogical theorems.

\hrulefill
\subsubsection*{Wff Depth}

\begin{itemize}
\item{If $\mc{A}$ is an atomic formula then $D(\mc{A}) = 0$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ then $D(\mc{A}) = D(\mc{B}) + 1$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $D(\mc{A}) = \text{max}(D(\mc{B}) + D(\mc{C})) + 1$.}
\item{If $\mc{A} \equiv ((\forall x) \mc{B})$ then $D(\mc{A}) = D(\mc{B}) + 1$.}
\end{itemize}
\hrulefill

We now need the ability to break a Wff down into its constituent parts.
The constituent parts of a Wff are contained in its parsing tree $T(\mc{A})$.
We develop the parsing tree $T(\mc{A})$ by applying a recursive algorithm on the Wff. 
For any Wff we can find it's children by noticing its main connective, or top-level structure.
The elements of the parsing tree will be 4-tuples.
The first element of the tuple is one of $\{\text{Wff}, A, \lnot, \implies, \forall\}$.
This first element indicates how the sub-part is brought into $\mc{A}$. 
The second element is an integer indicating the depth of the part within the overall Wff.
The third element is an integer indicating the branch number of the part within the Wff.
And finally the fourth element is a Wff representing the part.

We extract the children of a top level Wff $\mc{A}$ via $C(\mc{A})$.
If $C(\mc{A})$ is the children tree for $\mc{A}$ then $C_{+n, +m}(\mc{A})$ is the same tree but with $n$ added to the second element of every tuple and $m$ added to the third element of every tuple.


\hrulefill
\begin{itemize}
\item{If $\mc{A}$ is atomic then $C(\mc{A}) = \emptyset$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ then $C(\mc{A}) = \{(\lnot, 1, 1, \mc{B})\} \cup C_{+1, +0}(\mc{B})$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$. Let $b$ equal the maximum branch number in $C(\mc{B})$. then $C(\mc{A}) = \{(\implies_L, 1, 1, \mc{B}), (\implies_R, 1, b+1, \mc{C})\} \cup C_{+1, +0}(\mc{B}) \cup C_{+1, +b}(\mc{C})$.}
\item{If $\mc{A} \equiv ((\forall x)\mc{B})$ then $C(\mc{A}) \equiv \{(\forall, 1, 1, \mc{B})\} \cup C_{+1, +0}(\mc{B})$.}
\end{itemize}
\hrulefill

Finally the total parsing tree for Wff $\mc{A}$ is

$$
T(\mc{A}) = \{(\text{Wff}, 0, 1, \mc{A})\} \cup C(\mc{A})
$$

\subsection{Parsing Tree Unique Numbers}

\subsubsection{Smallest Depth and Branch Number}
Here we will prove that $C(\mc{A})$ contains no depth or branch number less than 1. We prove this by induction on the depth of $\mc{A}$, $D(\mc{A})$.

If $D(\mc{A}) = 1$ the $\mc{A}$ is atomic so $C(\mc{A}) = \emptyset$. There are no depth and branch numbers appearing in $C(\mc{A})$ so the condition is satisfied.

Now for the induction hypothesis we assume that if $D(\mc{B}) < n$ then $C(\mc{A})$ contains no depth or branch numbers less than 1.

Consider $\mc{A}$ with $D(\mc{A}) = n > 1$. $\mc{A}$ must not be atomic since $D(\mc{A}) \neq 1$.

If $\mc{A} \equiv (\lnot \mc{B})$ then $C(\mc{A}) = \{(\lnot, 1, 1, \mc{B})\} \cup C_{+1, +0}(\mc{B})$. Since $D(\mc{B}) < D(\mc{A}) = n$ no depth or branch number smaller than 1 appear in $C(\mc{B})$ by the induction hypothesis. We see then that no depth or branch number smaller than 1 appears in $C(\mc{A})$.

If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $C(\mc{A}) = \{(\implies_L, 1, 1, \mc{B}), (\implies, 1, b+1, \mc{C})\} \cup C_{+1, +0}(\mc{B}) \cup C_{+1, +b}(\mc{C})$ with $b$ the maximum branch number in $C(\mc{B})$. Since $D(\mc{B}), D(\mc{C}) < D(\mc{A}) = n$ no depth or branch number smaller than 1 appears in either $C(\mc{B})$ or $C(\mc{C})$ by the induction hypothesis. We see then that no depth or branch number smaller than 1 appears in $C(\mc{A})$.

If $\mc{A} \equiv ((\forall x)\mc{B})$ then $C(\mc{A}) = \{(\forall, 1, 1, \mc{B})\} \cup C_{+1, +0}(\mc{B})$. Since $D(\mc{B}) < D(\mc{A}) = n$ no depth or branch number smaller than 1 appears in $C(\mc{B})$. We see therefore that no depth or branch number smaller than 1 appears in $C(\mc{A})$.

\subsubsection{Unique Depth and Branch Numbers}

We now prove that the tuple $(n, m)$ specifying the depth and branch number for every element of $C(\mc{A})$ is unique within $C(\mc{A})$. 

If $D(\mc{A}) = 1$ then $\mc{A}$ is atomic so $C(\mc{A}) = \emptyset$. There are no elements in $C(\mc{A})$ so the condition is vacuously satisfied.

For the induction hypothesis we assume that if $D(\mc{B}) < n$ then the depth and branch number tuples in $C(\mc{B})$ are unique.

Suppose $D(\mc{A}) = n > 1$. $\mc{A}$ cannot be atomic.

If $\mc{A} \equiv (\lnot \mc{B})$ then $C(\mc{A}) = \{(\lnot, 1, 1, \mc{B})\} \cup C_{+1, +0}(\mc{B})$. Since $D(\mc{B}) < D(\mc{A}) = n$ all of the tuples in $C(\mc{B})$ are unique by the induction hypothesis. There is no depth number smaller than 1 in $C(\mc{B})$ by the previous theorem so there is no depth number in $C_{+1, +0}(\mc{B})$ smaller than 2. This means all of the tuples in $C(\mc{A})$ are unique since $C(\mc{A})$ only adds one tuple, with depth number 1, to $C_{+1, +0}(\mc{B})$.

If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $C(\mc{A}) = \{(\implies, 1, 1, \mc{B}), (\implies, 1, b+1, \mc{C})\} \cup C_{+1, +0}(\mc{B}) \cup C_{+1, +b}(\mc{C})$ with $b$ the maximum branch number in $C(\mc{B})$. Since $D(\mc{B}), D(\mc{C}) < D(\mc{A}) = n$ all of the tuples in $C(\mc{B})$ are unique and all of the tuples in $C(\mc{C})$ are unique by the induction hypothesis. However, there is not yet guarantee against duplication of tuples between $C(\mc{B})$ and $C(\mc{C})$. There is no depth number smaller than 1 in both $C(\mc{B})$ and $C(\mc{C})$ by the previous theorem. Therefore, there is no depth number smaller than 2 in either $C_{+1, +0}(\mc{B})$ or $C_{+1, +b}(\mc{C})$. In particular this means that the set of depth and branch numbers from $\{(\implies, 1, 1, \mc{B})\} \cup C_{+1, +0}$ are unique amongst each other and the set of depth and branch numbers from $\{(\implies, 1, b+1, \mc{C})\} \cup C_{+1, +b}(\mc{C})$ are also unique amongst each other. There is no branch number smaller than 1 appearing in $C(\mc{C})$ so the smallest branch number appearing in the latter set from the previous sentence is $b+1$. This means the branch numbers between the two sets are unique since the former has a largest branch number of $b$ and the latter has a smaller branch number of $b+1$. This concludes the proof that if $\mc{A} \equiv (\mc{B} \implies \mc{C})$ that all depth and branch tuples in $C(\mc{A})$ are unique.

If $\mc{A} \equiv ((\forall x)\mc{B})$ then $C(\mc{A}) = \{(\forall, 1, 1, \mc{B})\} \cup C_{+1, +0}(\mc{B})$. Since $D(\mc{B}) < D(\mc{A}) = n$ all of the tuples in $C(\mc{B})$ are unique by the induction hypothesis. There is no depth number smaller than 1 in $C(\mc{B})$ by the previous theorem so the smallest depth number in $C_{+1, +0}(\mc{B})$ is 2. This means all of the tuples in $C(\mc{A})$ are unique.

\subsubsection{Unique Depth and Branch Numbers for Full Tree}

Since no depth number smaller than 1 appears in $C(\mc{A})$, we have that the depth and branch numbers in 

$$
T(\mc{A}) = \{(\text{Wff}, 0, 1, \mc{A})\} \cup C(\mc{A})
$$

are unique.


\subsubsection{Maximum Depth Number is Equal to Wff Depth}

If $\mc{A}$ is atomic then $D(\mc{A}) = 1$ and $C(\mc{A}) = \{(A, 1, 1, \mc{A})\}$ so we see the maximum depth number is 1 as needed.

For the induction hypothesis we assume that if $D(\mc{B}) < n$ then the maximum depth number in $C(\mc{B})$ is equal to $D(\mc{B})$.

If $\mc{A} \equiv (\lnot \mc{B})$ then $D(\mc{A}) = D(\mc{B}) + 1$ and $C(\mc{A}) = \{(\lnot, 1, 1, \mc{B})\} \cup C_{+1, +0}(\mc{B})$. The maximum depth number in $C(\mc{B})$ is $D(\mc{B})$ by induction hypothesis so the maximum depth number in $C(\mc{A})$ is $D(\mc{B}) + 1 = D(\mc{A})$ as needed.

If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $D(\mc{A}) = \text{max}(\mc{B}, \mc{C}) + 1$. The maximum depth number in $C(\mc{B})$ is $D(\mc{B})$ and the maximum depth number in $C(\mc{C})$ is $D(\mc{C})$ by the induction hypothesis. The maximum depth number in $C(\mc{A})$ is then $\text{max}(D(\mc{B}) + 1, D(\mc{C})+1) = \text{max}(D(\mc{B}), D(\mc{C})) + 1$ as needed.

If $\mc{A} \equiv ((\forall x)\mc{B})$ then $D(\mc{A}) = D(\mc{B}) + 1$. The maximum depth number in $C(\mc{B})$ is $D(\mc{B})$ by the induction hypothesis so the maximum depth number in $C(\mc{A})$ is $D(\mc{B}) + 1$ as needed.

It is clear than the maximum depth number in $T(\mc{A})$ is also equal to $D(\mc{A})$.

\newpage

\section{Proof for Equivalent Wff Replacement}

Consider the closed Wff $\mc{A}$.
Suppose the Wff $\mc{P}$ is a part of $\mc{A}$. By this we mean that $\mc{P}$ appears as the fourth element of one of the tuples listed in $T(\mc{A})$. In this case we say $\mc{P} \in T(\mc{A})$ (with a slight abuse of notation).
Suppose we have another Wff $\mc{Q}$ with $FV(\mc{Q}) \subset FV(\mc{P})$ and we have

$$
\nu \vdash ((\forall x_1)(\ldots((\forall x_n) (\mc{P} \iff \mc{Q})))).
$$

Let $\mc{A}' \equiv B\mc{Q}C$. I will now prove that $\nu \vdash (\mc{A} \iff \mc{A}')$. 
For this I will need to define the depth of a Wff, $D(\mc{A})$.



We will prove the desired theorem by metalogical strong induction on the depth of the Wff $\mc{A}$.
That is, we will prove that the theorem holds in the case when $D(\mc{A}) = 1$.
Then, if $D(\mc{A}) = n$, we will assume the theorem holds for all integers $<n$ and the prove again that the theorem holds by $n$. 
In this way we will be assured the theorem holds for all Wff depths.

\subsection*{Base Case}

If $D(\mc{A}) = 1$ then $\mc{A}$ is an atomic Wff.
Since $\mc{A}$ contains $\mc{P}$ as a substring $\mc{P}$ must also be atomic and we must have $\mc{A} \equiv \mc{P}$.
We then also have that $\mc{A}' \equiv \mc{Q}$.
Note that we must have $FV(\mc{P}) = FV(\mc{Q}) = \emptyset$ since $\mc{A}$ is an atomic closed Wff.
It is then the case that $(\mc{A} \iff \mc{A}') \equiv (\mc{P} \iff \mc{Q})$, but we already have that $\nu \vdash (\mc{P} \iff \mc{Q})$.

\subsection*{Induction Step}

Suppose $D(\mc{A}) = n$.
We have that 
$$
\nu \vdash ((\forall x_1)(\ldots((\forall x_n) (\mc{P} \iff \mc{Q})))).
$$
We assume then that, for any Wff $\mc{B}$ with $D(\mc{B}) < n$ which contains $\mc{P}$ as a substring that we also can derive
$$
\nu \vdash (\mc{B} \iff \mc{B}')
$$
where $\mc{B}'$ is the result of replacing $\mc{P}$ by $\mc{Q}$ as above.




\section{Proof that Extension by Predicate Definition is a Conservative Extension}

Suppose $\mc{T}_2$ is an extension by predicate definition of $\mc{T}_1$ as in the previous section.
We will now prove that $\mc{T}_2$ is a conservative extension of $\mc{T}_1$.
For this we must prove that

$$
\Phi_{\mc{T}_2} \cap \mc{L}_1 = \Phi_{\mc{T}_1}
$$

Suppose $\mc{A} \in \Phi_{\mc{T}_1}$.
Then $\Gamma_1 \vdash_{\mc{T}_1} \mc{A}$.
We can apply the weakening inference rule ($W$) in $\mc{T}_2$ to then prove that $\Gamma_2 = \Gamma_1 \cup \{\nu\} \vdash_{\mc{T}_2} \mc{A}$.
This means $\mc{A} \in \Phi_{\mc{T}_2}$.
Since $\mc{A} \in \Phi_{\mc{T}_2}$ and $\mc{A} \in \mc{L}_1$ this means that

$$
\Phi_{\mc{T}_1} \subset \Phi_{\mc{T}_2} \cap \mc{L}_1
$$

The challenge for this proof is to prove that 

$$
\Phi_{\mc{T}_2} \cap \mc{L}_1 \subset \Phi_{\mc{T}_1}
$$

Suppose $\mc{A} \in \Phi_{\mc{T}_2} \cap \mc{L}_1$.
This means that $\mc{A} \in \mc{L}_1$ and $\mc{A} \in \Phi_{\mc{T}_2}$.
Because $\mc{A} \in \Phi_{\mc{T}_2}$ so that there is a proof tree in $\mc{T}_2$ concluding $\Gamma_2 \vdash_{\mc{T}_2} \mc{A}$.
Our goal is to prove that $\Gamma_1 = \Gamma_2 \backslash \{\nu\} \vdash_{\mc{T}_1} \mc{A}$.

We will prove this in a few steps.
First we must define the concept of the translation of a Wff from $\mc{L}_2$ into $\mc{L}_1$.
Suppose $\mc{A} \in \mc{L}_2$. 
We then define $\mc{A}^\# \in \mc{L}_1$ by induction.

\hrulefill
\subsubsection*{Wff Translation}
\begin{itemize}
\item{\textbf{Atomic Formulas:} If $\mc{A}$ is an atomic formula then if $\mc{A} \equiv Pt_1\ldots t_n$ then $\mc{A}^\# \equiv \mc{Q}$}
\end{itemize}
\hrulefill


We will accomplish this by metalogically deconstruction of the $\mc{T}_2$ proof of $\Gamma_2 \vdash_{\mc{T}_2} \mc{A}$.
This proof has length $N$.
We will prove by metalogical induction on proof length that if $\Gamma_2 \vdash_{\mc{T}_2} \mc{A}$ via a proof of length $N$, then it is possible to find a proof of $\Gamma_1 \vdash_{\mc{T}_1} \mc{A}$ (of unspecified length).

Suppose we have a proof that concludes $\Gamma \vdash_{\mc{T}} \mc{A}$. 
We define the length of the proof to be the line number on which $\Gamma \vdash_{\mc{T}} \mc{A}$ appears minus the number of premises.
This means that if $\mc{A}$ is an axiom of $\mc{T}$ then it is possible to prove $\Gamma \vdash_{\mc{T}} \mc{A}$ in zero steps.

For induction we first work with a proof of length zero as our base case.
If $\Gamma \vdash_{\mc{T}_2} \mc{A}$ in zero steps then $\mc{A}$ is an axiom of $\mc{T}_2$.
This means that $\mc{A} \in \Gamma \cup \{\nu\}$

\end{document}


