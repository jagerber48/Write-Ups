\documentclass[12pt]{article}

\usepackage{amssymb, amsmath, amsfonts}
\usepackage{bussproofs}
\usepackage{amsthm}
\usepackage{braket}

\makeatletter
\newtheoremstyle{break}% name
    {12pt}%         Space above, empty = `usual value'
    {12pt}%         Space below
    {\addtolength{\@totalleftmargin}{1.5em}
     \addtolength{\linewidth}{-3em}
     \parshape 1 1.5em \linewidth
     \itshape}% Body font
    {}%         Indent amount (empty = no indent, \parindent = para indent)
    {\bfseries}% Thm head font
    {}%        Punctuation after thm head
    {\newline}% Space after thm head: \newline = linebreak
    {}%         Thm head spec
\theoremstyle{break}
\newtheorem{definition}{Definition}[section]
\theoremstyle{break}
\newtheorem{theorem}{Theorem}[section]
\theoremstyle{break}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{break}
\newtheorem{lemma}[theorem]{Lemma}
\theoremstyle{break}
\newtheorem{informal definition}[definition]{Informal Definition}


\newcommand{\bv}[1]{\boldsymbol{#1}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\bc}[1]{\bv{\mc{#1}}}
\newcommand{\qq}[1]{``#1''}
\newcommand{\NUBF}[0]{\mathbb{N}_{(U, B, \mathcal{F})}}

\begin{document}
\title{Logic Notes}
\author{Justin Gerber}
\date{\today}
\maketitle

\section*{Old Introduction}

In this document I will try to get straight some of my thoughts on formal logic. At the moment the goal is to build up to a metalogic proof, in an extended Lemmon system of logic, that extensions by definition are conservative extensions to a formal theory. A lot of the difficulty comes from 1) the fact that the Lemmon system does not seem to be heavily used in the logic community so there are not really references; when syntactic (as opposed to semantic) proofs are given they often use the Hilbert system and 2) many references use semantic arguments to complete the proof (such as the completeness theorem and model theory generally) and I would like to see a purely syntactical proof.

I am following two references. ``Modern Logic: A Text in Elementary Symbolic Logic'' by Graehme Forbes and ``Lectures in Logic and Set Theory: Volume I'' by George Tourlakis. The former is my reference on Lemon logic and the rules therein while The Tourlakis book is more rigorous and does a cleaner job introducing ideas with the metalanguage or metatheory including the concept of inducting on formulas using the metalanguage. However, Tourlakis uses the Hilbert system and semantic proofs in cases which is why I need to adapt both approaches.

\section*{Preface}
The previous introduction was written around February 2019, the last time I took up investigations into formal logic. I'm revisiting formal logic again (Aug 2021).
Typically I find myself looking into formal logic after the following sequence of events.
1) I am interested in some physics topic, typically somewhat mathematically oriented in nature, 2) I look into the math supporting this topic, 3) I get curious about the deep mathematical definitions or theorems involved in the math topic 4) I get into very deep math subjects such as topology or the very definitions of functions 5) I finally find myself back facing formal logic.

Back in 2019 I believe I was interested in the relationship between classical and quantum random variables.
This led me to look into mathematical formulations of probability theory which took me eventually to measure theory.
In learning measure theory I was attempting to learn about Borel sets and some basic topology again.
I believe I then got curious about the definition of infinite unions and this brought me down to formal logic.
I was then curious at the time about how to build from ZFC up to larger mathematical theories.

Such an undertaking requires the extension by definition of a logical theory.
Extension by definition of a logical theory involves introducing a new symbol to the language and adding a ``defining'' axiom for that logic into the theory.
For appropriate defining axioms the new symbol should not change the theory in the sense that the extension of the theory is conservative.
An extension of a theory is conservative if a formula of the new language, which is a valid formula of the old language, can be deduced in the new theory exactly when it can be deduced in the old theory.
The challenge I faced last time was trying to prove the claim that appropriate extensions by definition are conservative.

I learned a lot about formal logic and had some luck, but the approach I was taking ended up being too tedious.
Two references I utilized addressed the problem: \textit{Lectures in Logic and Set Theory: Volume I} by George Tourlakis and \textit{Introduction to Mathematical Logic} by Elliott Mendelson.
However, both of these books were based on Hilbert style formal logic.

At the time I found this frustrating.
My background in formal logic (from a course I took my freshman year in college in 2009) was from an introductory textbook called \textit{Modern Logic: A Text in Elementary Symbolic Logic} by Graehme Forbes.
This book used a natural deduction approach for proofs and visualized proofs using a Lemmon logic tabular proof style.
I found this latter style to be very natural, and, combined with self study on the Zermelo-Fraenkel axioms of set theory, I, at least subconsciously, understood that it would be possible to put all of mathematics into this mathematical formalism.

I found the axiomatic approaches in the Hilbert style logic to be unnatural and annoying.
A theory didn't seem elegant when all tautologies are assumed as axioms.
It was much more natural to me to have a theory which has no ``logical'' axioms, but from which tautologies could be derived via the rules of inference.
This is of course the same story as the origin of Gentzen's natural deduction.

I have recently revisited this problem.
The physics problem I was trying to understand is how to derive the 3D multipole vector fields describing electromagnetic radiation from multipole charge and current distributions.
This led me to try to understand some theorems about differential equations.
In learning about existence theorems for solutions to, for example, the Laplace equation, i was directed towards, simultaneously, it seems, complex analysis and the fundamental theorem of algebra.
In proving the fundamental theorem of algebra I found myself visiting some familiar topics in multivariable calculus.
At this time I hit upon a confusion/frustration that has cropped its head up for me time and time again.
This frustration regards the notation for partial derivatives.

We often have, for example $\partial f/\partial x$.
It is implicitly understood that this means differentiation with respect to the `first parameter' of the function $f$.
In cases of nested functions the notation gets complicated, confusing, and sometimes ambiguous so I seek a rigorous definition.
A long story short, this led me to try to understand a set theoretic definition of a multivariable function.
Finally, in the context of ZFC set theory, function notation arises from symbols which have been added to the theory via extensions by definition.
This brought me back to formal logic.

I walked over many of the tracks I had explored before.
One thing I understood then and I understand now is that much of my difficulty in developing the proof I was interested in was that I did not have a formal enough definition of the rules of inference.
I was relying on the rules of inference as they were laid out in the Graehme textbook.
Unfortunately the definitions there were too heavily tied to the Lemmon tabular proof structure making it very difficult to reason generally about proof structure.

Just recently I came to a more thorough understanding of the sequent calculus.
I believe that the sequent calculus provides rigorous enough definitions of inference rules for me to complete the proof I am interested in now.
Additionally, I've hit upon a reference which is close to the flavor of formal logic I am interested in.
This is \textit{Structural Proof Theory} by Sara Negri and Jan Von Plato.
In fact, I learned that exactly what I am trying to do is a topic in the field of structural proof theory.

One of the major difficulties I found when trying to proof extensions by definition are conservative in the past is the fact that natural deduction logic typically has many inference rules.
When developing an inductive metalogic proof it is necessary to induct over all of these different rules making the proof extremely tedious.
To this end I became very curious if any of the inference rules are redundant.
That is, it is known that some logical connectives can be written in terms of others.
For example: $\mc{A}\land\mc{B} \equiv \lnot(\mc{A}\implies \lnot\mc{B})$.
If the rules of inference involving $\land$ could be derived from the rules of inference for $\implies$ and $\lnot$ then it would be possible to dispense with the $\land$ symbol as a native part of the language and simply maintain it as an abbreviation for the above expression, also taking the corresponding inference rules as derived rules.
The would reduce the number of cases over which we need to induct for the metalogical proofs that follow.
I believe this program is possible and that will be part of this document.

Another difficulty I faced was that the rules of inference involving quantifiers sometimes involve various restrictions on the formulas which are being replaced.
Without a nice way to notate these restrictions I've found it difficult to work with these rules in metalogical proofs.
I hope that in this revisit I'll be able to overcome this difficulty.


\newpage
\section{Introduction}

\subsection{Formal Logic and Language}
All of traditional mathematics can be expressed in terms of mathematical set theory.
Set theory itself is expressed in terms of a formal logic called first-order logic \textbf{FOL} which is expressed in the language of first-order logic \textbf{LFOL} .
The \textbf{LFOL} is a written formal language which means that it is composed of a clearly and well-defined set of symbols which constitutes the alphabet of that language and clear rules of syntax which identify `appropriate' ways in which the symbols can be combined to form formulas.\footnote{Sometimes formulas are called well-formed formulas (Wffs), sentences, or words.}
In addition to \textbf{LFOL}, \textbf{FOL} includes a deductive calculus based on inference rules which allows us to derive new formulas from old formulas in a way which will be made clear below.
\textbf{FOL} is fully captured by the language in which it is expressed, \textbf{LFOL}, and the inference rules which allow for deduction within the logic.

A formal theory within a formal logic begins with a set of axioms which are a subset of the formulas of the formal of the language.
It is then possible, using the inference rules, to derive new formulas from these axioms using the inference rules.
The main question we ask about a formal theory is which formulas can be proven from the given axioms?

\subsection{Syntax and Semantics}
Two central topics in formal logic are syntax and semantics.
Syntax pertains to the formal rules mentioned above such as which symbols are in the alphabet, how can they be combined to form formulas, and what a
Semantics is the task of assigning meaning to formulas.
In its simplest form semantics is the assigning of a truth value to formulas.

Let's consider an example.
Suppose we would like to formalize the statement: If $x$ is an integer and $x$ is odd then $x \div 2$ is an integer.

$$
\forall x((\in x \mathbb{N} \land Ox)\implies \in \div x 2 \mathbb{N})
$$

The smallest element in our formal language is the \textit{term}.
A term is one of three things (1) a concrete object such as the number \qq{2}, (2) a variable into which concrete objects can be `plugged in' such as \qq{x}, or (3) an $n$-ary function of other terms such as \qq{$\div x2$}.
In fact, below we will express a concrete object as a 0-ary function.
The terms in the above expression are:

\begin{align*}
& x \\
& \mathbb{N} \\
& x \\
& \div x 2 \\
& x \\
& 2 \\
& \mathbb{N}
\end{align*}

Here I have listed every term as it appears from left to right.
We can see that some terms appear multiple times in the formula.
The only variable which appears is \qq{$x$}.
We see \qq{2} and \qq{$\mathbb{N}$} appearing as 0-ary functions.
We see \qq{$\div$} appearing as a 2-ary function with arguments \qq{$x$} and \qq{2} which are a variable and 0-ary function respectively.
Note that in our formal language we always use prefix notation for functions and predicates so we symbolize \qq{$\div x2$} rather than \qq{$x \div 2$} as we would see in typical infix notation.

The next larger element in our formal language is the \textit{atomic formula}.
An atomic formula is the combination of an $n$-ary predicate symbol and $n$ terms.
We have 2 predicate symbols in the above formula: \qq{$\in$} is a 2-ary predicate and \qq{$O$} is a 1-ary predicate symbol.
The atomic formulas are then

\begin{align*}
\mc{A} \equiv& \in x \mathbb{N}\\
\mc{B} \equiv& Ox\\
\mc{C} \equiv& \in \div x2 \mathbb{N}
\end{align*}

Here $\mc{A}$, $\mc{B}$, and $\mc{C}$ are metalanguage symbols which stand for the corresponding formulas on the right hand sides of the $\equiv$ symbol.

Finally, the next larger object in our language is the formula.
Atomic formulas themselves are already formulas, but we can form larger more complex formulas by stringing together smaller formulas with connectives such as \qq{$\land$} and \qq{$\implies$} or quantifying with $(\forall x)$.
We see sub-formulas above:

\begin{align*}
& \forall x((\in x \mathbb{N} \land Ox)\implies \in \div x 2 \mathbb{N}) \\
& ((\in x \mathbb{N} \land Ox)\implies \in \div x 2 \mathbb{N}) \\
& (\in x \mathbb{N} \land Ox)\\
& \in x \mathbb{N}\\
& Ox \\
&  \in \div x 2 \mathbb{N}
\end{align*}

The above discussion has been a purely syntactic analysis of the given formula.
That is, we broke down the formula into smaller parts of the formal language.
We will see below that this formula is, in fact, a formula according to the formal rules of syntax for \textbf{LFOL}.

Semantics comes in when we give an \textit{interpretation} to the various expressions above.
For example, when we \textit{interpret} the symbol \qq{2} as the number 2, the expression \qq{$\div x 2$} as the division of the variable $x$ by the number 2, or predicate \qq{$Ox$} to mean that the variable $x$ is odd, we are imposing semantics onto the expression.
We also are making a semantic interpretation any time we impose or deduce a truth value for a given formula.
For example, on all of the usual interpretations, we would assign a value of false to the formula above because the odd integers are,ga in fact, not divisible by 2.

In this document I would like to follow a game formalist approach to formal logic.
This is an approach which holds that it is possible to describe set theory, and by extension, all of mathematics using only syntactic methods.
Or, at the very least, it asks the question how far one can get using such an approach.
To that end, I would like it to be clear that all definitions and arguments should be self-contained within purely syntactic analyses.
At a few times I may make reference to semantic ideas, but this is only to motivate definitions and manipulations, and such references should be disposable with respect to the main focus of this work.
One could, instead, proceed by dropping semantics entirely from the narrative.
In such an approach some definitions may appear unmotivated, at least at first, but that is not, technically, a problem for the game formalist.

\subsection{Outline}

The goal of this work is to prove that extensions by definition to a formal theory are conservative extensions.

I will begin with a thorough outlining of \textbf{LFOL}
I begin by defining the alphabet and the various symbols of the language.
This is followed detailed definitions and theorems pertaining to lists which will allow us to effectively manipulate the strings over the alphabet which will constitute the terms and formulas of \textbf{LFOL}.
Next the syntax of \textbf{LFOL} is defined which allows us to identify which strings constitute formulas.
The next technical sections involve the definition of free and bound variables and a study of the replacement of free and bound variables within formulas.
Such manipulations will be important for abstract proof theory involving quantification.

% TODO Work on this section after writing the proof theory stuff.
After \textbf{LFOL} is laid out we move into proof theory.
We being by describing sequents and follow with the concept of a formal proof and our corresponding rules of inference.
We then derive a number of derived inference rules, including inference rules for the abbreviated symbols which were not included natively in the language.

Next I will define a logical theory based on a set of axioms.
At this point I will then define a conservative extension to a theory, and an extension by definition to a theory.
There are two types of extensions by definition.
We either define new predicate symbols, or new function symbols.
In the remainder of this document I will prove that these two forms of extensions by definition to a logical theory are in fact conservative extensions.
These proofs will utilize metalogical induction.

\newpage
\section{Informal Mathematics}
The goal of this work is to carefully define \textbf{LFOL} and \textbf{FOL} so that we can eventually formally describe ZFC set theory within \textbf{FOL} to serve as a foundation for the formal description of much of modern mathematics.
However, we will not be able to perform this task starting from nothing.
Indeed, to create this foundation for formal mathematics we will find ourselves relying on \textit{informal} conceptions of various simple mathematical constructs.
In this chapter I will outline these prerequisite informal mathematical concepts necessary for this project.
In this chapter we will have "definitions" for informal concepts, but these definitions should not be taken as seriously as those that appear in later chapters of this work or in formal mathematical texts.
These definitions should be thought of a bit more like definitions of English words which are understood by speakers of the language and less like formal definitions which could, for example, be encoded on a computer.

Note that these informal definitions may closely mirror the definitions of their formal counterparts.
However, we would have a logical circularity problem if we required the formal concept of a set to define the formal concept of a set.
Instead, we rely on informal definitions initially and formalize them later, when we are able.

\begin{informal definition}[Logic]
To build up our formal system of logic we will indeed require an informal system of logic.
It is necessary that we informally or intuitively understand concepts like
\begin{itemize}
\item{If $A$ is true and $B$ is true then `$A$ and $B$` is true}
\item{If $A$ implies $C$ and $B$ implies $C$ then if `$A$ or $B$' is true then $C$ is implied.}
\end{itemize}
\end{informal definition}

\begin{informal definition}[Object]
An object can be a wide range of things.
Examples of an object are: a table, an apple or a word.
An object should be clearly identifiable and nameable.
\end{informal definition}

The main types of objects we will encounter in this work will be symbols.
\begin{informal definition}[Symbol]
A symbol is a type of object.
Written symbols are specific two dimensional patterns depicted on paper or screens.
We will typically use symbols from the English alphabet and numeral system as well some punctuation symbols and brackets.
\end{informal definition}

\begin{informal definition}[Object Equality]
Suppose the symbols $x$, $y$, and $z$ represent objects.
We define the concept of object equality.
\begin{itemize}
\item{If $x$ and $y$ are the same item then we may write $x\equiv y$. Otherwise we write $x\not \equiv y$. This relies on the identifiability property of informal objects.}
\item{If $x\equiv y$ then $y\equiv x$.}
\item{If $x\equiv y$ and $x\equiv z$ then $y\equiv z$.}
\end{itemize}
From the first property we can see that we always have $x\equiv x$.
\end{informal definition}
Note that in the formal setting it is possible to define equality as a binary relation satisfying certain properties but we will find having a direct informal definition of equality to be more useful and intuitive.

The next informal concept we need is that of a set.
\begin{informal definition}[Set]
A set is a collection of distinct objects.
If, for example, $a$, $b$, and $c$ are objects then the set containing these objects may be denoted as $\{a, b, c\}$.
Note that sets only enumerate \textit{distinct} objects so if $x\equiv y$ then $\{x, y\}$ is the same set as $\{x\}$ and $\{y\}$.

If a certain object denoted by $x$ is contained in a set $S$ then we denote this with the set membership symbol as $x\in S$.
\end{informal definition}
We may require informal sets with an infinite number of objects.
If we have one or more sets there are ways to extract new sets composed of the elements of the initial sets.

\begin{informal definition}[Subset]
If $S$ and $T$ are sets and all elements that are contained in $S$ are also contained in $T$ then we say $S$ is a subset of $T$ and we denote this by $S\subset T$.
This is equivalent to the condition that $x\in S$ implies $x\in T$.
\end{informal definition}

\begin{informal definition}[Set Equality]
If $S$ and $T$ are sets and $S\subset T$ and $T\subset S$ then we say $S=T$.
It should be informally understood that
\begin{itemize}
\item{$S=S$}
\item{If $S=T$ then $T=S$}
\item{If $U$ is a third set then if $S=T$ and $S=U$ then $T=U$.}
\end{itemize}
\end{informal definition}

\begin{informal definition}[Subset Specification]
Suppose $S$ is a set.
Then we may extract a subset of $S$ by picking out all components of $x\in S$ which satisfy a certain property $\phi(x)$.
This specification set is denoted using so-called `set-builder notation' as
$$
\{x\in S: \phi(x)\}
$$
\end{informal definition}

Note that sets constructed this way are always subsets of a larger set $S$.
If we were to drop this set and allow sets of the form $\{x:\phi(x)\}$, i.e. to allow all objects $x$ satisfying some property $\phi(x)$ to form a set, then we open ourselves to the Russell Paradox.

The Russell Paradox occurs if we allows sets of the $\{x:\phi(x)\}$ to be a set.
In that case we would be permitted to let $\phi(x)$ mean `$x$ is a set which does not contain itself'.
We might denote the Russell set by
$$
R = \{x: x \not \in x\}
$$
But then we may ask the question is $R\in R$?
If $R\in R$ then we would expect $R\not \in R$ because $R$ contains itself.
But if $R\not \in R$ we would expect $R\in R$ because it does not contain itself.

Another type of set pathology we may encounter is that of an `infinite descending sequence of sets'.
Russell's paradox arose from the ability to define overly arbitrary sets.
Our subset specification resolution to Russell's paradox however, does not prevent us to have a set $S$ which is  contained in itself: $S\in S$.
Allowing this and similar pathologies may or may not present problems.
This type of pathology will be prevented in formal set theory with the axiom of regularity.
However, in our informal treatment, since we will always be working with simple enough sets (finite sets of symbols and collections thereof), we won't go through the trouble of providing an informal version of the axiom of regularity.

Note also that in formal set theory all objects are sets so we are much more prone and must be more wary of `objects containing themselves' pathologies.
But in our informal setting we are considering sets of objects, and those objects need not be sets.
This informal approach alleviates some of the burden.
And finally, since we are being informal, we need not be so careful as to account for every edge and corner case because we are not trying to develop a perfect system, only one which can be understood by our readers.

\begin{informal definition}[Set Union]
If $S$ and $T$ are sets then we call the set containing all elements of $S$ and all elements of $T$ the union of $S$ and $T$ and denote it by $S\cup T$.
The union $S\cup T$ can equivalently be specified by the following sentence.
Object $x\in S\cup T$ iff $x \in S$ or $x\in T$.
\end{informal definition}
In formal set theory we require an axiom which explicitly allows the union of two sets to also be a set.
But in our informal theory we take it for granted that the union of two sets exists and merely have a definition to give a name and introduce syntax for that set.
Likewise, in formal set theory we require a pairing axiom to allow the set $\{S, T\}$ to be a formal set, but again, we take that for granted in the informal setting.

\begin{definition}[Set Intersection]
If $S$ and $T$ are sets then we call the set containing all elements which are included in both sets $S$ and $T$ to be the intersection of $S$ and $T$ and denote it by $S\cap T$.
We can construct the set intersection using subset specification:
$$
S\in T = \{x\in S: x\in T\}
$$
\end{definition}

\begin{definition}[Set Subtraction]
If $S$ and $T$ are sets then we may create a new set by collect all elements of $S$ which are not in $T$.
This new set is called the set subtraction of $S$ and $T$ and is denoted by $S\setminus T$.
We can construct the set subtraction using subset specification:
$$
S\setminus T = \{x\in S: x \not \in T\}
$$
\end{definition}

\begin{theorem}[Set Subtraction is Distributive Over Set Union]
Suppose $S_1,\ldots, S_n, T$ are sets.
Then
$$
(S_1\setminus T)\cup \ldots \cup (S_n\setminus T) = (S_1\cup\ldots \cup S_n)\setminus T
$$
We only informally prove this by noting that if $x$ is contained in the set on the left $x\in (S_i\setminus T)$ for at least of of $1\le i \le n$ which means that $x\not \in T$ but that $x\in S_i$ for one of $1\le i \le n$.
But the set on the right is characterized by $x\in S_i$ for at least one of $1 \le i \le n$ but $x\not \in T$.
In the formal set theory this would be proven using induction.
\end{theorem}

\begin{informal definition}[Ordered Pair]
If $x$ and $y$ are objects then we can create a new object called the ordered pair of $x$ and $y$.
This ordered pair is denoted by $\braket{x, y}$.
In general $\braket{x, y} \not = \braket{y, x}$ unless $x = y$.
\end{informal definition}
In formal set theory we can formalize tuples as $\braket{x, y} = \{\{x\}, \{x, y\}\}$, but we have no need of such heavy formalism here and allow the informal ordered pair to stand as its own informal concept.

\begin{informal definition}[Cartesian Product]
If $S$ and $T$ are two sets then we may construct a set which consists of all ordered pairs of elements from $S$ and $T$ respectively.
This set is called the Cartesian product of $S$ and $T$ and is denoted by $S\times T$.
If $s\in S$ and $t\in T$ then $\braket{s, t} \in S\times T$.
Similarly, if $x\in S\times T$ then there exists $s\in S$ and $t\in T$ with $x = \braket{s, t}$.
\end{informal definition}

\begin{informal definition}[Relation]
If $S$ and $T$ are two sets then a relation $R$ between $S$ and $T$ is any subset of $S\times T$. That is $R\subset S\times T$.
If $s\in S$, $t\in T$, and $\braket{s, t}\in R$ then we say $s$ and $t$ are related by $R$ and may write $Rst$ using prefix notation, $R(s, t)$ using prefix notation with parentheses for clarity, or sometimes, $sRt$ using infix notation for binary relations.
\end{informal definition}
A common relation is the less than relation, $<$.
We have, for example, that $\braket{3, 4} \in <$ because $3<4$, but $\braket{4,3}\not \in <$ because $4\not < 3$.

\begin{informal definition}[Function]
A function is a relation which satisfies two properties.
Suppose $S$ and $T$ are two sets and $R$ is a relation between them, i.e. $R\subset S\times T$.
\begin{itemize}
\item{\textbf{Total}: For every $s\in S$ there exists a $t\in T$ such that $\braket{s, t} \in S$. That is every element in $s$ is related to at least one element in $T$.}
\item{\textbf{Functional}: If $s\in S$ and $t, t'\in T$ then if $R(s, t)$ and $R(s, t')$ then $t\equiv t'$. That is, if $s\in S$ is related to $t\in T$ it is not related to any other element in $T$.}
\end{itemize}
If a relation is total and functional then we say that relation is a function.
Equivalently, a function is a relation which is total and functional.
If $f$ is a function with domain $S$ and range $Y$ we may indicate this by $f:S\to T$.
For each $s\in S$, we denote the unique element $t\in T$ which satisfies $\braket{s, t}\in f$ by $f(s)$ so that $f(s) = t$.
\end{informal definition}

\begin{informal definition}[Function Restriction]
Suppose $f$ is a function $f:S \to T$.
Suppose $X\subset S$.
We define
$$
f|_X = \{\braket{x, t} \in S\times T: x\in X\}
$$

$f|_X$ is the function which is defined only on $X\subset S$ but which, on this set $X$, takes on the same values as $f$.
\end{informal definition}

\begin{informal definition}[The Natural Numbers]
We require an informal understanding of the natural numbers.
These are the counting number $0, 1, 2, \ldots$.
We denote the (informal) set of all natural numbers as $\mathbb{N}$.
We will also require an understanding of simple relations like $<$, $\ge$, $=$ on $\mathbb{N}$ as well as some simple addition.
\end{informal definition}

\begin{informal definition}[Induction Over the Natural Numbers]
Suppose $\phi$ is some property or expression which depends on a natural number $n\in \mathbb{N}$.
If we can prove that
\begin{itemize}
\item{Base Case: $\phi$ is true for $n=0$}
\item{Inductive Case: $\phi$ being true for $n$ implies $\phi$ being true for $n+1$}
\end{itemize}
Then we may conclude that $\phi$ is true for all $n\in \mathbb{N}$.
Informally, this makes sense because we can use the base case to tells us that $\phi$ holds for $n=0$, then we can use the induction case to show that $\phi$ holds for $n=1 = 0+1$ (since it holds for $n=0$) and so on.
Once the natural numbers have been formalized it will be possible to formalize the inductive reasoning process.
\end{informal definition}

\begin{informal definition}[Structurally Inductive]
Suppose $U$ is a set, $B\subset U$ and $\mathcal{F}$ is a family of functions where each $f\in \mathcal{F}$ is a function $f:U^n \to U$ for some $n\in \mathbb{N}$.

We say a set $C\subset U$ is $(U, B, \mathcal{F})$-inductive if it satisfies two properties:
\begin{itemize}
\item{$B \subset C$}
\item{For each $f\in \mathcal{F}$ with $f:U^n \to U$ then for each $\bv{c}\in C^n \subset U^n$ we have $f(\bv{c}) \in C$}
\end{itemize}

This idea generalizes induction the natural numbers.
Indeed if we let $B= \{0\}$ and $\mathcal{F} = \{+_1\}$ where $+_1: \mathbb{N} \to \mathbb{N}$ is defined by $+_1(n) = n+1$ then we can, informally, see that $\mathbb{N}$ is $(\mathbb{N}, \{0\}, \{+_1\})$-inductive.

\end{informal definition}

\begin{informal definition}[Structural Inductive Closure]
We define the $(U, B, \mathcal{F})$-inductive closure $\mathbb{N}_{(U, B, \mathcal{F})}$ by
$$
\NUBF = \bigcap \{C \in \mathcal{P}(U): C \text{ is } (U, B, \mathcal{F})\text{-inductive} \}
$$

Intuitively the structural inductive closure $\NUBF$ includes $B$, those elements which can be reached by successive applications of functions $f\in \mathcal{F}$ and no other elements.
Any element of $c \in \NUBF$ can be deconstructed using a `parse tree' in which each leaf is an element of $B$ and each non-leaf node is an element $f \in \mathcal{F}$ such that each node can be identified with $f$ acting on it's children.
\end{informal definition}

\begin{informal definition}[Structural Induction]
Consider $C = \NUBF$.
Suppose $\phi$ is a formula which depends on an element $c\in C$.
If we can prove that

\begin{itemize}
\item{Base Case: $\phi$ is true for $c \in B$}
\item{Inductive Case: For each $f\in \mathcal{F}$ with $f:U^n \to U$ we have that $\phi$ being true for $c_1, \ldots, c_n \in C$ implies $\phi$ is true for $f(c_1, \ldots, c_n)$}
\end{itemize}

Then we may conclude that $\phi$ is true for all $c\in C$.

\end{informal definition}

\begin{informal definition}[Structural Recursion]
Consider $C = \NUBF$ and a set $S$.

Suppose

\begin{itemize}
\item{There is a function $h_B: B \to S$}
\item{For each $f\in \mathcal{F}$ with $f:U^n \to U$ there is a corresponding function $\tilde{f}$ with $\tilde{f}:S^n \to S$}
\item{For each $f\in \mathcal{F}$ with $f:U^n \to U$ we have that $f|_{C^n}$ is injective}
\item{For each $f\in \mathcal{F}$ with $f:U^n \to U$ we have that $\text{Img}(f|_{C^n}) \cap B = \emptyset$.}
\item{For each pair $f, f' \in \mathcal{F}$ with $f:U^n \to U$ and $f':U^{n'}\to U$ we have that $\text{Img}(f|_{C^n}) \cap \text{Img}(f'|_{C^{n'}}) = \emptyset$}
\end{itemize}

Then it follows that there exists a unique function $h:C \to S$ which satisfies

\begin{itemize}
\item{for $c\in B$ we have $h(c) = h_B(c)$}
\item{If $f\in \mathcal{F}$ with $f:U^n \to U$, $c_1, \ldots, c_n \in C$, and $f(c_i) = s_i$ for $0\le i < n$ then $h(f(c_1, \ldots, c_n)) = \tilde{f}(h(c_1), \ldots, h(c_n))$.}
\end{itemize}

The latter two constraints on $f|_{C^n}$ ensure that for any $c\in C$ we can uniquely "break it down" into constituent components combined together using functions from $f$.
Then, once $c$ has been broken down into elements of $B$, we may calculate corresponding elements of $s$ using $h_B$ and then combine these elements using the corresponding functions $\tilde{f}$.
\end{informal definition}

\newpage
\section{Syntax of the Language of First Order Logic (LFOL)}

At the basis of a formal language is a set of symbols which can be concatenated together to form strings.
If a string follows certain allowed construction rules, specified by the syntax of the language, then the string is said to be a well-formed formula, or formula.

There are multiple classes of symbol objects, each of which plays a different role in the syntax of the language.
We first distinguish between logical and non-logical symbols.
Logical symbols are symbols which are related purely to the formal presentation of the language.
Logical symbols include things like logical connectives and parentheses.
The second main class of symbols is the class of non-logical symbols.
The non-logical symbols include the predicate and function symbols.
The symbols included in the non-logical symbols may vary from one application of the formal language to the next.


\subsection{The Lexicon of LFOL:}

\begin{definition}[Logical Symbols]
The logical symbols are defined as follows.
\begin{itemize}
\item{\textbf{Variables:} Variable symbols such as $x_1, x_2, \ldots, v_1, v_2, \ldots$. We will try to use lower case letters from near the end of the alphabet for variables. The set of all variables is denoted $\textbf{Var}$.}
\item{\textbf{Logical Operators:} The logical operators for implication: $\implies$ and negation: $\lnot$. The set of all logical operators is denoted $\textbf{Lop}$.}
\item{\textbf{Quantifiers:} The \qq{for all} quantifier: $\forall$. The set of all quantifiers is denoted $\textbf{Quant}$.}
\item{\textbf{Punctuation Marks:} Parenthesis $($ and $)$. The set of all punctuation marks is denoted $\textbf{Punct}$.}
\end{itemize}

The set of all logical symbols is denoted $\textbf{LogSymb}$ and is the disjoint union of the variables, logical operators, quantifiers and punctuation marks: $\textbf{LogSymb} = \textbf{Var} \cup \textbf{Lop} \cup \textbf{Quant} \cup \textbf{Punct}$.
\end{definition}

We have chosen to use a restricted set of logical operators in the language to the exclusion of the conjunction, $\land$, disjunction, $\lor$, and equivalence, $\iff$ connectives and the existential quantifier, $\exists$.
This reduced set of logical symbols will reduce the number of cases over which we need to induct for various metalanguage proofs.
However, we will need to introduce the $\land$, $\lor$, and $\iff$ symbols as abbreviations and metalogically derive the appropriate corresponding rules of inference.

\begin{definition}[Non-Logical Symbols]
The non-logical symbols are defined as follows.
\begin{itemize}
\item{\textbf{Predicate Symbols:} for each arity $0 \le n \le N_{pred}$ predicate symbols such as $P, Q, R$. Examples are $\in, \subset, <$. These examples all have arity 2 but other aritys are possible. The contradiction symbol, $\curlywedge$, is taken to be a 0-ary predicate in our language. The set of all $n$-ary predicate symbols is denoted $\textbf{Pred}_n$. The set of all predicate symbols is denoted $\textbf{Pred}$ and is the disjoint union of the sets of all $n$-ary predicate symbols: $\textbf{Pred} = \bigcup_{i=1}^{N_{\text{pred}}} \textbf{Pred}_i$.}
\item{\textbf{Function Symbols}: for each arity $0 \le n \le N_{\text{func}}$ function symbols such as $f, g, h$. Examples are $+$ and $\times$, these examples both have arity $2$. Note that function symbols of arity $0$ are the same as constants such as $a,b,c,\emptyset,4$. The set of all $n$-ary function symbols is denoted $\textbf{Func}_n$. The set of all function symbols is the disjoint union of the sets of all $n$-ary function symbols and is denoted $\textbf{Func} = \bigcup_{n=1}^{N_{\text{func}}} \textbf{Func}_i$. We reserve constants $\alpha_0, \alpha_1, \ldots \in \textbf{Func}_0$ as `placeholder' constants that exist in any logical theory. The placeholder constants will be used for proofs involving universal specification and generalization, but will not be permitted to appear in logical axioms.}
\end{itemize}

The set of all non-logical symbols is denoted $\textbf{NonLogSymb}$ and is the disjoint union of the predicate and function symbols: $\textbf{NonLogSymb} = \textbf{Pred} \cup \textbf{Func}$. If we are not worried about having an infinite number of distinct symbol types then we can let $N_{\text{pred}} = N_{\text{func}} = \infty$.
\end{definition}

\begin{definition}[The Set of All Symbols]
The set of all symbols in the language is denoted $\textbf{Symb}$ and is the union of the logical and non-logical symbols: $\textbf{Symb} = \textbf{LogSymb} \cup \textbf{NonLogSymb}$.
\end{definition}

\subsection{Strings}
\subsubsection{Definition of and Basic Facts about Lists}
The symbols in $\textbf{Symb}$ can be combined into collections of symbols called strings.
Formally, we will define strings as finite ordered lists of symbols.
This section will be devoted to various definitions and metalinguistic or syntactic proofs regarding ordered lists.
The technical formalism presented here will be required to establish the unique readability of formulas in $\textbf{LFOL}$ and to permit proofs regarding free and bound variable substitution.

\begin{definition}[Natural Number Subset]
Let $[n]$ denote the (possibly empty) set of natural numbers (including 0) which are less than $n$.
\end{definition}

As an example we have that $[n] = \{0, 1, 2, 3, 4\}$.
Note that $[0] = \{\} = \emptyset$.

\begin{definition}[Finite Ordered List]
If $\Sigma$ is any set of objects then a length $n$ finite ordered list of objects in $\Sigma$ is a functional mapping from $\bar{n}$ to $\Sigma$.\footnote{When we use the term `list' below we always mean `finite ordered list'.}
The set of all lists of length $n$ over a set of objects $\Sigma$ is denoted $\Sigma_n^*$.
The set of all lists is the union of all lists of any length $\Sigma^* = \bigcup_{i=0}^{\infty} \Sigma_n^*$.
\end{definition}


An example of a list of elements of $\textbf{Symb}$ defined above is is
\begin{align*}
[&[0, x_1],\\
&[1, \lnot],\\
&[2, Q],\\
&[3, \curlywedge],\\
&[4, (]]
\end{align*}
This list could also be written as
$$
[[0, x_1], [1, \lnot], [2, Q], [3, \curlywedge], [4, (]]
$$
We can also abbreviate the list expression by excluding the explicit inclusion of the numbering:
$$
[x_1, \lnot, Q, \curlywedge, (]
$$

\begin{definition}[List Length]
If $\mc{L}\in\Sigma_n^*\subset \Sigma^*$ is a mapping from $\bar{n}$ into a set of objects $\Sigma$ then we say that $\mc{L}$ is a length $n$ list of elements of $\Sigma$.
We denote the length of the list by $|\mc{L}| = n$.
Note that $|\mc{L}| \ge 0$.
\end{definition}

The natural number length of a list is a fundamental part of its definition in this formal setting.

\begin{definition}[Symbol Equality]
If $\alpha, \beta \in \Sigma$ and $\alpha$ and $\beta$ represent the same object then we may write $\alpha \equiv \beta$.
Otherwise, if $\alpha$ and $\beta$ represent different objects we may write $\alpha \not \equiv \beta$.
\end{definition}

\begin{definition}[List Element Extraction]
Suppose $\mc{L}\in\Sigma_n^*\subset \Sigma^*$ so that $|\mathcal{L}| = n$.
For all $i$ with $0 \le i < n$ we denote the $i^{\text{th}}$ element of $\mc{L}$ (with 0-indexing) by $\mc{L}[i]$.
\end{definition}

For example, in the list above we have $\mc{L}[2] \equiv Q$.

\begin{definition}[List Equality]
If $\mc{A}, \mc{B}\in\Sigma_n^*$ so that $|\mc{A}| = |\mc{B}| = n$ and for all $0 \le i < n$ we have $\mc{A}[i] \equiv \mc{B}[i]$ then we say that $\mc{A}$ and $\mc{B}$ are the same list and we may write $\mc{A} \equiv \mc{B}$.
\end{definition}

Recall that a list can be a mapping from $\bar{0} = \emptyset$.

\begin{definition}[Empty Lists]
Suppose $\mc{L}\in\Sigma_0^*$ so that $|\mc{L}|=0$.
Then we say $\mc{L}$ is an empty list.
\end{definition}

\begin{theorem}[The Empty List is Unique]
Suppose $\mc{E}, \mc{E}' \in \Sigma_0^n$ so that they are both empty lists with $|\mc{E}| = |\mc{E}'| = 0 $.
Since there is no $i$ satisfying $0\le i < 0$ we need not check equality for any elements of these empty lists.
Therefore $\mc{E} \equiv \mc{E}'$.
We denote the unique empty list by $\mc{E}$ and note that $\mc{E}$ can be expressed as $[]$.
\end{theorem}

\subsubsection{List Concatenation}
Suppose $\mc{A}, \mc{B}\in\Sigma^*$.
We define the concatenation of $\mc{A}$ and $\mc{B}$ as follows.

\begin{definition}[List Concatenation]
Suppose $\mc{A}, \mc{B}\in\Sigma^*$.
Then $\mc{A}\circ \mc{B}$, the concatenation of $\mc{A}$ and $\mc{B}$ is defined as

\begin{enumerate}
\item{$\mc{A}\circ\mc{B} \in \Sigma_{|\mc{A}| + |\mc{B}|}^*$ so that $|\mc{A}\circ\mc{B}| = |\mc{A}| + |\mc{B}|$.}
\item{For all $0\le i < |\mc{A}|$ let $(\mc{A}\circ \mc{B})[i] \equiv \mc{A}[i]$.}
\item{For all $0\le i < |\mc{B}|$ let $(\mc{A}\circ\mc{B})[|\mc{A}| + i] \equiv \mc{B}[i]$.}
\end{enumerate}
Note that we may write $\mc{A}\circ\mc{B}$ as $(\mc{A}\circ\mc{B})$ and we may abbreviate it as $\mc{A}\mc{B}$ or $(\mc{A}\mc{B}$).
A consequence of this notation is that we can indicate the $i^{\text{th}}$ element of $\mc{A}\circ\mc{B}$ as $(\mc{A}\circ\mc{B})[i]$ or $(\mc{A}\mc{B})[i]$.
Note that list concatenation can be thought of as a function from $\Sigma^* \times \Sigma^* \to \Sigma^*$.
\end{definition}

\begin{theorem}[Equality of Concatenation with List implies Equality of Lists]
Suppose $\mc{A}, \mc{A}', \mc{B}, \mc{B}' \in \Sigma^*$.
If $\mc{A}\mc{B}\equiv \mc{A}\mc{B}'$ then $\mc{B}\equiv \mc{B}$.
If $\mc{A}\mc{B}\equiv \mc{A}'\mc{B}$ then $\mc{A}\equiv \mc{A}$.

Suppose $\mc{A}\mc{B} \equiv \mc{A}\mc{B}'$.
We have $|\mc{A}\mc{B}| = |\mc{A}| + |\mc{B}| = |\mc{A}\mc{B}'| = |\mc{A}| + |\mc{B}'|$ so $|\mc{B}| = |\mc{B}'| = n$.
Choose $0\le i < n$.
By the definition of concatenation, we have $\mc{A}\mc{B}[|\mc{A}| + i] \equiv \mc{A}\mc{B}'[|\mc{A}| + i] \equiv \mc{B}[i] \equiv \mc{B}'[i]$.
Since this holds for $0\le i < n$ we have $\mc{B} \equiv \mc{B}'$.

Now suppose $\mc{A}\mc{B}\equiv \mc{A}'\mc{B}$.
We have $|\mc{A}\mc{B}| \equiv |\mc{A}| + |\mc{B}| = |\mc{A}'\mc{B}| = |\mc{A}'\mc{B}| = |\mc{A}'| + |\mc{B}|$ so $|\mc{A}| = |\mc{A}'| = n$. Consider $0\le i < n$.
By the definition of concatenation, we have $\mc{A}\mc{B}[i] \equiv \mc{A}'\mc{B}[i] \equiv \mc{A}[i] \equiv \mc{A}'[i]$.
Since this holds for $0 \le i < n$ we have $\mc{A} \equiv \mc{A}'$.
\end{theorem}

\begin{theorem}[Concatenation with the Empty List]
Suppose $\mc{B}\in\Sigma^*$.
Then $\mc{E}\mc{B} \equiv \mc{B} \mc{E} \equiv \mc{B}$.

Suppose $|\mc{B}|= 0$ so that $\mc{B}\equiv \mc{E}$ so that $\mc{E}\mc{B} \equiv \mc{B}\mc{E} \equiv \mc{E}\mc{E}$.
But, by the definition of concatenation, $\mc{E}\mc{E}\in\Sigma_0^*$ so $\mc{E}\mc{E}\equiv \mc{E}\equiv \mc{B}$.

Now suppose $|\mc{B}|>0$.

We have that $|\mc{E}\mc{B}| = |\mc{E}| + |\mc{B}| = |\mc{B}|$.
By the definition of concatenation, we have that, for $0 \le i < |\mc{B}|$, that $(\mc{E}\mc{B})[|\mc{E}| + i] \equiv (\mc{E}\mc{B})[i] \equiv \mc{B}[i]$.
This means that $\mc{E}\mc{B}\equiv\mc{B}$.

We have that $|\mc{B}\mc{E}| = |\mc{B}| + |\mc{E}| = |\mc{B}|$.
By the definition of concatenation, we have that, for $\le i \le |\mc{B}|$, that $(\mc{B}\mc{E})[i] \equiv \mc{B}[i]$.
This means that $\mc{B}\mc{E}\equiv \mc{B}$.
\end{theorem}


\begin{theorem}[List Concatenation is Associative]
\label{thm:concatassoc}
Let $\mc{A}, \mc{B}, \mc{C}\in\Sigma^*$.
Then $(\mc{A}\circ\mc{B})\circ \mc{C} \equiv \mc{A}\circ(\mc{B}\circ\mc{C})$.

If $\mc{A}\equiv \mc{E}$ then $(\mc{A}\mc{B})\mc{C} \equiv \mc{B}\mc{C} \equiv \mc{A}(\mc{B}\mc{C})$.
If $\mc{B} \equiv \mc{E}$ then $(\mc{A}\mc{B})\mc{C}\equiv \mc{A}\mc{C} \equiv \mc{A}(\mc{B}\mc{C})$.
If $\mc{C} \equiv \mc{E}$ then $(\mc{A}\mc{B})\mc{C}\equiv \mc{A}\mc{B}\equiv \mc{A}(\mc{B}\mc{E})$.

Now suppose $|\mc{A}|, |\mc{B}|, |\mc{C}| > 0$ so that none are empty.
Note that
$$
|(\mc{A}\mc{B})\mc{C}| = |\mc{A}\mc{B}| + |\mc{B}| = |\mc{A}|+|mc{B}|+|\mc{C}| = |\mc{A}| + |\mc{B}\mc{C}| = |\mc{A}(\mc{B}\mc{C})|.
$$
Let $0\le i < |\mc{A}|+|\mc{B}|+|\mc{C}| = |(\mc{A}\mc{B})\mc{C}| = |\mc{A}(\mc{B}\mc{C})|$.
We consider three cases.

\begin{itemize}
\item{Suppose $0\le i < |\mc{A}|$. Consider $(\mc{A}\mc{B})\mc{C}$. Since $0\le i < |\mc{A}| \le |\mc{A}\mc{B}|$ we have $((\mc{A}\mc{B})\mc{C})[i] \equiv (\mc{A}\mc{B})[i]$ but we also have that this is equal to $\mc{A}[i]$ so $((\mc{A}\mc{B})\mc{C})[i] \equiv \mc{A}[i]$. On the other hand, consider $\mc{A}(\mc{B}\mc{C})$. Since $0\le i < |\mc{A}|$ we have $(\mc{A}(\mc{B}\mc{C}))[i]\equiv \mc{A}[i]$ so $((\mc{A}\mc{B})\mc{C})[i] \equiv (\mc{A}(\mc{B}\mc{C}))[i]$.}
\item{Suppose $|\mc{A}| \le i < |\mc{A}| + |\mc{B}|$. Let $j = i - |\mc{A}|$ so that $0\le j < |\mc{B}|$.
Consider $(\mc{A}\mc{B})\mc{C}$. Because $0\le |\mc{A}| \le i < |\mc{A}|+|\mc{B}| = |\mc{A}\mc{B}|$ we have that $((\mc{A}\mc{B})\mc{C})[i] \equiv (\mc{A}\mc{B})[i]$.
We then have $(\mc{A}\mc{B})[i] \equiv (\mc{A}\mc{B})[|\mc{A}|+j] \equiv \mc{B}[j] \equiv \mc{B}[i - |\mc{A}|]$ so that $((\mc{A}\mc{B})\mc{C})[i] \equiv \mc{B}[i-|\mc{A}|]$. Now consider $\mc{A}(\mc{B}\mc{C})$. We have $(\mc{A}(\mc{B}\mc{C}))[i] \equiv (\mc{A}(\mc{B}\mc{C}))[|\mc{A}|+j]$ but since $0\le j < |\mc{B}| \le |\mc{B}\mc{C}|$ we have $(\mc{A}(\mc{B}\mc{C}))[j] \equiv (\mc{B}\mc{C})[j]$. Again since $0\le j < |\mc{B}|$, we have $(\mc{B}\mc{C})[j] \equiv \mc{B}[j]\equiv \mc{B}[i-|\mc{A}|]$ so that $(\mc{A}(\mc{B}\mc{C}))[i] \equiv \mc{B}[i-|\mc{A}|]$. This means $((\mc{A}\mc{B})\mc{C})[i] \equiv (\mc{A}(\mc{B}\mc{C}))[i]$.}
\item{Suppose $|\mc{A}| + |\mc{B}| \le i < |\mc{A}|+|\mc{B}|+|\mc{C}|$.
Let $j=i-|\mc{A}|-|\mc{B}| = i-|\mc{A}\mc{B}|$ so that $0\le j < |\mc{C}|$. Consider $(\mc{A}\mc{B})\mc{C}$. We have that $((\mc{A}\mc{B})\mc{C})[i] \equiv ((\mc{A}\mc{B})\mc{C})[|\mc{A}\mc{B}|+j] \equiv \mc{C}[j]\equiv \mc{C}[i-|\mc{A}|-|\mc{B}|]$. On the other hand, consider $\mc{A}(\mc{B}\mc{C})$. We have $(\mc{A}(\mc{B}\mc{C}))[i] \equiv (\mc{A}(\mc{B}\mc{C}))[|\mc{A}|+|\mc{B}|+j]$ but $|\mc{B}| \le |\mc{B}|+j < |\mc{B}|+|\mc{C}| \equiv |\mc{B}\mc{C}|$ so $(\mc{A}(\mc{B}\mc{C}))[|\mc{A}|+|\mc{B}|+j] \equiv (\mc{B}\mc{C})[|\mc{B}|+j] \equiv \mc{C}[j] \equiv \mc{C}[i-|\mc{A}|-|\mc{B}|]$. But this means $((\mc{A}\mc{B})\mc{C})[i] \equiv (\mc{A}(\mc{B}\mc{C}))[i]$.}
\end{itemize}

We have shown for $0\le i < |(\mc{A}\mc{B})\mc{C}| = |\mc{A}(\mc{B}\mc{C})|$ that $((\mc{A}\mc{B})\mc{C})[i] \equiv (\mc{A}(\mc{B}\mc{C}))[i]$ which means $(\mc{A}\mc{B})\mc{C}\equiv \mc{A}(\mc{B}\mc{C})$.
\end{theorem}

\begin{definition}[Multiple Concatenation]
Here we define the set of $n$-concatenations over a list of lists.
Let $\mc{A}_1,\ldots, \mc{A}_n \in \Sigma^*$ and let $S \in (\Sigma^*)^*_n$ with $S = [\mc{A}_1, \ldots, \mc{A}_n]$.
Note that $S$ is a list of lists which is different than a list of symbols.
We will define $\circ_n(S)$ to be the set of all lists which arise from ordered concatenations of the $\mc{A}_i$ contained in $S$.
We define $\circ_n$ recursively.

\begin{itemize}
\item{We define $\circ_1([\mc{A}_0]) = \{\mc{A}_0\}$}
\item{$\circ_n([\mc{A}_0,\ldots, \mc{A}_{n-1}]) = \bigcup_{i=1}^{n-1} \{\mc{X}\circ\mc{Y}:\mc{X} \in \circ_i([\mc{A}_0,\ldots \mc{A}_{i-1}]) \text{ and } \mc{Y}\in \circ_{n-i}([\mc{A}_{i}, \ldots, \mc{A}_{n-1}])\}$}
\end{itemize}
We say that $\circ_n(S)$ is the set of all parenthesizations of the ordered concatenations of the $n$ lists in $S$.

\end{definition}

As an example consider $\circ_4([\mc{A}, \mc{B}, \mc{C}, \mc{D}])$. This will be equal to

\begin{align*}
\circ_4([\mc{A}, \mc{B}, \mc{C}, \mc{D}]) = \big\{& (\mc{A}\circ (\mc{B}\circ(\mc{C}\circ\mc{D}))),\\
& (\mc{A}\circ((\mc{B}\circ\mc{C})\circ\mc{D})) \\
& ((\mc{A}\circ\mc{B})\circ(\mc{C}\circ\mc{D})) \\
& ((\mc{A}\circ(\mc{B}\circ\mc{D}))\circ\mc{D}) \\
& (((\mc{A}\circ\mc{B})\circ\mc{C})\circ\mc{D}) \big\}
\end{align*}

%\begin{align*}
%\circ_4(\mc{A}, \mc{B}, \mc{C}, \mc{D}) = \{&\Braket{\mc{A}\circ\Braket{\mc{B}\circ\Braket{\mc{C}\circ\mc{D}}}},\\
%&\mc{A}\circ\Braket{\Braket{\mc{B}\circ\mc{C}}\circ\mc{D}}},\\
%&\Braket{\mc{A}\circ\mc{B}}\circ\Braket{\mc{C}\circ\mc{D}}},\\
%&\Braket{\mc{A}\circ\Braket{\mc{B}\circ\mc{C}}}\circ\mc{D}},\\
%&\Braket{\Braket{\Braket{\mc{A}\circ\mc{B}}\circ\mc{C}}\circ\mc{D}}\}
%\end{align*}


\begin{theorem}[Generalized Associativity of Multiple Concatenation]
The generalized associativity property for multiple concatenation states that if $S\in (\Sigma^*)_n^*$ then $\circ_n(S)$ contains one unique element.
That is, for $\mc{A}, \mc{B}\in\Sigma^*$, if $\mc{A}, \mc{B} \in \circ_n(S)$ then $\mc{A}\equiv \mc{B}$.
In this case we have $S=[\mc{A}_0,\ldots,\mc{A}_{n-1}]$ with $\mc{A}_0,\ldots,\mc{A}_{n-1}\in \Sigma^*$ and the unique element may be denoted by $\mc{A}_0\circ\ldots \circ \mc{A}_{n-1}$ or $\mc{A}_0\ldots\mc{A}_{n-1}$.
We prove this by induction on $n$.

If $n=1$ then $|S|=1$ and $S = [\mc{A}]$.
We then have $\circ_1(S) = \{\mc{A}\}$.
This set only has one element so we are done.

Now suppose that $n>1$ but for all $1\le m<n$ that if $S\in (\Sigma^*)^*_m$ with $S=[\mc{B}_0,\ldots, \mc{B}_{m-1}]$ with $\mc{B}_0, \ldots, \mc{B}_{m-1}\in\Sigma^*$, then $\circ_m([\mc{B}_0, \ldots, \mc{B}_{m-1}])$ contains a single unique element denoted by $\mc{B}_0\ldots\mc{B}_{m-1}$.

We require the following useful result using the induction hypothesis.
Suppose $1\le m < n$ and $S\in (\Sigma^*)^*_m$ with $S=[\mc{B}_0,\ldots,\mc{B}_{m-1}]$ with $\mc{B}_0,\ldots,\mc{B}_{m-1}\in\Sigma^*$.
Now suppose $1 \le l < m < n$.
Consider $S[0:l] = [\mc{B}_0, \ldots, \mc{B}_{l-1}]$.
We can see $|S[0:l]| = l$ so $S[0:l] \in (\Sigma^*)^*_l$.
By the induction hypothesis, we then have that $\circ_l(S[0:l]) = \circ_l([\mc{B}_0,\ldots, \mc{B}_{l-1}])$ has a unique element denoted by $\mc{B}_0\ldots\mc{B}_{l-1}$.
We also have $1 \le m-l < m < n$.
Consider $S[l:m] = [\mc{B}_l,\ldots, \mc{B}_{m-1}]$.
We can see $|S[l:m]| = m-l$ so $S[l:m]\in (\Sigma^*)^*_{m-l}$.
By the induction hypothesis, we then have that $\circ_{m-l}(S[l:m]) = \circ_{m-l}([\mc{B}_l,\ldots,\mc{B}_{m-1}])$ has a unique element denoted by $\mc{B}_l\ldots\mc{B}_{m-1}$.
By the definition of multiple concatenation, this means
$$
\mc{B}_0\ldots\mc{B}_{l-1}\circ \mc{B}_l\ldots\mc{B}_{m-1} \in \circ_m(S) = \circ_m([\mc{B}_0,\ldots,\mc{B}_{m-1}])
$$
But, again by the induction hypothesis, this set has a unique element denoted by $\mc{B}_0\ldots\mc{B}_{m-1}$.
This means
$$
\mc{B}_0\ldots\mc{B}_{l-1}\circ\mc{B}_l\ldots\mc{B}_{m-1} \equiv \mc{B}_0\ldots\mc{B}_{m-1}
$$
We will use this result below.

Consider now $S\in (\Sigma^*)^*_n$ with $S = [\mc{A}_0,\ldots,\mc{A}_{n-1}]$ with $\mc{A}_0,\ldots,\mc{A}_{n-1}\in \Sigma^*$.
Consider
\begin{align*}
\circ_n([\mc{A}_0,\ldots, \mc{A}_{n-1}]) = \bigcup_{i=1}^{n-1} \{\mc{X}\circ\mc{Y}:\mc{X} \in \circ_i([\mc{A}_0,\ldots \mc{A}_{i-1}]) \text{ and } \mc{Y}\in \circ_{n-i}([\mc{A}_{i}, \ldots, \mc{A}_{n-1}])\}
\end{align*}
Because $1\le i<n$ we know, by the induction hypothesis, that $\circ_i([\mc{A}_0, \ldots, \mc{A}_{i-1}])$ has a unique element denoted by $\mc{A}_0\ldots\mc{A}_{i-1}$.
Likewise, because $1 \le n-i < n$, we know, again by the induction hypothesis, that $\circ_{n-i}([\mc{A}_i, \ldots, \mc{A}_{n-1}])$ has a unique element denoted by $\mc{A}_i \ldots \mc{A}_{n-1}$.
We can then write
\begin{align}
\circ_n([\mc{A}_0, \ldots, \mc{A}_{n-1}]) = \bigcup_{i=1}^{n-1} \{\mc{A}_0\ldots\mc{A}_{i-1}\circ\mc{A}_i\ldots\mc{A}_{n-1}\}
\end{align}

Now suppose $\mc{X}, \mc{Y} \in \circ([\mc{A}_0, \ldots, \mc{A}_{n-1}])$.
This means there exists $j, k$ with $1 \le j \le n-1$ and $1 \le k \le n-1$ with
\begin{align}
\mc{X} \equiv& \mc{A}_0\ldots\mc{A}_{j-1} \circ \mc{A}_j \ldots \mc{A}_{n-1}\\
\mc{Y} \equiv& \mc{A}_0\ldots\mc{A}_{k-1} \circ \mc{A}_k \ldots \mc{A}_{n-1}
\end{align}
We will now show $\mc{X} \equiv \mc{Y}$.

If $j=k$ then clearly $\mc{X}\equiv \mc{Y}$.

We now suppose, without loss of generality, that $k>j$.
Consider $\mc{A}_j\ldots\mc{A}_{n-1}$.
Because $1 \le k-j < n-j < n$, and the intermediate result above, we have
$$
\mc{A}_j\ldots\mc{A}_{n-1} \equiv \mc{A}_j\ldots\mc{A}_{k-1}\circ \mc{A}_k\ldots\mc{A}_{n-1}
$$
Consider also $\mc{A}_0\ldots \mc{A}_{k-1}$.
Because $1 \le j < k < n$, and the intermediate result above, we have
$$
\mc{A}_0\ldots\mc{A}_{k-1} \equiv \mc{A}_0\ldots\mc{A}_{j-1}\circ\mc{A}_j\ldots\mc{A}_{k-1}
$$

We then combine to write
\begin{align}
\mc{X} \equiv& \mc{A}_0\ldots\mc{A}_{j-1} \circ (\mc{A}_j\ldots\mc{A}_{k-1} \circ \mc{A}_k\ldots\mc{A}_{n-1})\\
\mc{Y} \equiv& (\mc{A}_0\ldots\mc{A}_{j-1}\circ\mc{A}_j\ldots\mc{A}_{k-1})\circ\mc{A}_k\ldots\mc{A}_{n-1}
\end{align}
But, we can see by Theorem \ref{thm:concatassoc} (simple associativity of concatenation), that $\mc{X}\equiv \mc{Y}$.
This implies that $\circ_n(\mc{A}_0,\ldots, \mc{A}_{n-1})$ has a unique element which we may denote as
\begin{align*}
\mc{A}_0\ldots\mc{A}_{n-1}
\end{align*}

\end{theorem}

\begin{theorem}[Length of Multiple Concatenations]
\label{thm:list:lenmultconcat}
Suppose $\mc{A} \equiv \mc{A}_0 \ldots \mc{A}_{n-1}$ with $\mc{A}_0,\ldots,\mc{A}_{n-1}\in\Sigma^*$.
We will prove by induction on $n$ that $|\mc{A}| = \sum_{k=0}^{n-1} |\mc{A}|_k$.

Suppose $n=1$.
Then $\mc{A} \equiv \mc{A}_0$ so $|\mc{A}| = |\mc{A}_0|$.

Now suppose that $n>1$ but for $1\le m < n$ that if $\mc{B} \equiv \mc{B}_0\ldots\mc{B}_{m-1}$ that $|\mc{B}| = \sum_{k=0}^{m-1} |\mc{B}_k|$.

We have
$$
\mc{A} \equiv \mc{A}_0 \ldots \mc{A}_{n-2} \circ \mc{A}_{n-1}
$$
But, because $n-1<n$, by the induction hypothesis we have
$$
|\mc{A}_0\ldots\mc{A}_{n-2}| = \sum_{k=0}^{n-2} |\mc{A}_k|
$$
But by the definition of concatenation, we have
$$
|\mc{A}| = \sum_{k=0}^{n-2} |\mc{A}_k| + |\mc{A}_{n-1}| = \sum_{k=0}^{n-1}|\mc{A}_{n-1}|
$$
\end{theorem}

\begin{theorem}[Multiple Concatenation Element Extraction]
\label{thm:list:multconcatextract}
Suppose $\mc{A} \equiv \mc{A}_0\ldots\mc{A}_{n-1}$ with $\mc{A}_0,\ldots \mc{A}_{n-1}\in\Sigma^*$.
Now choose $0\le k < n$ and let $L_{\mc{A},k} = \sum_{j=0}^{k-1} |\mc{A}_j|$ for $k>0$ and $L_{\mc{A},k}=0$ for $k=0$.
Then for $0\le i < |\mc{A}_k|$ we have $\mc{A}[L_{\mc{A},k}+i] \equiv \mc{A}_k[i]$.

Suppose $n=1$.
Then $\mc{A} \equiv \mc{A}_0$ and we must choose $k=0$ with $L_{\mc{A},0}=0$ and $0 \le i < |\mc{A}_0|$.
We have $\mc{A}[L_{\mc{A},0}+i] \equiv \mc{A}[i] \equiv \mc{A}_0[i]$.

Now suppose $n>1$ but for $1\le m < n$ that if $\mc{B}\equiv \mc{B}_0\ldots\mc{B}_{m-1}$ that for $0\le k < m$ that if $L_{\mc{B},k} = \sum_{j=0}^{k-1} |\mc{B}_j|$ then for $0\le i < |\mc{B}_k|$ we have $\mc{B}[L_{\mc{B},k} + i] \equiv \mc{B}_k[i]$.

Now suppose $\mc{A}\equiv \mc{A}_0\ldots \mc{A}_{n-1}$ and $0\le k < n$ with $L_{\mc{A},k} = \sum_{j=0}^{k-1}|\mc{A}_j|$ and $0\le i < |\mc{A}_k|$.
We have that
$$
\mc{A}\equiv \mc{A}_0\ldots \mc{A}_{n-2} \circ \mc{A}_{n-1}
$$
Note that $|\mc{A}_0\ldots\mc{A}_{n-2}| = \sum_{j=0}^{(n-1)-1} |\mc{A}_j|$
We consider two cases.

First suppose $0\le k < n-1$.
Then
$$
L_{\mc{A},k} + i = \sum_{j=0}^{k-1}|\mc{A}_j| + i < \sum_{j=0}^k |\mc{A}_j| \le \sum_{j=0}^{n-2}|\mc{A}_j| = |\mc{A}_0\ldots\mc{A}_{n-2}|
$$
because $0\le i < |\mc{A}_k|$.
This means, by the definition of concatenation, that
$$
\mc{A}[L_{\mc{A},k}+i] \equiv (\mc{A}_0\ldots\mc{A}_{n-2})[L_{\mc{A}, k}+1]
$$
But, by the induction hypothesis we have that $(\mc{A}_0\ldots\mc{A}_{n-2})[L_{\mc{A},k}+i] \equiv \mc{A}_k[i]$ so
$\mc{A}[L_{\mc{A},k}+i] \equiv \mc{A}_k[i]$.

Now suppose $k=n-1$.
Then
$$
L_{\mc{A},k} = \sum_{j=0}^{(n-1)-1}|\mc{A}_j| = |\mc{A}_0\ldots \mc{A}_{n-2}|
$$
so
$$
\mc{A}[L_{\mc{A},k} +i] \equiv \mc{A}_{n-1}[i]
$$
by the definition of concatenation.
\end{theorem}

\subsubsection{List Slicing and Sublists}
In addition to combining lists to form larger lists, we can also slice lists to form smaller lists.

\begin{definition}[List Slicing]
Suppose $\mc{A} \in \Sigma^*$, $0\le i < |\mc{A}|$ and $0 \le n \le |\mc{A}| - i$.
We define $\mc{A}[i:i+n]\in\Sigma^*$ recursively.

\begin{itemize}
\item{If $n=0$ then $\mc{A}[i:i+n] \equiv \mc{E}$.}
\item{If $n=1$ then $\mc{A}[i:i+n] \equiv [\mc{A}[i]]$.\footnote{Note here the distinction between $[\mc{A}[i]]\in\Sigma^*$ and $\mc{A}[i]\in\Sigma$.}}
\item{If $n>1$ then $\mc{A}[i:i+n] \equiv \mc{A}[i:i+n-1] \circ \mc{A}[i+n-1:i+n]$ noting that $\mc{A}[i+n-1:i+n] \equiv [\mc{A}[i+n-1]]$.}
\end{itemize}

Suppose $i \le j \le |\mc{A}|$.
Then $0 \le j-i \le |\mc{A}|-i$ so $\mc{A}[i:i+(j-i)] \equiv \mc{A}[i:j]$ is well-defined according to the definition above.


\end{definition}

\begin{definition}[Sublists and Initial Parts]
Suppose $\mc{A}\in\Sigma^*$, $0\le i < |\mc{A}|$, $i\le j \le |\mc{A}|$ and $\mc{B} \equiv \mc{A}[i:j]$.
\begin{itemize}
\item{We say $\mc{B}$ is a sublist of $\mc{A}$.}
\item{If $i>0$ or $j<|\mc{A}|$ then we say $\mc{B}$ is a proper sublist of $\mc{A}$. Clearly if $\mc{B}$ is a proper sublist of $\mc{A}$ then it is a sublist of $\mc{A}$.}
\item{If $i=0$ then we say $\mc{B}$ is an initial part of $\mc{A}$. Clearly if $\mc{B}$ is an initial part of $\mc{A}$ then it is a sublist of $\mc{A}$.}
\item{If $i=0$ and $j < |\mc{A}|$ then we say $\mc{B}$ is a proper initial part of $\mc{A}$. Clearly if $\mc{B}$ is a proper initial part of $\mc{A}$ then it is an initial part of $\mc{A}$.}
\end{itemize}
\end{definition}

\begin{theorem}[The Empty List is an Initial Part of Every List]
\label{thm:list:emptylistisinitpart}
If $\mc{A}\in\Sigma^*$ then $\mc{A}[0:0] \equiv \mc{E}$ so $\mc{E}$ is an initial part of $\mc{A}$.
\end{theorem}

\begin{theorem}[Length of Sublist]
\label{thm:list:lensublist}
Suppose $\mc{A}\in\Sigma^*$, $0\le i < |\mc{A}|$ and $0\le n \le |\mc{A}|-i$.
Then $|\mc{A}[i:i+n]| = n$.

Suppose $n=0$ so that $\mc{A}[i:i+n] \equiv \mc{E}$.
Then $|\mc{A}[i:i+n]| = 0 = n$.

Suppose $n=1$ so that $\mc{A}[i:i+n] \equiv [\mc{A}[i]]$ so $|\mc{A}[i:i+n]| =1 = n$.

Now suppose $n>1$ but that for $m<n$ with $0\le m \le |\mc{A}|-i$ that $|\mc{A}[i:i+m]| = m$.

We have
$$
\mc{A}[i:i+n] \equiv \mc{A}[i:i+n-1]\circ[\mc{A}[i+n-1]]
$$
Let $m=n-1$ and note $m<n$ and $0\le m \le |\mc{A}|-i$.
Then, by the induction hypothesis, $|\mc{A}[i:i+n-1]|= n-1$.
We also have $|[\mc{A}[i+n-1]]|=1$.
We then have, by the definition of concatenation, that $|\mc{A}[i:i+n]| = (n-1)+1 = n$.
\end{theorem}

\begin{theorem}[Sublist Element Extraction]
\label{thm:list:sublistelementextraction}
Suppose $\mc{A}\in\Sigma^*$, $0\le i < |\mc{A}|$, $0\le n \le |\mc{A}|-i$ and $0\le k < n$.
Then $(\mc{A}[i:i+n])[k] \equiv \mc{A}[i+k]$.

Suppose $n=0$.
Then $\mc{A}[i:i+n] \equiv \mc{E}$.
There is no $k$ satisfying $0\le k < 0$ so the theorem is vacuously true.

Suppose $n=1$.
Then $\mc{A}[i:i+n] \equiv [\mc{A}[i]]$.
We must have $k=0$ and we see $(\mc{A}[i:i+n])[k] \equiv \mc{A}[i]\equiv \mc{A}[i+k]$.

Now suppose $n>1$ but for $m<n$ with $0\le m \le |\mc{A}|-i$ and $0\le k < m$, that $(\mc{A}[i:i+m])[k] \equiv \mc{A}[i+k]$

We have
$$
\mc{A}[i:i+n]\equiv \mc{A}[i:i+n-1] \circ [\mc{A}[i+n-1]]
$$
By Theorem \ref{thm:list:lensublist}, $|\mc{A}[i:i+n-1]| = n-1$.
Let $m=n-1$ noting that $m<n$ and $0\le m \le |\mc{A}|-i$.

There are two cases.
If $k < m = n-1$ then
$$
(\mc{A}[i:i+n])[k] \equiv (\mc{A}[i:i+n-1])[k]
$$
by the definition of concatenation.
Note that $0\le k < m$ so, by the induction hypothesis, $\mc{A}[i:i+n-1])[k]\equiv \mc{A}[i+k]$ meaning $(\mc{A}[i:i+n])[k] \equiv \mc{A}[i+k]$.

If $k=m=n-1=|\mc{A}[i:i+n-1]|$ then
$$
(\mc{A}[i:i+n])[k] \equiv [\mc{A}[i+n-1]][0] \equiv \mc{A}[i+n-1] \equiv \mc{A}[i+k]
$$
by the definition of concatenation.

\end{theorem}

\begin{corollary}[A List is a Sublist of Itself]
\label{corr:list:listsublistself}
Suppose $\mc{A}\in\Sigma^*$.
Then $\mc{A}\equiv \mc{A}[0:|\mc{A}|]$

Let $0\le k < |\mc{A}|$.
By Theorem \ref{thm:list:sublistelementextraction} we have $(\mc{A}[0:|\mc{A}|])[k] \equiv \mc{A}[k]$, but this means $\mc{A}[0:|\mc{A}|] \equiv \mc{A}$.
So we see that $\mc{A}$ is a sublist and initial part of itself.
\end{corollary}

\begin{theorem}[Mutual Initial Parts are Equal]
Suppose $\mc{A}, \mc{A}'\in \Sigma^*$ and $\mc{A}$ is an initial part of $\mc{A}'$ and $\mc{A}'$ is an initial part of $\mc{A}$.
Then $\mc{A}\equiv \mc{A}'$.

We have $\mc{A} \equiv \mc{A}'[0:|\mc{A}|]$ so $|\mc{A}| \le|\mc{A}'|$ and $\mc{A}'\equiv \mc{A}[0:|\mc{A}'|]$ so $|\mc{A}'| \le |\mc{A}|$.
Since $|\mc{A}|\le |\mc{A}|'$ and $|\mc{A}'| \le |\mc{A}|$ we have $|\mc{A}| = |\mc{A}'|$.
So $\mc{A}' \equiv \mc{A}[0:|\mc{A}|] \equiv \mc{A}$ where the second equality follows from Corollary \ref{corr:list:listsublistself}.
\end{theorem}

The following theorem will be critical for proving facts about strings within the formal language.

\begin{theorem}[Corresponding Intermediate Parts]
\label{thm:list:correspondingintermediateparts}
Suppose $\mc{A}, \mc{B}, \mc{C}, \mc{D}, \mc{C}', \mc{D}'\in \Sigma^*$.
If $\mc{A}\equiv \mc{B}\mc{C}\mc{D} \equiv \mc{B}\mc{C}'\mc{D}'$ and $|\mc{C}|\le|\mc{C}'|$ then $\mc{C}$ is an initial part of $\mc{C}'$ so that $\mc{C}\equiv \mc{C}'[0:|\mc{C}|]$.

If $\mc{C}\equiv \mc{E}$ then $\mc{C}'[0:0] \equiv \mc{E} \equiv \mc{C}$ so $\mc{C}$ is an initial part of $\mc{C}'$.

If $\mc{C}$ is non-empty then consider $0\le j < |\mc{C}| \le |\mc{C}'|$.
Let $i=j + |\mc{B}|$ so that $0 \le i < |\mc{B}|+|\mc{C}| = |\mc{B}\mc{C}| \le |\mc{B}|+|\mc{C}'| = |\mc{B}\mc{C}'|$.
Then $\mc{A}[i] \equiv (\mc{B}\mc{C})[i] \equiv (\mc{B}\mc{C}')[i]$.
But we have
\begin{align*}
(\mc{B}\mc{C})[i] &\equiv (\mc{B}\mc{C})[j + |\mc{B}|] \equiv \mc{C}[j] \\
(\mc{B}\mc{C}')[i] &\equiv (\mc{B}\mc{C}')[j+|\mc{B}|] \equiv \mc{C}'[j] \equiv (\mc{C}'[0:|\mc{C}|])[j]
\end{align*}
Where the last equality follows by Theorem \ref{thm:list:sublistelementextraction}.
We then have $\mc{C}\equiv \mc{C}'[0:|\mc{C}|]$.
\end{theorem}

\begin{corollary}[Corresponding Intermediate Parts]
\label{corr:list:correspondingintermediateparts}
Suppose $\mc{A}, \mc{B}, \mc{C}, \mc{D}, \mc{C}', \mc{D}'\in \Sigma^*$ and $\mc{A}\equiv \mc{B}\mc{C}\mc{D} \equiv \mc{B}\mc{C}'\mc{D}'$.
Then either $\mc{C}$ is an initial part of $\mc{C}'$ or $\mc{C}'$ is an initial part of $\mc{C}$.

This follows because we either have $|\mc{C}|\le |\mc{C}'|$ or $|\mc{C}'|\le |\mc{C}|$ and Theorem \ref{thm:list:correspondingintermediateparts}.
\end{corollary}

\subsubsection{Strings from Lists}
Up until now we have been discussing lists of over a general set of objects $\Sigma$.

\begin{definition}[String]
A string is an element of $\textbf{Str} = \textbf{Symb}^*$.
In other words, every string is a finite ordered list of symbols and any finite ordered list of symbols is a string.
\end{definition}

The only distinctive feature about strings compared to lists is that if we have a list of symbols such as

$$
[x_1, \lnot, Q, \curlywedge, (]
$$

We will typically abbreviate it as

$$
x_1\lnot Q\curlywedge (
$$

dropping the separating commas and enclosing square brackets.
In many cases this abbreviation does not cause any confusion.
However, one typical ambiguity caused by this abbreviation is that when we write down, for example, the symbol $\lnot$ it is not clear if we are talking about the symbol $\lnot\in\textbf{Symb}$ or the list containing the symbol $[\lnot] \in \textbf{Str}=\textbf{Symb}^*$.
If it is not clear which is meant from context then it should be explicitly states.

For strings we replace the terms `sublist' and `proper sublist' with `substring' and `proper substring'.
\subsection{The Syntax of LFOL}

Now that we are familiar with strings and their properties and manipulations, we are ready to describe the \textbf{LFOL}.
The \textbf{LFOL} is essentially a subset of \textbf{Str} consisting of strings satisfying certain syntactic or grammatical rules.
We construct \textbf{LFOL} by defining incrementally more complex syntactic structures beginning with terms, followed by atomic formulas, followed by full formulas.

\begin{definition}[Terms]
$t \in \textbf{Term}$ iff $t \in \textbf{Str}$ and satisfies one of
\begin{itemize}
\item{\textbf{Variables:} $t\equiv x$ with $x \in \textbf{Var}$.\footnote{Here it should be clear that we mean $t\equiv [x]\in\textbf{Str}$ as opposed to $t\equiv x \in \textbf{Symb}$.}}
\item{\textbf{Functions:} $t \equiv ft_1 \ldots t_n$ with $f \in \textbf{Func}_n$ and $t_1, \ldots, t_n \in \textbf{Term}$. We may abuse notation and write this as $f(t_1, \ldots, t_n)$ or $f(\bv{t})$. Note that if $f$ is a 0-ary function symbol then $f$ is a term representing a constant.}
\end{itemize}
\end{definition}

Semantically, if the expressions of the formal language are making statements, then terms are the subjects of those statements.
0-ary functions represent constants or concrete objects while variables represent place-holders into which other terms can be `plugged in'.

The next larger syntactic elements in \textbf{LFOL} are the atomic formulas defined as follows.

\begin{definition}[Atomic Formulas]
$\mc{A} \in \textbf{Atom}$ iff $\mc{A} \in \textbf{Str}$ and satisfies

\begin{itemize}
\item{\textbf{Atomic Formulas:} $\mc{A} \equiv Pt_1\ldots t_n$ with $P\in \textbf{Pred}_n$ and $t_1, \ldots, t_n \in \textbf{Term}$. We may abuse notation and write this as $P(t_1,t_2,\ldots,t_n)$ or $P(\bv{t})$. Under these conventions $\curlywedge$ is an atomic formula.}
\end{itemize}
\end{definition}

Semantically, an atomic formula express a boolean statement about the terms within its scope.
If the terms contain no variables then we can, semantically, think of the atomic formula as having a truth value.
If any of the terms contain variables then the atomic formula would only have a truth value once terms with no variables are `plugged in' for all variables.\footnote{These ideas will be made more concrete below when we formally define the free variables of a formula.}



The final layer of syntax is the formula, sometimes called a well-formed formula  (Wff).
Formulas combine multiple atomic formulas into more complex expressions using the logical operators.

\begin{definition}[Formulas]
If $\mc{A}\in\textbf{Str}$ then $\mc{A}\in\textbf{Form}$ iff $\mc{A}$ satisfies one of

\begin{itemize}
\item{\textbf{Atomic Formulas:} $\mc{A}\in\textbf{Atom}$.}
\item{\textbf{Negation:} $\mc{A}\equiv (\lnot \mc{B})$ with $\mc{B} \in \textbf{Form}$.}
\item{\textbf{Implication:} $\mc{A}\equiv (\mc{B}\implies \mc{C})$ with $\mc{B}, \mc{C} \in \textbf{Form}$.}
\item{\textbf{Quantification:} $\mc{A}\equiv (\forall x \mc{B})$ with $x\in \textbf{Var}$ and $\mc{B}\in \textbf{Form}$. We say all substrings of $\mc{B}$ (including $\mc{B}$) appear in the scope of $\forall x$ in $\mc{A}$.}
\end{itemize}
\end{definition}

Semantically, like atomic formulas, formulas express, now more complex, statements about terms.

\begin{definition}[Negation, Implication, Quantification]
Suppose $\mc{A}\in\textbf{Form}$.
We define three new subsets of $\textbf{Str}$: $\textbf{Neg}$, $\textbf{Imp}$ and $\textbf{Quant}$
\begin{itemize}
\item{$\mc{A}\in\textbf{Neg}$ iff $\mc{A}\equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{Form}$.}
\item{$\mc{A}\in\textbf{Imp}$ iff $\mc{A}\equiv (\mc{A}\implies \mc{B})$ with $\mc{B}, \mc{C}\in\textbf{Form}$.}
\item{$\mc{A}\in\textbf{Quant}$ iff $\mc{A} \equiv ((\forall x)\mc{B})$ with $x\in\textbf{Var}$ and $\mc{B}\in\textbf{Form}$.}
\end{itemize}
\end{definition}

From these definitions the following theorem is clear

\begin{theorem}[Permissive Formula Categorization]
\label{thm:synt:formcats}
From the definitions of $\textbf{Form}$, $\textbf{Atom}$, $\textbf{Neg}$, $\textbf{Imp}$ and $\textbf{Qaunt}$, it is clear that $\textbf{Atom}, \textbf{Neg}, \textbf{Imp}, \textbf{Quant} \subset \textbf{Form}\subset \textbf{Str}$.
It is also clear that $\textbf{Form} = \textbf{Atom}\cup \textbf{Neg} \cup \textbf{Imp} \cup \textbf{Quant}$.

This means that if $\mc{A}\in\textbf{Form}$ then either $\mc{A}\in\textbf{Atom}$, $\mc{A}\in\textbf{Neg}$, $\mc{A}\in\textbf{Imp}$, or $\textbf{Quant}$.
\end{theorem}

While it may be intuitively clear that it is not possible for $\mc{A}$ to be an element of any two or more of $\textbf{Atom}$, $\textbf{Neg}$, $\textbf{Imp}$ or $\textbf{Quant}$, this is not directly implied by these definitions.
The fact that these four sets are pairwise disjoint will be proven in the next section on unique readability.

\subsection{Unique Readability}

They syntax of $\textbf{LFOL}$ has an important property that we will refer to as unique readability.
Informally, unique readability means that the language has no ambiguity in the sense that each formula can be interpreted in exactly one way.
This is perhaps easiest to illustrate by way of counter-example.

In the rule for implications we specified that if $\mc{A}\equiv (\mc{B}\implies \mc{C})$ with $\mc{B},\mc{C}\in\textbf{Form}$ then $\mc{A}\in\textbf{Form}$.
Suppose we had left the parentheses out of the rule.
Consider then the candidate formula
$$
\mc{A}\equiv \mc{B}\implies \mc{C}\implies \mc{D}
$$
If we define $\mc{F} \equiv \mc{B}\implies \mc{C}$ and $\mc{G}\equiv \mc{C}\implies \mc{D}$ then we have
$$
\mc{A}\equiv \mc{F}\implies \mc{D} \equiv \mc{B}\implies \mc{G}
$$
This formula $\mc{A}$ then has two, possibly inconsistent, decompositions or interpretations.
When the parentheses are included such ambiguity is not possible as we will prove in this section by demonstrating the unique readability of permissive formulas.


\begin{theorem}[No Proper Initial Part of a Term is a Term]
\label{thm:read:termnopropinit}
Suppose $t \in\textbf{Term}$.
Then if $t'\in\textbf{Term}$ and $t'$ is an initial part of $t$ we have $t'\equiv t$.
Conversely, this theorem means that if $t'$ is a proper initial part of $t$ then $t'\not\in\textbf{Term}$.


If $|t|=0$ then $t\equiv \mc{E}$ and $t\not \in \textbf{Term}$ so we must have $|t|>0$.

If $|t|=1$ then we must have $t'\equiv t[0:1]\equiv t$ because $t[0:0]\equiv \mc{E}\not\in\textbf{Term}$.

Suppose $|t|>1$ but that for any $s\in\textbf{Term}$ with $|s|<|t|$ that if $s'\in\textbf{Term}$  and $s'$ is an initial part of $s$ that $s'\equiv s$.

We have $t, t' \in \textbf{Term}$ so $t\equiv f t_1\ldots t_n$ and $t' \equiv f' t_1'\ldots t_m'$ with $f\in\textbf{Func}_n$, $f'\in\textbf{Func}_m$ and $t_1, \ldots,t_n,t_1',\ldots,t_m'\in\textbf{Term}$.
Because $t'$ is an initial part of $t$ we have $t' \equiv t[0:|t'|]$.
By Theorems \ref{thm:list:multconcatextract} and \ref{thm:list:sublistelementextraction} we have $t[0]\equiv t'[0] \equiv f\equiv f'$ so that $t' \equiv f t_1'\ldots t_n'$.
Note that, by Theorem \ref{thm:list:lenmultconcat} that $|t_1|,\ldots,|t_n|,|t_1'|,\ldots,|t_n'| < |t|$.

We now prove, by a secondary induction, that, for $1\le i \le n$ that $t_i'\equiv t_i$.

Suppose $i=1$.
We have $t\equiv f \circ t_1 \circ t_2\ldots t_n$ and $t' \equiv f \circ t_1' \circ t_2'\ldots t_n'$.
Corollary \ref{corr:list:correspondingintermediateparts} then tells us that either $t_1'$ is an initial part of $t_1$ or $t_1$ is an initial part of $t_1'$.
But because $|t_1|, |t_1'|<|t|$, the first induction hypothesis yields that in either case $t_1'\equiv t_1$.

Now suppose $1<i \le n$ but that for $j<i$ we have $t_j' \equiv t_j$.
We have $t'\equiv f t_1\ldots t_{i-1} \circ t_i' \circ t_{i+1}'\ldots t_n'$.
Corollary \ref{corr:list:correspondingintermediateparts} again implies $t_i$ is an initial part of $t_i'$ or $t_i'$ is an initial part of $t_i$, but the first induction hypothesis again yields that in either case $t_i\equiv t_i'$. Note that if $i = n$ then $\mc{D}$ and $\mc{D}'$ from Corollary \ref{corr:list:correspondingintermediateparts} correspond both to the empty set.
\end{theorem}

\begin{theorem}[Zeroth Symbols in a Term]
\label{thm:read:zerothsymbterm}
If $t\in \textbf{Term}$ then either $t[0]\in\textbf{Var}$ or $t[0]\in\textbf{Func}$.

This can be seen from the two construction rules for $t$.
If $t\equiv x$ with $x\in\textbf{Var}$ then $t[0]\equiv x \in\textbf{Var}$.
If $t\equiv f t_1\ldots t_n$ with $f\in\textbf{Func}_n$ and $t_1,\ldots, t_n\in\textbf{Term}$ then, by Theorem \ref{thm:list:multconcatextract}, we have $t[0] \equiv f \in \textbf{Func}$.
\end{theorem}

\begin{theorem}[Zeroth Symbol in a Permissive Formula]
\label{thm:read:zerothsymbpform}
Suppose $\mc{A}\in\textbf{Form}$.
\begin{itemize}
\item{If $\mc{A}\in\textbf{Atom}$ then $\mc{A}\equiv Pt_1\ldots t_n$ with $P\in\textbf{Pred}$ and $t_1,\ldots,t_n\in\textbf{Term}$. We have $\mc{A}[0] \equiv P \in \textbf{Pred}$.}
\item{If $\mc{A}\in\textbf{Neg}$ then $\mc{A}\equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{Form}$. We have $\mc{A}[0] \equiv ($.}
\item{If $\mc{A}\in\textbf{Imp}$ then $\mc{A}\equiv(\mc{B}\implies \mc{C})$ with $\mc{B}, \mc{C} \in \textbf{Form}$. We have $\mc{A}[0] \equiv ($.}
\item{If $\mc{A}\in\textbf{Quant}$ then $\mc{A}\equiv (\forall x \mc{B})$ with $x\in\textbf{Var}$ and $\mc{B}\in\textbf{Form}$. We have $\mc{A}[0] \equiv ($.}
\item{In total, if $\mc{A}\in\textbf{Form}$ then either $\mc{A}[0] \in \textbf{Pred}$ or $\mc{A}[0] \equiv ($.}
\end{itemize}
\end{theorem}

\begin{theorem}[First Symbols in a Formula]
\label{thm:read:firstsymbform}
Suppose $\mc{A}\in\textbf{Form}$ and $|\mc{A}|>1$.
\begin{itemize}
\item{If $\mc{A}\in\textbf{Atom}$ then $\mc{A}\equiv Pt_1\ldots t_n$ with $P\in\textbf{Pred}$ and $t_1,\ldots,t_n\in\textbf{Term}$.
Note that since $|\mc{A}|>1$ we have $P\not \in \textbf{Pred}_0$. We have $\mc{A}[1] \equiv t_1[0]$. We know from Theorem \ref{thm:read:zerothsymbterm} that $t_1[0] \in \textbf{Var}$ or $t_1[0] \in \textbf{Func}$ so $\mc{A}[1]\in \textbf{Var}$ or $\mc{A}[1] \in \textbf{Func}$.}
\item{If $\mc{A}\in\textbf{Neg}$ then $\mc{A} \equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{Form}$. We have $\mc{A}[1] \equiv \lnot$.}
\item{If $\mc{A}\in\textbf{Imp}$ then $\mc{A}\equiv (\mc{B} \implies \mc{C})$ with $\mc{B}, \mc{C}\in\textbf{Form}$. We have $\mc{A}[1] \equiv \mc{B}[0]$ but we know from Theorem \ref{thm:read:zerothsymbpform} that $\mc{B}[0] \in \textbf{Pred}$ or $\mc{B}[0] \equiv ($ so $\mc{A}[1]\in\textbf{Pred}$ or $\mc{A}[1] \equiv ($.}
\item{If $\mc{A} \in \textbf{Quant}$ then $\mc{A} \equiv (\forall x \mc{B})$ with $x\in \textbf{Var}$ and $\mc{B}\in\textbf{Form}$. We have $\mc{A}[1] \equiv \forall$}
\item{In total we have $\mc{A}[1] \in \textbf{Var}$, $\mc{A}[1]\in\textbf{Func}$, $\mc{A}[1] \equiv \lnot$, $\mc{A}[1]\in\textbf{Pred}$, $\mc{A}[1]\equiv ($ or $\mc{A}[1] \equiv \forall$.}
\end{itemize}
\end{theorem}

\begin{theorem}[Formula Categories are Disjoint]
\label{thm:read:formcatdisjoint}
The sets $\textbf{Atom}, \textbf{Neg}, \textbf{Imp}, \textbf{Quant}\subset \textbf{Form}$ are all pairwise disjoint.
Suppose $\mc{A}\in\textbf{Form}$.
From Theorem \ref{thm:synt:formcats} we know that either $\mc{A}\in\textbf{Atom}$, $\mc{A}\in\textbf{Neg}$, $\mc{A}\in\textbf{Imp}$, or $\mc{A}\in\textbf{Quant}$.

Suppose $\mc{A}\in\textbf{Atom}$. Then $\mc{A}[0] \in \textbf{Pred}$. But for $\mc{A}\in \textbf{Neg}$, $\mc{A}\in\textbf{Imp}$ or $\textbf{Quant}$, we have $\mc{A}[0] \equiv ( \not \in \textbf{Pred}$ so $\mc{A} \not \in \textbf{Neg}, \textbf{Imp}, \textbf{Form}$.

Suppose $\mc{A}\in \textbf{Neg}$.
We already know $\mc{A}\not \in \textbf{Atom}$.
We have $\mc{A}[1] \equiv \lnot$.
But for $\mc{A}\in\textbf{Imp}$ or $\mc{A}\in\textbf{Quant}$ we have $\mc{A}[1]\in \text{Pred}$, $\mc{A}[1]\equiv ($ or $\mc{A}[1]\equiv \forall$.
But $\lnot \not \in \textbf{Pred}$, $\lnot \not \equiv ($ and $\lnot\not \equiv \forall$ so $\mc{A}\not \in \textbf{Imp}$ and $\mc{A} \not \in \textbf{Quant}$.

Suppose $\mc{A} \in \textbf{Imp}$.
We already know $\mc{A} \not \in \textbf{Atom}$ and $\mc{A} \not \in \textbf{Neg}$.
We have $\mc{A}[1] \in \textbf{Pred}$ or $\mc{A}[1]\equiv ($.
But for $\mc{A} \in \textbf{Quant}$ we have $\mc{A}[1] \equiv \forall$.
But $\forall \not \in \textbf{Pred}$ and $\forall \not \equiv ($ so $\mc{A} \not \in \textbf{Quant}$.

Suppose $\mc{A}\in\textbf{Quant}$.
We already know $\mc{A} \not \in \textbf{Neg}$, $\mc{A}\not \in \textbf{Imp}$ and $\mc{A} \not \in \textbf{Imp}$.

This means that $\textbf{Atom}, \textbf{Neg}, \textbf{Imp}, \textbf{Quant}\subset \textbf{Form}$ are all pairwise disjoint.
\end{theorem}

\begin{theorem}[Initial Part of an Atomic Formula]
\label{thm:read:initpartatom}
Suppose $\mc{A}\in\textbf{Atom}$.
Then if $\mc{A}'$ is an initial part of $\mc{A}$ and $\mc{A}'\in\textbf{Form}$ then $\mc{A}' \equiv \mc{A}$.

We have $\mc{A} \equiv Pt_1\ldots t_n$ with $P\in \textbf{Pred}$ and $t_1,\ldots, t_n\in\textbf{Term}$.
We also have that $\mc{A}'[0] \equiv \mc{A}[0] \equiv P$.
If $\mc{A}'\in\textbf{Form}$ then we must have $\mc{A}'\in\textbf{Atom}$ and $\mc{A}' \equiv Pt_1'\ldots t_n'$.

We will prove by induction that $t_i' \equiv t_i$ for each $1\le i \le n$.

If $i=1$ then we have $\mc{A} \equiv P \circ t_1 \circ t_2\ldots t_n$ and $\mc{A}' \equiv P \circ t_1' \circ t_2'\ldots t_n'$.
Corollary \ref{corr:list:correspondingintermediateparts} then tells us that $t_1'$ is an initial part of $t_1$ or $t_1$ is an initial part of $t_1'$, but, Theorem \ref{thm:read:termnopropinit} says that a term can have no proper initial part so we must have $t_1'\equiv t_1$.

Now suppose that for $1 \le i < n$ but that for $j < i$ that $t_j' \equiv t_j$.
Then we have $\mc{A}\equiv P t_1\ldots t_{i-1} \circ t_i \circ t_{i+1} \ldots t_n$ and $\mc{A}' \equiv P t_1 \ldots t_{i-1} \circ t_i' \circ t_{i+1}' \ldots t_n'$.
Again, by Corollary \ref{corr:list:correspondingintermediateparts} we have that $t_i'$ is an initial part of $t_i$ or $t_i$ is an initial part of $t_i'$, but Theorem \ref{thm:read:termnopropinit} tells us a term can not have a proper initial part so we therefore have $t_i' \equiv t_i$.
\end{theorem}

\begin{theorem}[Initial Part of a Permissive Formula]
\label{thm:read:formnopropinitpart}
Suppose $\mc{A}\in\textbf{pForm}$.
If $\mc{A}'$ is an initial part of $\mc{A}$ and $\mc{A}'\in\textbf{Form}$ then $\mc{A}'\equiv \mc{A}$.

Suppose $|\mc{A}| = 1$.
If $|\mc{A}'| = 0$ then $\mc{A}'\not \in \textbf{Form}$ so we must have $|\mc{A}'| = 1$ so that $\mc{A}'\equiv \mc{A}$.

Now suppose $|\mc{A}|>1$ but that for $\mc{B}\in\textbf{Form}$ with $|\mc{B}| < |\mc{A}|$ that if $\mc{B}'$ is an initial part of $\mc{B}$ and $\mc{B}' \in \textbf{Form}$ that $\mc{B}'\equiv \mc{B}$.

Suppose $\mc{A}\in\textbf{Atom}$.
Theorem \ref{thm:read:initpartatom} tells us already that $\mc{A}'\equiv \mc{A}$.

Now suppose $\mc{A}\in\textbf{Neg}$ so that $\mc{A}\equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{Form}$ and $|mc{B}|<|\mc{A}|$.
Note that $\mc{A}[0] \equiv ($ and $\mc{A}[1]\equiv \lnot$.
This means we must have $|\mc{A}'|>2$ (because $($ and $(\lnot$ are not in $\textbf{Form}$) and $\mc{A}'[0]\equiv ($ and $\mc{A}'[1]\equiv \lnot$.
This means that $\mc{A}'\not \in \textbf{Atom}$ because the zeroth element of an atomic formula is in $\textbf{Pred}$.
It also means $\mc{A}'\not\in \textbf{Imp}$ because the first element of an implication is $($ or in $\textbf{Pred}$.
It also means $\mc{A}' \not \in \textbf{Quant}$ because the first element of a quantification is $\forall$.
Therefore we must have $\mc{A}'\in\textbf{Neg}$ with $\mc{A}'\equiv (\lnot \mc{B}')$.
Note $|\mc{B}'|<|\mc{A}'|\le |\mc{A}|$.
But, Corollary \ref{corr:list:correspondingintermediateparts} tells us that either $\mc{B}$ is an initial part of $\mc{B}'$ or $\mc{B}'$ is an initial part of $\mc{B}$.
But in both cases the induction hypothesis tells us that $\mc{B}'\equiv \mc{B}$ so that $\mc{A}'\equiv \mc{A}$.

Now suppose $\mc{A}\in\textbf{Imp}$ so that $\mc{A}\equiv (\mc{B}\implies \mc{C})$ with $\mc{B}, \mc{C}\in\textbf{Form}$ and $|\mc{B}|, |\mc{C}|<|\mc{A}|$.
Note that $\mc{A}[0]\equiv ($ and either $\mc{A}[1]\equiv ($ or $\mc{A}[1] \in\textbf{Pred}$ by Theorem \ref{thm:read:firstsymbform}.
This means $|\mc{A}'|>2$ (because $($ and $(P$ are not in $\textbf{Form}$) and $\mc{A}'[0] \equiv ($ and $\mc{A}'[1] \equiv ($ or $\mc{A}'[1] \in \textbf{Pred}$.
We must have $\mc{A}'\not\in\textbf{Atom}$ because the zeroth element of an atomic formula is in $\textbf{Pred}$.
We also have that $\mc{A}'\not\in\textbf{Neg}$ because the first element of a negation is $\lnot$.
We also have that $\mc{A}'\not\in\textbf{Quant}$ because the first element of a quantification is $\forall$.
So we must have $\mc{A}'\in\textbf{Imp}$ with $\mc{A}'\equiv (\mc{B}'\implies \mc{C}')$.
Note that $|\mc{B}'|,|\mc{C}'| < |\mc{A}'| \le |\mc{A}|$.
Corollary \ref{corr:list:correspondingintermediateparts} tells us that $\mc{B}'$ is an initial part of $\mc{B}$ or $\mc{B}$ is an initial part of $\mc{B}'$.
The induction hypothesis then tells us that $\mc{B}'\equiv \mc{B}$ so that $\mc{A}'\equiv (\mc{B}\implies \mc{C}')$.
Again Corollary \ref{corr:list:correspondingintermediateparts} tells us that $\mc{C}'$ is an initial part of $\mc{C}$ or $\mc{C}$ is an initial part of $\mc{C}'$, but the induction hypothesis then tells us that $\mc{C}'\equiv \mc{C}$.
We conclude, then, that $\mc{A}'\equiv \mc{A}$.

Now suppose $\mc{A}\in\textbf{Quant}$ so that $\mc{A}\equiv (\forall x \mc{B})$ with $x\in\textbf{Var}$ and $\mc{B}\in\textbf{Form}$ with $|\mc{B}|<|\mc{A}|$.
Note that $\mc{A}[0]\equiv ($ and $\mc{A}[1]\equiv \forall$.
This means $|\mc{A}'|>2$ (because $($ and $(\forall$ are not in $\textbf{Form}$ and $\mc{A}'[0]\equiv ($ and $\mc{A}'[1]\equiv \forall$.
We must have $\mc{A}'\not\in\textbf{Atom}$ because the zeroth element of an atomic formula is in $\textbf{Pred}$.
We must have $\mc{A}'\not\in\textbf{Neg}$ because the first element of a negation is $\lnot$.
We also must have $\mc{A}'\not\in\textbf{Imp}$ because the first element of an implication is $($ or is in $\textbf{Pred}$.
We must have that $\mc{A}'\in\textbf{Quant}$ with $\mc{A}'\equiv (\forall x' \mc{B}')$ with $x'\in\textbf{Var}$ and $\mc{B}'\in\textbf{Form}$.
Note that $|\mc{B}'|<|\mc{A}'| \le |\mc{A}|$.
But we must have $\mc{A}'[2] \equiv \mc{A}[2] \equiv x$ so that $x'\equiv x$.
Then, Corollary \ref{corr:list:correspondingintermediateparts} tells us that $\mc{B}'$ is an initial part of $\mc{B}$ or $\mc{B}$ is an initial part of $\mc{B}'$.
The induction hypothesis then tells us that $\mc{B}'\equiv \mc{B}$ so that $\mc{A}'\equiv \mc{A}$.
\end{theorem}

\begin{theorem}[Unique Readability of a Permissive Formula]
Suppose $\mc{A}\in\textbf{Form}$.
\begin{itemize}
\item{If $\mc{A}\in\textbf{Atom}$ then $\mc{A}\equiv Pt_1\ldots t_n$ with $P\in\textbf{Pred}_n$ and $t_1,\ldots, t_n$. If we also have $\mc{A}\equiv P' t_1'\ldots t_m'$ with $P'\in\textbf{Pred}_m$ and $t_1',\ldots,t_m'\in\textbf{Term}$ then $P' \equiv P$, $m=n$ and $t_i'\equiv t_i$ for $1\le i \le n$.}
\item{If $\mc{A}\in\textbf{Neg}$ then $\mc{A}\equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{Form}$. If we also have $\mc{A}\equiv (\lnot \mc{B}')$ with $\mc{B}'\in\textbf{Form}$ then $\mc{B}'\equiv \mc{B}$.}
\item{If $\mc{A}\in\textbf{Imp}$ then $\mc{A}\equiv (\mc{B}\implies \mc{C})$ with $\mc{B},\mc{C}\in\textbf{Form}$. If we also have $\mc{A}\equiv (\mc{B}'\implies \mc{C}')$ with $\mc{B}', \mc{C}'\in\textbf{Form}$ then $\mc{B}'\equiv \mc{B}$ and $\mc{C}'\equiv \mc{C}$.}
\item{If $\mc{A}\in\textbf{Quant}$ then $\mc{A}\equiv (\forall x \mc{B})$ with $x\in\textbf{Var}$ and $\mc{B}\in\textbf{Form}$. If we also have $\mc{A}\equiv (\forall x' \mc{B}')$ with $x'\in\textbf{Var}$ and $\mc{B}'\in\textbf{Form}$ then $x'\equiv x$ and $\mc{B}'\equiv \mc{B}$.}
\end{itemize}
We now prove each of these claims in turn.

Suppose $\mc{A}\in\textbf{Atom}$ with $\mc{A}\equiv Pt_1\ldots t_n\equiv P't_1'\ldots t_m'$ with $P\in\textbf{Pred}_n$, $P'\in\textbf{Pred}_m$ and $t_1,\ldots,t_n,t_1',\ldots t_m'\in\textbf{Term}$.
We have $\mc{A}[0]\equiv P\equiv P'$ so we also have $m=n$.
We perform induction to prove $t_i'\equiv t_i$ for $1\le i \le n$.
If $i=1$ then we have $\mc{A}\equiv P\circ t_1 \circ t_2\ldots t_n \equiv P\circ t_1' \circ t_2'\ldots t_n'$, But Corollary \ref{corr:list:correspondingintermediateparts} and Theorem \ref{thm:read:termnopropinit} tell us that $t_1'\equiv t_1$.
Now suppose $i > 1$but for $j<i$ that $t_j'\equiv t_j$.
Then $\mc{A}\equiv Pt_1\ldots t_{i-1} \circ t_i \circ t_{i+1} \ldots t_n \equiv Pt_1\ldots t_{i-1}\circ t_i' \circ t_{i+1}' \ldots t_n'$.
Corollary \ref{corr:list:correspondingintermediateparts} and Theorem \ref{thm:read:termnopropinit} again tell us $t_i'\equiv t_i$.

Now suppose $\mc{A}\in \textbf{Neg}$ with $\mc{A}\equiv (\lnot \mc{B}) \equiv (\lnot \mc{B}')$ with $\mc{B}, \mc{B}'\in\textbf{Form}$.
Corollary \ref{corr:list:correspondingintermediateparts} and Theorem \ref{thm:read:formnopropinitpart} tell us that $\mc{B}'\equiv \mc{B}$.

Now suppose $\mc{A}\in\textbf{Imp}$ with $\mc{A}\equiv (\mc{B}\implies \mc{C})\equiv (\mc{B}'\equiv \mc{C}')$ with $\mc{B}, \mc{B}', \mc{C}, \mc{C}' \in \textbf{Form}$.
One application of Corollary \ref{corr:list:correspondingintermediateparts} and Theorem \ref{thm:read:formnopropinitpart} tells us that $\mc{B}'\equiv \mc{B}$ and then a second application tells us $\mc{C}'\equiv \mc{C}$.

Now suppose $\mc{A}\in\textbf{Quant}$ with $\mc{A}\equiv (\forall x \mc{B}) \equiv (\forall x'\mc{B}')$ with $x, x' \in \textbf{Var}$ and $\mc{B}, \mc{B}'\in\textbf{Form}$.
We have that $\mc{A}[1] \equiv x \equiv x'$.
An application of \ref{corr:list:correspondingintermediateparts} and Theorem \ref{thm:read:formnopropinitpart} tells us that $\mc{B}'\equiv \mc{B}$.
\end{theorem}




\subsubsection{old}
The unique readability of permissive formulas allows to define recursively define functions on permissive formulas as we demonstrate below.

\begin{definition}[Depth of a Permissive Formula]
If $\mc{A}\in\textbf{pForm}$ then $D(\mc{A})$ is the formula depth (or depth) of $\mc{A}$ and is defined recursively.
\begin{itemize}
\item{If $\mc{A}$ is atomic then $D(\mc{A}) = 0$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ with $\mc{B}, \mc{C} \in \textbf{pForm}$ then $D(\mc{A}) = \text{max}(D(\mc{B}), D(\mc{C})) + 1$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$ then $D(\mc{A}) = D(\mc{B}) + 1$.}
\item{If $\mc{A} \equiv ((\forall x) \mc{B})$ with $x\in\textbf{Var}$ and $\mc{B} \in \textbf{pForm}$ then $D(\mc{A}) = D(\mc{B}) + 1$.}
\end{itemize}
\end{definition}

Note that it would not have been possible to define $D(\mc{A})$ without Theorem \ref{thm:uniqueread} and the other theorems of this section.
For example, suppose we have $\mc{A} \equiv (\lnot \mc{B})$.
However, suppose it is also the case that $\mc{A} \equiv (\lnot \mc{B}')$.
The function $D(\mc{A})$ would not be well defined unless we had a guarantee that $D(\mc{B}) = D(\mc{B}')$.
For a more extreme example suppose $\mc{A} \equiv (\lnot \mc{B})$ and $\mc{A} \equiv (\mc{C} \implies \mc{D})$.
We would then need a guarantee that $D(\mc{B}) = \text{max}(D(\mc{C}, \mc{D}))$.
Unique readability ensures that there is only one way to parse, or decompose, a permissive formula thus allowing us to define functions on the parsing of the permissive formula.

\subsection{Free and Bound Variable Identification}

In this section we will define the free and bound variables in a permissive formula $\mc{A}$.


\begin{definition}[Free Variables in Terms]
If $t\in \textbf{Term}$ then $t$ we define the set $FV(t)$ as follows.
\begin{itemize}
\item{\textbf{Variables:} If $t\equiv x$ with $x\in\textbf{Var}$ then $FV = \{x\}$}
\item{\textbf{Functions:} If $t\equiv ft_1\ldots t_n$ with $f\in\textbf{Func}_n$ and $t_1,\ldots,t_n \in \textbf{Term}$ then $FV(t) = FV(t_1)\cup\ldots \cup FV(t_n)$.}
\item{If $x\in \textbf{Var}$ then if $x\in FV(t)$ we say $x$ appears free in $t$. Otherwise we say $x$ does not appear free in $t$.}
\item{if $FV(t)=\emptyset$ then we say $t$ is closed. Otherwise we say $t$ is open.}
\end{itemize}
\end{definition}

Semantically closed terms are concrete objects about which statements can be made while open terms contain variables into which other terms can be plugged into to eventually realize a particular closed object and concrete object.

\begin{definition}[Free Variables in Permissive Formulas]
If $\mc{A}\in\textbf{pForm}$ then we define $FV(\mc{A})$ recursively.
\begin{itemize}
\item{If $\mc{A} \equiv Pt_1\ldots t_n$ for $n$-ary predicate $P$ then $FV(\mc{A}) = FV(t_1)\cup\ldots\cup FV(t_n)$. Note that this implies $FV(P)=\emptyset$ if $P$ is a 0-ary predicate such as $\curlywedge$.}
\item{If $\mc{A} \equiv (\mc{B}\implies \mc{C})$ for Wffs $\mc{B}$ and $\mc{C}$ then $FV(\mc{A}) = FV(\mc{B})\cup FV(\mc{C})$.}
\item{If $\mc{A} \equiv (\lnot\mc{B})$ then $FV(\mc{A}) = FV(\mc{B})$.}
\item{If $\mc{A} \equiv ((\forall x)\mc{B})$ then $FV(\mc{A}) = FV(\mc{B}) - \{x\}$.}
\item{If $x \in \textbf{Var}$ and $x\in FV(\mc{A})$ then we say $x$ appears free in $\mc{A}$. Otherwise we say $x$ does not appear free in $\mc{A}$.}
\item{If $FV(\mc{A}) = \emptyset$ then we say $\mc{A}$ is closed. Otherwise we say $\mc{A}$ is open.}
\end{itemize}
\end{definition}

Note that $((\forall x) \mc{B})$ is a permissive formula even if $x\not \in FV(\mc{B})$.
In such cases $\mc{A}$ exhibits redundant quantification.
For example $((\forall x) Py)$ is a permissive formula.

Semantically closed formulas are concrete statements which can have a truth value whereas open formulas contain variables into which terms can be plugged in to eventually realize a closed formula and concrete statement.

\begin{definition}[Quantified Variables in Permissive Formulas]
If $\mc{A}$ is a permissive formula then we define $QV(\mc{A})$ recursively.

\begin{itemize}
\item{If $\mc{A}$ is an atomic formula then $QV(\mc{A}) = \emptyset$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$ then $QV(\mc{A}) \equiv QV(\mc{B})$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ with $\mc{B}, \mc{C}\in\textbf{pForm}$ then $QV(\mc{A}) = QV(\mc{B}) \cup QV(\mc{C})$.}
\item{If $\mc{A} \equiv ((\forall x) \mc{B})$ with $x\in\textbf{Var}$ and $\mc{B}\in\textbf{pForm}$ then $QV(\mc{A}) = QV(\mc{B}) \cup \{x\}$.}
\end{itemize}
\end{definition}

Note that $((\forall x)\mc{B})$ is a permissive formula even if $x\in QV(\mc{B})$.
In such cases $\mc{A}$ exhibits repeated quantification.

Note also that it is possible to have $x\in FV(\mc{A})$ and $x\in QV(\mc{A})$.
For example
\begin{align}
\mc{A} \equiv (Px \implies ((\forall x) Qy))
\end{align}
We have $FV(\mc{A}) = \{x, y\}$ and $QV(\mc{A}) = \{x\}$.

\subsection{Strict Formulas}

We are now in a position to define strict formulas.
\begin{definition}[Strict Formulas]
$\mc{A}\in\textbf{sForm} = \textbf{Form}$ iff $\mc{A} \in \textbf{pForm}$ and satisfies one of
\begin{itemize}
\item{\textbf{Atomic Formulas}: $\mc{A}$ is an atomic formula.}
\item{\textbf{Implication}: $\mc{A} \equiv (\mc{B} \implies \mc{C})$ and $\mc{B}, \mc{C} \in \textbf{Form}$.}
\item{\textbf{Negation}: $\mc{A} \equiv (\lnot \mc{B})$ and $\mc{B}\in\textbf{Form}$.}
\item{\textbf{Quantification}: $\mc{A} \equiv ((\forall x)\mc{B})$ with $x\in\textbf{Var}$, $\mc{B}\in\textbf{Form}$, and $x\in FV(\mc{B})$ and $x\not\in QV(\mc{B})$.}
\end{itemize}
\end{definition}

Note that we do not forbid a variable to appear both free and bound in a strict formula.
For example,
$$
\mc{A} \equiv (Px \implies ((\forall x) Qxy))
$$
We have $\mc{A}\in\textbf{sForm}$ but $x\in FV(\mc{A})$ and $x\in QV(\mc{A})$.
However, if possible it is best to avoid this practice.

Note that we also do not forbid multiple uses of the same quantified variable:
$$
\mc{A}\equiv (((\forall x) Px) \land ((\forall x) Qx))
$$
has $\mc{A}\in\textbf{sForm}$ with $QV(\mc{A}) = \{x\}$.

\subsection{Free Variable Substitution}

\begin{definition}[Free Variable Substitution]
We define recursively the concept of free variable substitution.
Suppose $s, t \in \textbf{Term}$ and $x\in \textbf{Var}$.
\begin{itemize}
\item{If $s\in\textbf{Var}$ then if $s\equiv x$ we define $s[t/x] \equiv t$. If $s\not \equiv x$ then $s[t/x] \equiv x$.}
\item{If $s \equiv fs_1\ldots s_n$ with $f\in\textbf{Func}_n$ and $s_1, \ldots s_n \in \textbf{Term}$ then $s[t/x] \equiv fs_1[t/x]\ldots s_n[t/x]$. This definition implies that if $f\in \textbf{Func}_0$ and $s\equiv f$ then $s[t/x] \equiv s \equiv f$.}
\end{itemize}

Suppose $\mc{A} \in \textbf{pForm}$.
\begin{itemize}
\item{If $\mc{A}$ is atomic then $\mc{A} \equiv Ps_1\ldots s_n$ with $P\in\textbf{Pred}_n$ and $s_1\ldots s_n\in\textbf{Term}$. We define $\mc{A}[t/x] \equiv Ps_1[t/x]\ldots s_n[t/x]$. This definition implies that if $P\in\textbf{Pred}_0$ and $\mc{A}\equiv P$ then $\mc{A}[t/x] \equiv \mc{A} \equiv P$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ with $\mc{B}, \mc{C}\in\textbf{pForm}$ then $\mc{A}[t/x] \equiv (\mc{B}[t/x] \implies \mc{C}[t/x])$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$ then $\mc{A}[t/x] \equiv (\lnot \mc{B}[t/x])$.}
\item{If $\mc{A} \equiv ((\forall y) \mc{B})$ with $x\in\textbf{Var}$ and $\mc{B}\in\textbf{pForm}$ then if $x\not \equiv y$ then $\mc{A}[t/x] \equiv ((\forall y) \mc{B}[t/x])$. If $x \equiv y$ then $\mc{A}[t/x] \equiv \mc{A}$.}
\end{itemize}

\end{definition}

\begin{theorem}[Identical Free Variable Substitution]
\label{thm:idenfvsub}
Suppose $x\in\textbf{Var}$, $t\in\textbf{Term}$ and $\mc{A}\in\textbf{pForm}$.
Then $t[x/x] \equiv t$ and $\mc{A}[x/x]\equiv \mc{A}$.

Suppose $|t| = 0$.
Then either $t\in \textbf{Var}$ or $t\in\textbf{Func}_0$.
If $t\in\textbf{Var}$ then if $t\equiv x$ then $t[x/x] \equiv x \equiv t$ otherwise if $t \not \equiv x$ then $t[x/x] \equiv t$.
If $t\in\textbf{Func}_0$ then $t[x/x] \equiv t$.

Now suppose $|t|>0$ but that for $s\in\textbf{Term}$ with $|s|<|t|$ that $s[x/x] \equiv s$.

Since $|t|>0$ we have $t\not \in \textbf{Var}$ and $t\not \in \textbf{Func}_0$.
So $t\equiv fs_1\ldots s_n$ with $f\in\textbf{Func}_n$ and $s_1,\ldots, s_n\in\textbf{Term}$.
We have $|s_1|,\ldots,|s_n| < |t|$.
We have $t[x/x] \equiv f s_1[x/x]\ldots s_n[x/x]$ but, by the induction hypothesis $s_i[x/x] \equiv s$ for $1\le i \le n$ so $t[x/x]\equiv fs_1\ldots s_n \equiv t$.
\end{theorem}



\begin{theorem}[Free Variable Substitution Within a Term Results in a Term]
\label{thm:fvsubintermgivesterm}
Suppose $s, t\in\textbf{Term}$ and $x\in\textbf{Var}$.
We prove by induction on $|s|$ that $s[t/x] \in \textbf{Term}$.

if $|s| = 1$ then there are two cases.
First suppose $s\in \textbf{Var}$.
If $s\in \textbf{Var}$ then if $s\equiv x$ $s[t/x] \equiv t$ so $s[t/x]\in\textbf{Term}$.
If $s \not \equiv x$ then $s[t/x] \equiv s$ so again $s[t/x] \in \textbf{Term}$.
The other case is $s\equiv f$ with $f\in \textbf{Func}_0$.
In this case $s[t/x] \equiv s$ so $s[t/x] \in \textbf{Term}$.

Now suppose $|s| > 1$.
The induction hypothesis is that for any term $r$ with $|r|<|s|$ we have $r[t/x] \in \textbf{Term}$.
We have $s\equiv ft_1\ldots t_n$ with $f\in\textbf{Func}_n$. $s[t/x] \equiv f t_1[t/x]\ldots t_n[t/x]$, but each of $t_i[t/x]\in\textbf{Term}$ for $1\le i \le n$ so $s[t/x]\in\textbf{Term}$.
\end{theorem}

\begin{theorem}[Free Variable Substitution in Term for Variable the Does Not Appear Free]
\label{thm:fvsubtermvarnotappear}
Suppose $s, t \in \textbf{Term}$ and $x \in \textbf{Var}$. Suppose that $x\not \in FV(s)$. Then $s[t/x] \equiv s$.

If $|s| = 1$ then there are two cases. First suppose $s\in \textbf{Var}$. Since $x\not \in FV(s)$ we must have $s \equiv y \not \equiv x$ so $s[t/x] \equiv s$. The other case is that $s\in \textbf{Func}_0$ in which case $s[t/x]\equiv s$.

Now suppose $|s|>1$ The induction hypothesis is that for any term $r$ with $|r| < |s|$ that if $x \not \in FV(r)$ that $r[t/x] \equiv r$. We have that $s \equiv fr_1\ldots r_n$ with $f$ in $\textbf{Func}_n$. We have that $FV(s) = FV(r_1)\cup\ldots\cup FV(r_n)$. Since $x\not \in FV(s)$ it must be the case that $x \not \in FV(r_i)$ for $1\le i \le n$. Now $s[t/x] \equiv fr_1[t/x]\ldots r_n[t/x]$ but since $|r_i|<|s|$ and $x\not \in FV(r_i)$ we have that $r_i[t/x] \equiv r_i$ for each $1\le i \le n$ so that $s[t/x] \equiv fr_1\ldots r_n \equiv s$.
\end{theorem}

\begin{theorem}[Free Variable Substitution Within a Permissive Formula Results in a Permissive Formula]
Suppose $\mc{A}\in\textbf{pForm}$, $x\in\textbf{Var}$ and $t \in \textbf{Term}$.
We prove, by induction of $D(\mc{A})$, that $\mc{A}[t/x] \in \textbf{pForm}$.

Suppose $D(\mc{A}) = 0$.
Then $\mc{A}$ is atomic so $\mc{A}\equiv Pt_1\ldots t_n$ with $P\in\textbf{Pred}$ and $t_1,\ldots, t_n\in\textbf{Term}$. $\mc{A}[t/x] \equiv Pt_1[t/x]\ldots t_n[t/x]$, but, by the previous theorem, $t_1[t/x],\ldots,t_n[t/x]\in\textbf{Term}$ so $\mc{A}[t/x]$ is atomic so $\mc{A}[t/x]\in\textbf{pForm}$.

Now suppose that $D(\mc{A}) > 0$ but that for any $\mc{B}$ with $D(\mc{B}) < D(\mc{A})$ we have that $\mc{B}[t/x] \in \textbf{pForm}$.

$\mc{A}$ cannot be atomic since $D(\mc{A})>0$.

Suppose $\mc{A} \equiv (\mc{B}\implies \mc{C})$ with $\mc{B}, \mc{C}\in\textbf{pForm}$.
Then $\mc{A}[t/x] \equiv (\mc{B}[t/x] \implies \mc{C}[t/x])$.
Since $D(\mc{A}) = \text{max}(D(\mc{B}) + D(\mc{C}))$ we have $D(\mc{B}), D(\mc{C}) < D(\mc{A})$ so, by the induction hypothesis, $\mc{B}[t/x], \mc{C}[t/x]\in\textbf{pForm}$ which means $\mc{A}[t/x]\in\textbf{pForm}$.

Suppose $\mc{A} \equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$.
Then $\mc{A}[t/x] \equiv (\mc{B}[t/x])$.
$D(\mc{B}) = D(\mc{A}) -1 < D(\mc{A})$ so, by the induction hypothesis, $\mc{B}[t/x]\in\textbf{pForm}$ which means $\mc{A}[t/x]\in\textbf{pForm}$.

Suppose $\mc{A} \equiv ((\forall y) \mc{B})$ with $y\in\textbf{Var}$ and $\mc{B}\in\textbf{pForm}$.
If $x\equiv y$ then $\mc{A}[t/x] \equiv \mc{A}$ so $\mc{A}[t/x]\in\textbf{pForm}$.
If $x\not\equiv y$ then $\mc{A}[t/x] \equiv ((\forall y) \mc{B}[t/x])$.
$D(\mc{B}) = D(\mc{A})-1$ so $\mc{B}[t/x]\in\textbf{pForm}$ which means $\mc{A}[t/x]\in\textbf{pForm}$.

\end{theorem}

\begin{theorem}[Free Variable Substitution in a Permissive Formula for a Variable that Does Not Appear Free]
\label{thm:fvsubpformnotappear}
Suppose $t\in \textbf{Term}$, $x\in \textbf{Var}$ and $\mc{A} \in \textbf{pForm}$. If $x\not \in FV(\mc{A})$ then $\mc{A}[t/x] \equiv \mc{A}$.

Suppose $D(\mc{A}) = 0$. Then $\mc{A}$ is atomic so $\mc{A} \equiv P t_1\ldots t_n$. $FV(\mc{A}) \equiv FV(t_1)\cup\ldots\cup FV(t_n)$. Since $x\not \in FV(\mc{A})$ we must have $x\not \in FV(t_i)$ for $1\le i \le n$. $\mc{A}[t/x] \equiv Pt_1[t/x]\ldots t_n[t/x]$. But since $x\not \in FV(t_i)$ we have, by Theorem \ref{thm:fvsubtermvarnotappear}, that $t_i[t/x] \equiv t_i$ so that $\mc{A}[t/x] \equiv Pt_1\ldots t_n \equiv \mc{A}$.

Now suppose that $D(\mc{A}) > 0$ but that for any $\mc{B}\in\textbf{pForm}$ with $D(\mc{B}) < D(\mc{A})$ that if $x \not \in FV(\mc{B})$ that $\mc{B}[t/x] \equiv \mc{B}$.

$\mc{A}$ cannot be atomic because $D(\mc{A})>0$.

Suppose $\mc{A}$ is an implication with $\mc{A} \equiv (\mc{B} \implies \mc{C})$ with $\mc{B}, \mc{C}\in\textbf{pForm}$.
Since $x\not \in FV(\mc{A})$ we must have $x\not \in FV(\mc{B})$ and $x \not \in FV(\mc{C})$.
We also have $D(\mc{B}), D(\mc{C}) < D(\mc{A})$.
We have $\mc{A}[t/x] \equiv (\mc{B}[t/x] \implies \mc{C}[t/x])$ but, by the induction hypothesis, we have that $\mc{B}[t/x] \equiv \mc{B}$ and $\mc{C}[t/x] \equiv \mc{C}$ so $\mc{A}[t/x] \equiv \mc{A}$.

Suppose $\mc{A}$ is a negation with $\mc{A} \equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$.
Since $x\not \in FV(\mc{A})$ we must have $x\not \in FV(\mc{B})$.
We also have $D(\mc{B}) < D(\mc{A})$.
We have $\mc{A}[t/x] \equiv (\lnot \mc{B}[t/x])$ but, by the induction hypothesis, we have $\mc{B}[t/x] \equiv \mc{B}$ so that $\mc{A}[t/x] \equiv \mc{A}$.

Suppose $\mc{A}$ is a quantification with $\mc{A} \equiv ((\forall y)\mc{B})$ with $y\in\textbf{Var}$ and $\mc{B}\in\textbf{pForm}$.
We have the $D(\mc{B}) < D(\mc{A})$.
If $x\equiv y$ then $\mc{A}[t/x] \equiv \mc{A}$.
We have that $FV(\mc{A}) = FV(\mc{B})\setminus\{y\}$.
If $x \not \equiv y$ then $x\not \in FV(\mc{A})$ implies that $x\not \in FV(\mc{B})$.
In this case $\mc{A}[t/x] \equiv ((\forall y) \mc{B}[t/x])$ but the induction hypothesis gives us that $\mc{B}[t/x] \equiv \mc{B}$ so $\mc{A}[t/x] \equiv \mc{A}$.
\end{theorem}

\begin{theorem}[Free Variables in a Term after Free Variable Substitution]
\label{thm:fvtermafterfvsub}
Suppose $s, t \in \textbf{Term}$ and $x\in\textbf{Var}$. If $x \not \in FV(s)$ then $s[t/x] \equiv s$ so $FV(s[t/x]) = FV(s)$. If $x\in FV(s)$ then $FV(s[t/x]) = (FV(s)\setminus \{x\})\cup FV(t)$. We prove this by induction.

Suppose $|s|=1$. Since $x\in FV(s)$ this implies that $s \equiv x$ so that $FV(s)=\{x\}$. Then $s[t/x] \equiv t$ so $FV(s[t/x]) = FV(t)$ but $(FV(s)\setminus\{x\})\cup FV(t) = FV(t)$ as needed.

Now suppose that $|s|>1$ and for $r\in \textbf{Term}$ with $|r|<|s|$ that if $x\in FV(r)$ that $FV(r[t/x]) = (FV(r)\setminus \{x\})\cup FV(t)$. We have that $s\equiv fr_1\ldots r_n$ and $FV(s) = FV(r_1)\cup\ldots\cup FV(r_n)$.

Since $x\in FV(s)$ by assumption then it must be in at least one of the $r_i$. Suppose that there are $1\le j \le n$ values of $i$ for which $x \in FV(r_i)$ and $k = n-j$ values of $i$ for which $x \not \in FV(r_i)$.

Suppose that $x\in FV(r_i)$ for $1\le j \le n$ values of $i$ and $x \not \in FV(r_i)$ for $k = n-j$ values of $i$. Let $a_i$ for $1 \le i \le j$ enumerate the values for which $x\in FV(r_{a_i})$ and let $b_i$ for $1 \le i \le n-j$ enumerate the values for which $x \not \in FV(r_{b_i})$.

We then have $s[t/x] \equiv fr_1[t/x]\ldots r_n[t/x]$ and we can write
$$
FV(s[t/x]) = FV(r_{a_1}[t/x])\cup\ldots\cup FV(r_{a_j}[t/x]) \cup FV(r_{b_1}[t/x])\cup \ldots \cup FV(r_{b_k}[t/x])
$$

By the induction hypothesis we have $FV(r_{a_i}[t/x]) = (FV(r_{a_i})\setminus \{x\}) \cup FV(t)$ and $FV(r_{b_i}[t/x]) = FV(r_{b_i})$ so

\begin{align}
FV(s[t/x]) = \big(&((FV(r_{a_1})\setminus \{x\}) \cup FV(t)) \cup \ldots \cup ((FV(r_{a_j})\setminus \{x\}) \cup FV(t)) \nonumber\\
&\cup FV(r_{b_1}) \cup\ldots \cup FV(r_{b_k})\big) \nonumber
\end{align}

Using some set identities and the fact that $x \not \in FV(r_{b_i})$ we can rewrite this as

\begin{align}
FV(s[t/x]) &= \left(\left(FV(r_{a_1}) \cup \ldots \cup FV(r_{a_j}) \cup FV(r_{b_1}) \cup \ldots \cup FV(r_{b_{n-j}})\right) \setminus \{x\}\right) \cup FV(t) \nonumber\\
&= \left(\left(FV(r_1)\cup \ldots \cup FV(r_n)\right)\setminus \{x\}\right)\cup FV(t) \nonumber\\
&= (FV(s)\setminus \{x\})\cup FV(t) \nonumber
\end{align}
\end{theorem}

\begin{theorem}[Free Variables in a Permissive Formula after Free Variable Substitution]
\label{thm:fvinpformafterfvsub}
Suppose $\mc{A} \in \textbf{pForm}$, $x \in \textbf{Var}$ and $t\in\textbf{Term}$. If $x\not \in \mc{A}$ then $\mc{A}[t/x]\equiv \mc{A}$ according to Theorem \ref{thm:fvsubpformnotappear} so $FV(\mc{A}[t/x]) =  FV(\mc{A})$. If $x\in FV(\mc{A})$ then $FV(\mc{A}[t/x]) = (FV(\mc{A})\setminus \{x\})\cup FV(t)$.

Suppose $\mc{A}$ is atomic so that $\mc{A} \equiv Pt_1\ldots t_n$. $FV(\mc{A}) = FV(t_1)\cup\ldots \cup FV(t_n)$. Since $x\in FV(\mc{A})$ then $x$ must be in at least one $t_i$ for $1 \le i \le n$. Suppose that there are $1\le j \le n$ values of $i$ for which $x\in FV(t_i)$ and $k=n-j$ values of $i$ fo which $x\not \in FV(t_i)$. Let $a_i$ for $1\le i \le j$ enumerate the values which $x\in FV(t_{a_i})$ and let $b_i$ for $1 \le i \le n-j$ enumerate the values for which $x\not \in FV(t_{b_i})$.

We have $\mc{A}[t/x] \equiv Pt_1[t/x]\ldots t_n[t/x]$ so

\begin{align}
FV(\mc{A}[t/x]) &= FV(t_1[t/x])\cup\ldots \cup FV(t_n[t/x]) \nonumber\\
&= FV(t_{a_1}[t/x])\cup\ldots\cup FV(t_{a_j}[t/x]) \cup FV(t_{b_1}[t/x]) \cup \ldots \cup FV(t_{b_k}[t/x]) \nonumber\\
\end{align}

According to Theorem \ref{thm:fvtermafterfvsub}, $FV(t_{a_i}[t/x]) = (FV(t_{a_i})\setminus\{x\})\cup FV(t)$ and $FV(t_{b_i}[t/x]) = FV(t_{b_i})$ so

\begin{align}
FV(\mc{A}[t/x]) =& \begin{aligned}[t]\big( &((FV(t_{a_1})\setminus\{x\})\cup FV(t)) \cup \ldots \cup ((FV(t_{a_j})\setminus\{x\})\cup FV(t)) \\
& \cup FV(t_{b_1}) \ldots \cup FV(t_{b_k}) \big)\end{aligned} \notag \\
=& \left(\left(FV(t_{a_1})\cup \ldots \cup FV(t_{a_j}) \cup FV(t_{b_1}) \cup \ldots \cup FV(t_{b_k})\right)\setminus \{x\} \right) \cup FV(t) \notag\\
=& \left(\left(FV(t_1)\cup \ldots \cup FV(t_n)\right)\setminus\{x\}\right)\cup FV(t) \notag\\
=& (FV(\mc{A})\setminus\{x\})\cup FV(t) \notag
\end{align}

\end{theorem}

\begin{theorem}[Quantified Variables after Free Variable Substitution]
\label{thm:qvafterfvsub}
Suppose $\mc{A} \in \textbf{pForm}$, $x\in\textbf{Var}$ and $t\in \textbf{Term}$. Then $QV(\mc{A}[t/x]) = QV(\mc{A})$.

Suppose $D(\mc{A}) = 0$. Then $\mc{A}$ is an atomic formula so $QV(\mc{A}) = \emptyset$. But $\mc{A}[t/x]$ is also an atomic formula so $QV(\mc{A}[t/x]) = \emptyset = QV(\mc{A})$.

Now assume that $D(\mc{A})>0$ and for $\mc{B}\in \textbf{pForm}$ that if $D(\mc{B})<D(\mc{A})$ that $QV(\mc{B}[t/x]) = QV(\mc{B})$.

If $\mc{A}$ is a negation then $\mc{A}\equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$ and $D(\mc{B}) < D(\mc{A})$. We have that $QV(\mc{A}[t/x]) = QV((\lnot \mc{B}[t/x])) = QV(\mc{B}[t/x]) = QV(\mc{B}) =  QV(\mc{A})$.

If $\mc{A}$ is an implication then $\mc{A} \equiv (\mc{B} \implies \mc{C})$ with $\mc{B}, \mc{C} \in \textbf{pForm}$ and $D(\mc{B}), D(\mc{C}) < D(\mc{A})$. We have
\begin{align}
QV(\mc{A}[t/x]) &= QV((\mc{B}[t/x]\implies \mc{C}[t/x])) = QV(\mc{B}[t/x]) \cup QV(\mc{C}[t/x]) \notag\\
&= QV(\mc{B}) \cup QV(\mc{C}) = QV(\mc{A}) \notag
\end{align}

If $\mc{A}$ is a quantification then $\mc{A} \equiv ((\forall y) \mc{B})$ with $y\in \textbf{Var}$, $\mc{B}\in\textbf{pForm}$, and $D(\mc{B}) < D(\mc{A})$. If $y\equiv x$ then $\mc{A}[t/x] \equiv \mc{A}$ so $QV(\mc{A}[t/x]) = QV(\mc{A})$. If $y\not \equiv x$ then $\mc{A}[t/x] \equiv ((\forall y) \mc{B}[t/x])$. In this case $QV(\mc{A}[t/x]) = QV(\mc{B}[t/x]) \cup \{y\} = QV(\mc{B})\cup\{y\} = QV(\mc{A})$.

\end{theorem}

\begin{theorem}[Free Variable Substitution in a Strict Formula Results in a Strict Formula]
Suppose $\mc{A}\in\textbf{sForm} = \textbf{Form}$, $x\in\textbf{Var}$ and $t\in\textbf{Term}$. We prove by induction on $D(\mc{A})$ that $\mc{A}[t/x]\in\textbf{Form}$.

If $D(\mc{A}) = 0$ then $\mc{A}$ is atomic. $\mc{A}[t/x]$ is also atomic so $\mc{A}[t/x]\in\textbf{Form}$.

Now suppose $D(\mc{A})>0$ but that for any $\mc{B}\in\textbf{Form}$ that if $D(\mc{B}) < D(\mc{A})$ that $\mc{B}[t/x]\in\textbf{Form}$.

$\mc{A}$ cannot be atomic because $D(\mc{A})\neq 0$.

If $\mc{A}$ is a negation then $\mc{A} \equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{Form}$ and $D(\mc{B})<D(\mc{A})$. We have $\mc{A}[t/x] \equiv (\lnot \mc{B}[t/x])$ but $\mc{B}[t/x]\in\textbf{Form}$ by the induction hypothesis so $\mc{A}[t/x]\in\textbf{Form}$.

If $\mc{A}$ is an implication then $\mc{A}\equiv (\mc{B}\implies \mc{C})$ with $\mc{B}, \mc{C}\in\textbf{Form}$ and $D(\mc{B}), D(\mc{C}) < D(\mc{A})$. We have $\mc{A}[t/x] \equiv (\mc{B}[t/x] \implies \mc{C}[t/x])$ but by the induction hypothesis $\mc{B}[t/x], \mc{C}[t/x]\in\textbf{Form}$ so $\mc{A}[t/x]\in\textbf{Form}$.

If $\mc{A}$ is a quantification then $\mc{A} \equiv ((\forall y) \mc{B})$ with $y\in\textbf{Var}$, $\mc{B}\in \textbf{Form}$, $D(\mc{B})<D(\mc{A})$, $y\in FV(\mc{B})$ and $y\not \in QV(\mc{B})$. If $y= x$ then $\mc{A}[t/x] \equiv \mc{A}$ so $\mc{A}[t/x]\in\textbf{Form}$. If $y\not \equiv x$ then $\mc{A}[t/x] \equiv ((\forall y) \mc{B}[t/x])$. By the induction hypothesis $\mc{B}[t/x]\in\textbf{Form}$. Because $x\in FV(\mc{B})$, we have, by the above theorem, that $FV(\mc{B}[t/x]) = (FV(\mc{B})\setminus\{x\})\cup FV(t)$. Since $y \neq x$ this means $y \in FV(\mc{B}[t/x])$. We also have, by another theorem above, that $QV(\mc{B}[t/x]) = QV(\mc{B})$ so $y\not \in QV(\mc{B}[t/x])$. This means that $\mc{A}[t/x]\in\textbf{Form}$.

\end{theorem}

Note that there are some term substitutions which are valid syntactically, but which are not "logically valid" in a way which will be described below.
As an example consider
$$
\mc{A} \equiv (Px \land ((\forall y) Qxy))
$$
Now let us substitute $y$ for $x$:
$$
\mc{A}[y/x] \equiv (Py \land ((\forall y) Qyy)
$$
Note how the semantic meaning of this formulas has been changed by the substitution.
In particular the problem is that the term being substituted in, $y$ in this case, became quantified over after being substituted for the $x$ in $Qxy$ within the scope of $(\forall y)$.
In such a circumstance we say the variable $y$ (which occurred in the substitution term) was "captured" by the $(\forall y)$ quantifier in $\mc{A}$.
To avoid such variable capture in logical deductions below we define the concept of substitutability.

\begin{definition}[Free Variable Substitutability]
Suppose $\mc{A}\in\textbf{pForm}$, $x\in\textbf{Var}$ and $t\in\textbf{Term}$. If $t$ is substitutable for $x$ in $\mc{A}$ then we say $S(\mc{A}, t, x)$. If it is the case that $S_F(\mc{A},t , x)$ then we may indicate this by letting $\mc{A} \equiv _{[t/x]}\mc{A}$.

\begin{itemize}
\item{If $\mc{A}$ is an atomic formula that $S_F(\mc{A}, t, x)$.}
\item{If $\mc{A} \equiv (\mc{B}\implies \mc{C})$ then $S_F(\mc{A}, t, x)$ if $S_F(\mc{B}, t, x)$ and $S_F(\mc{C}, t, x)$.}
\item{If $\mc{A}\equiv (\lnot \mc{B})$ then $S_F(\mc{A}, t, x)$ if $S(\mc{B}, t, x)$.}
\item{If $\mc{A}\equiv((\forall y) \mc{B})$ with $y\in\textbf{Var}$ then $S_F(\mc{A}, t, x)$ if $S_F(\mc{B}, t, x)$ and $y\not \in FV(t)$ or $x\not \in FV(\mc{B})$.}
\end{itemize}

\end{definition}

\begin{theorem}[Permissive Formula Depth after Free Variable Substitution]
\label{thm:pformdepthaftertermsub}
Suppose $\mc{A} \in \textbf{pForm}$, $x\in\textbf{Var}$ and $t\in\textbf{Term}$. Then $D(\mc{A}[t/x] = D(\mc{A})$.

Suppose $D(\mc{A}) = 0$.
Then $\mc{A}$ is atomic so $\mc{A}\equiv Pt_1\ldots t_n$ with $P\in\textbf{Pred}_n$ and $t_1,\ldots,t_n \in \textbf{Term}$.
$\mc{A}[t/x] \equiv P t_1[t/x]\ldots t_n[t/x]$.
By Theorem \ref{thm:fvsubintermgivesterm}, $t_1[t/x],\ldots, t_n[t/x]\in\textbf{Term}$ so $\mc{A}[t/x]$ is atomic so $D(\mc{A}[t/x])=0=D(\mc{A})$.

Now suppose that $D(\mc{A})>0$ but if $\mc{B}\in\textbf{pForm}$ with $D(\mc{B}) < D(\mc{A})$ that $D(\mc{B}[t/x]) = D(\mc{B})$.

$\mc{A}$ can't be atomic because $D(\mc{A}) >0$.

If $\mc{A}\equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$ then $D(\mc{A}) = D(\mc{B}) + 1$.
We have $\mc{A}[t/x] \equiv (\lnot \mc{B}[t/x])$ so $D(\mc{A}[t/x]) = D(\mc{B}[t/x]) + 1$
But, $D(\mc{B}) < D(\mc{A})$ so $D(\mc{B}[t/x]) = D(\mc{B})$ so $D(\mc{A}[t/x]) = D(\mc{A})$.

If $\mc{A} \equiv (\mc{B}\implies \mc{C})$ with $\mc{B}, \mc{C}\in\textbf{pForm}$ then $D(\mc{A}) = \text{max}(D(\mc{B}), D(\mc{C})) + 1$.
We have that $\mc{A}[t/x] \equiv (\mc{B}[t/x] \implies \mc{C}[t/x])$ so $D(\mc{A}[t/x]) = \text{max}(D(\mc{B}[t/x]), D(\mc{C}[t/x])) + 1$.
But $D(\mc{B}), D(\mc{C}) < D(\mc{A})$ so $D(\mc{B}[t/x]) = D(\mc{B})$ and $D(\mc{C}[t/x]) = D(\mc{C})$ so $D(\mc{A}[t/x]) = D(\mc{A})$.

If $\mc{A} \equiv ((\forall y)\mc{B})$ with $y\in\textbf{Var}$ and $\mc{B}\in\textbf{pForm}$ then $D(\mc{A}) = D(\mc{B}) + 1$.
If $x\equiv y$ then $\mc{A}[t/x] = \mc{A}$ so $D(\mc{A}[t/x]) = D(\mc{A})$.
If $x\not \equiv y$ then $\mc{A}[t/x] \equiv ((\forall y)\mc{B}[t/x])$ and $D(\mc{A}[t/x]) = D(\mc{B}[t/x]) + 1$.
But, $D(\mc{B}) < D(\mc{A})$ so $D(\mc{B}[t/x]) = D(\mc{B})$ so $D(\mc{A}[t/x]) = D(\mc{A})$.
\end{theorem}

\subsection{Bound Variable Substitution}

In addition to `plugging' terms into free variables, we are also interested in replacing one bound variable with a different variable.
This will help us to plug one formula into another by allowing us to replace all the bound variables in the first formula with variables which are unused in the second formula.
This is  necessary to avoid double quantification.

\begin{definition}[Bound Variable Substitution]
Suppose $\mc{A}\in\textbf{pForm}$ and $x, y \in \textbf{pForm}$.
We define $\mc{A}\{y/x\}$, the bound variable substitution of $y$ for $x$ in $\mc{A}$, as follows.

\begin{itemize}
\item{If $\mc{A}$ is atomic then $\mc{A}\{y/x\} \equiv \mc{A}$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$ then $\mc{A}\{y/x\} \equiv (\lnot \mc{B}\{y/x\})$.}
\item{If $\mc{A}\equiv (\mc{B}\implies\mc{C})$ with $\mc{B}, \mc{C} \in \textbf{pForm}$ then $\mc{A}\{y/x\} \equiv (\mc{B}\{y/x\} \implies \mc{C}\{y/x\})$.}
\item{Suppose $\mc{A} \equiv ((\forall z) \mc{B})$ with $z\in\textbf{Var}$ and $\mc{B}\in\textbf{pForm}$. If $z \equiv x$ then $\mc{A}\{y/x\} \equiv ((\forall y) (\mc{B}[y/x])\{y/x\})$. If $z\not \equiv x$ then $\mc{A}\{y/x\} \equiv ((\forall z)\mc{B}\{y/x\})$.}
\end{itemize}
\end{definition}

\begin{theorem}[Identical Bound Variable Substitution]
\label{thm:identqvsub}
Suppose $\mc{A}\in\textbf{pForm}$ and $x\in\textbf{Var}$.
Then $\mc{A}\{x/x\} \equiv \mc{A}$.

Suppose $D(\mc{A}) = 0$ so that $\mc{A}$ is atomic.
Then $\mc{A}\{x/x\} \equiv \mc{A}$.

Now suppose $D(\mc{A}) > 0$ but for any $\mc{B}\in\textbf{pForm}$ with $D(\mc{B}) < D(\mc{A})$ that $\mc{B}\{x/x\} \equiv \mc{B}$.

Suppose $\mc{A}\equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$.
We have $D(\mc{B}) < D(\mc{A})$ and $\mc{A}\{x/x\} \equiv (\lnot \mc{B}\{x/x\})$.
But, by the induction hypothesis $\mc{B}\{x/x\} \equiv \mc{B}$ so $\mc{A}\{x/x\} \equiv \mc{A}$.

Suppose $\mc{A}\equiv (\mc{B}\implies \mc{C})$ with $\mc{B}, \mc{C}\in\textbf{pForm}$.
We have $D(\mc{B}), D(\mc{C}) < D(\mc{A})$ and $\mc{A}\{x/x\} \equiv (\mc{B}\{x/x\}\implies \mc{C}\{x/x\})$.
But, by the induction hypothesis, $\mc{B}\{x/x\}\equiv \mc{B}$ and $\mc{C}\{x/x\} \equiv \mc{C}$ so $\mc{A}\{x/x\} \equiv \mc{A}$.

Suppose $\mc{A} \equiv ((\forall z)\mc{B})$ with $z\in\textbf{Var}$ and $\mc{B}\in\textbf{pForm}$.
We have $D(\mc{B}) < D(\mc{A})$.
If $z\not \equiv x$ then $\mc{A}\{x/x\} \equiv ((\forall z)\mc{B}\{x/x\})$ but, by the induction hypothesis, $\mc{B}\{x/x\} \equiv \mc{B}$ so $\mc{A}\{x/x\} \equiv \mc{A}$.
If $z \equiv x$ then $\mc{A}\{x/x\} \equiv ((\forall z)(\mc{B}[z/z])\{z/z\})$.
By Theorem \ref{thm:idenfvsub}, $\mc{B}[z/z] \equiv \mc{B}$.
The induction hypothesis then gives us that $\mc{B}\{z/z\} \equiv \mc{B}$ so $\mc{A}\{x/x\} \equiv \mc{A}$.
\end{theorem}

\begin{theorem}[Bound Variable Substitution in a Permissive Formula Results in a Permissive Formula]
Suppose $\mc{A}\in\textbf{pForm}$ and $x, y \in \textbf{Var}$. Then $\mc{A}\{y/x\}\in\textbf{pForm}$.

Suppose $D(\mc{A}) = 0$ so that $\mc{A}$ is atomic. Then $\mc{A}\{y/x\}\equiv \mc{A}$ so $\mc{A}\{y/x\} \in \textbf{pForm}$.

Now suppose $D(\mc{A}) >0$ and that if $\mc{B}\in\textbf{pForm}$ with $D(\mc{B}) < D(\mc{A})$ that $\mc{B}\{y/x\} \in \textbf{pForm}$.

$\mc{A}$ cannot be atomic because $D(\mc{A}) > 0$.

Suppose $\mc{A}\equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$.
Then $\mc{A}\{y/x\} \equiv (\lnot \mc{B}\{y/x\})$ but $D(\mc{B})<D(\mc{A})$ so $\mc{B}\{y/x\}\in\textbf{pForm}$ so $\mc{A}\{y/x\}\in\textbf{pForm}$.

Suppose $\mc{A} \equiv (\mc{B}\implies \mc{C})$ with $\mc{B}, \mc{C}\in\textbf{pForm}$.
Then $\mc{A}\{y/x\} \equiv (\mc{B}\{y/x\} \implies \mc{C}\{y/x\})$.
But, $D(\mc{B}), D(\mc{C}) < D(\mc{A})$ so $\mc{B}\{y/x\}, \mc{C}\{y/x\}\in\textbf{pForm}$ so $\mc{A}\in\textbf{pForm}$.

Suppose $\mc{A}\equiv ((\forall z)\mc{B})$ with $z\in\textbf{Var}$ and $\mc{B}\in\textbf{pForm}$.
If $z\not \equiv x$ then $\mc{A}\{y/x\} \equiv ((\forall z)\mc{B}\{y/x\})$.
But $D(\mc{B}) < D(\mc{A})$ so $\mc{B}\{y/x\} \in \textbf{pForm}$ so $\mc{A}\{y/x\}\in\textbf{pForm}$.
If $z\equiv x$ then $\mc{A}\{y/x\} \equiv ((\forall y) (\mc{B}[y/x])\{y/x\})$.
By Theorem \ref{thm:pformdepthaftertermsub}, $D(\mc{B}[t/x]) = D(\mc{B}) < D(\mc{A})$ so $(\mc{B}[y/x])\{y/x\}\in\textbf{pForm}$ so $\mc{A}\{y/x\}\in\textbf{pForm}$.
\end{theorem}

\begin{theorem}[Depth of a Permissive Formula after Bound Variable Substitution]
\label{thm:pformdepthafterqvsub}
Suppose $\mc{A}\in \textbf{pForm}$ and $x, y \in \textbf{Var}$.
Then $D(\mc{A}\{y/x\}) = D(\mc{A})$.

Suppose $D(\mc{A}) = 0$. Then $\mc{A}$ is atomic so $\mc{A}\{y/x\} = \mc{A}$ so $D(\mc{A}\{y/x\}) + D(\mc{A})$.

Now suppose $D(\mc{A}) > 0$ but that for $\mc{B}\in\textbf{pForm}$ with $D(\mc{B}) < D(\mc{A})$ that $D(\mc{B}\{y/x\}) = D(\mc{B})$.

$\mc{A}$ cannot be atomic because $D(\mc{A})>0$.

Suppose $\mc{A} \equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$.
We have $D(\mc{B}) < D(\mc{A})$ and $D(\mc{A}) = D(\mc{B}) + 1$.
We have $\mc{A}\{y/x\} \equiv (\lnot \mc{B}\{y/x\})$ so $D(\mc{A}\{y/x\}) = D(\mc{B}\{y/x\}) + 1$ but $D(\mc{B}\{y/x\}) = D(\mc{B})$ by the induction hypothesis so $D(\mc{A}\{y/x\}) = D(\mc{A})$.

Suppose $\mc{A} \equiv (\mc{B}\implies \mc{C})$ with $\mc{B}, \mc{C}\in \textbf{pForm}$.
We have $D(\mc{B}), D(\mc{C}) < D(\mc{A})$ and $D(\mc{A}) = \text{max}(D(\mc{B}), D(\mc{C})) + 1$.
We have $\mc{A}\{y/x\} \equiv (\mc{B}\{y/x\} \implies \mc{C}\{y/x\})$ so $D(\mc{A}\{y/x\}) = \text{max}(D(\mc{B}\{y/x\}), D(\mc{C}\{y/x\})) + 1$ but, by the induction hypothesis, $D(\mc{B}\{y/x\}) = D(\mc{B})$ and $D(\mc{C}\{y/x\}) = D(\mc{C})$ so $D(\mc{A}\{y/x\}) = D(\mc{A})$.

Suppose $\mc{A} \equiv ((\forall z)\mc{B})$ with $z\in\textbf{Var}$ and $\mc{B}\in\textbf{pForm}$.
We have $D(\mc{B}) < D(\mc{A})$ and $D(\mc{A}) = D(\mc{B}) + 1$.
If $x\equiv z$ then $\mc{A}\{y/x\} \equiv ((\forall y)(\mc{B}[y/x])\{y/x\}$ so $D(\mc{A}\{y/x\}) = D((\mc{B}[y/x])\{y/x\}) + 1$.
But $D(\mc{B}[y/x]) = D(\mc{B})<D(\mc{A})$ by Theorem \ref{thm:pformdepthaftertermsub} so, by the induction hypothesis, $D((\mc{B}[y/x])\{y/x\}) = D(\mc{B}[y/x]) = D(\mc{B})$ so $D(\mc{A}\{y/x\}) = D(\mc{A})$.
If $x\not \equiv z$ then $\mc{A}\{y/x\} \equiv ((\forall z) \mc{B}\{y/x\})$ and $D(\mc{A}\{y/x\}) = D(\mc{B}\{y/x\}) + 1$.
But, by the induction hypothesis, $D(\mc{B}\{y/x\}) = D(\mc{B})$ so $D(\mc{A}\{y/x\}) = D(\mc{A})$.

\end{theorem}

\begin{theorem}[Bound Variable Substitution for a Variable the Does Not Appear Bound]
\label{thm:bndvarsubforvarnotappear}
Suppose $\mc{A}\in\textbf{pForm}$ and $x, y \in \textbf{Var}$ and $x\not \in QV(\mc{A})$. Then $\mc{A}\{y/x\}\equiv \mc{A}$.

Suppose $D(\mc{A}) = 0$ so that $\mc{A}$ is atomic.
Then $\mc{A}\{y/x\} \equiv \mc{A}$.

Now suppose that $D(\mc{A}) > 0$ and that if $\mc{B}\in\textbf{pForm}$ with $D(\mc{B}) < D(\mc{A})$ that if $x\not \in QV(\mc{B})$ that $\mc{B}\{y/x\} \equiv \mc{B}$.

Suppose $\mc{A} \equiv ((\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$.
We have $D(\mc{B}) < D\mc{A})$.
We also have that $x\not \in QV(\mc{B})$ because $x\not \in QV(\mc{A})$.
We have that $\mc{A}\{y/x\} \equiv (\lnot \mc{B}\{y/x\})$ but, by the induction hypothesis, $\mc{B}\{y/x\} \equiv \mc{B}$ so $\mc{A}\{y/x\}\equiv \mc{A}$.

Suppose $\mc{A} \equiv (\mc{B}\implies \mc{C})$ with $\mc{B}, \mc{C} \in \textbf{pForm}$.
We have $D(\mc{B}), D(\mc{C}) < D(\mc{A})$.
We also have that $x\not \in QV(\mc{B})$ and that $x\not \in QV(\mc{C})$ because $x\not \in QV(\mc{A})$.
We have that $\mc{A}\{y/x\} \equiv (\mc{B}\{y/x\} \implies \mc{C}\{y/x\})$ but, by the induction hypothesis, $\mc{B}\{y/x\} \equiv \mc{B}$ and $\mc{C}\{y/x\}\equiv \mc{C}$ so $\mc{A}\{y/x\} \equiv \mc{A}$.

Suppose $\mc{A}\equiv ((\forall z) \mc{B})$ with $z\in\textbf{Var}$ and $\mc{B}\in\textbf{pForm}$.
We have that $D(\mc{B}) < D(\mc{A})$.
Because $x\not \in QV(\mc{A})$ we know that $z\not \equiv x$ and that $x\not \in QV(\mc{B})$.
We have that $\mc{A}\{y/x\} \equiv ((\forall z)\mc{B}\{y/x\})$ but, by the induction hypothesis, $\mc{B}\{y/x\} \equiv \mc{B}$ so $\mc{A}\{y/x\} \equiv \mc{A}$.
\end{theorem}

\begin{theorem}[Quantified Variables after Bound Variable Substitution]
\label{thm:qvafterqvsub}
Suppose $\mc{A}\in\textbf{pForm}$ and $x, y\in\textbf{Var}$.
If $x\not \in QV(\mc{A})$ then by Theorem \ref{thm:bndvarsubforvarnotappear} we know that $\mc{A}\{y/x\} \equiv \mc{A}$ so $QV(\mc{A}\{y/x\}) = QV(\mc{A})$. If $x\in QV(\mc{A})$ then we now prove that $QV(\mc{A}\{y/x\}) = (QV(\mc{A})\setminus\{x\})\cup\{y\}$.

Suppose $D(\mc{A}) = 0$ so that $\mc{A}$ is atomic.
Then $QV(\mc{A}) = \emptyset$ so $x\not \in QV(\mc{A})$ so we know $QV(\mc{A}\{y/x\}) = QV(\mc{A})$.

Now suppose that $D(\mc{A}) > 0$ but that for $\mc{B}\in\textbf{pForm}$ with $D(\mc{B}) < D(\mc{A})$ that, if $x\in QV(\mc{B})$, that $QV(\mc{B}\{y/x\}) = (QV(\mc{B})\setminus\{x\})\cup\{y\}$.

$\mc{A}$ cannot be atomic because $D(\mc{A})>0$.

Suppose $\mc{A}\equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$ so that $QV(\mc{A}) = QV(\mc{B})$.
We have $D(\mc{B}) < D(\mc{A})$.
Since $x \in QV(\mc{A})$ we have $x \in QV(\mc{B})$.
We have $\mc{A}\{y/x\} \equiv (\lnot \mc{B}\{y/x\})$ so $QV(\mc{A}\{y/x\}) = QV(\mc{B}\{y/x\})$ but, by the induction hypothesis, $QV(\mc{B}\{y/x\}) = (QV(\mc{B})\setminus \{x\})\cup\{y\}$ so $QV(\mc{A}\{y/x\}) = QV(\mc{A})$.

Suppose $\mc{A}\equiv (\mc{B}\implies \mc{C})$ with $\mc{B}, \mc{C}\in\textbf{pForm}$ so that $QV(\mc{A}) = QV(\mc{B}) \cup QV(\mc{C})$.
We have $D(\mc{B}), D(\mc{C}) < D(\mc{A})$.
Since $x \in QV(\mc{A})$ we have $x \in QV(\mc{B})$ and/or $x \in QV(\mc{C})$.
We have $\mc{A}\{y/x\} \equiv (\mc{B}\{y/x\} \implies \mc{C}\{y/x\})$ so $QV(\mc{A}\{y/x\}) = QV(\mc{B}\{y/x\}) \cup QV(\mc{C}\{y/x\})$.

If $x\in QV(\mc{B})$ then, by the induction hypothesis, $QV(\mc{B}\{y/x\}) = (QV(\mc{B})\setminus\{x\})\cup\{y\}$. If $x\not \in QV(\mc{B})$ then $QV(\mc{B}\{y/x\}) = QV(\mc{B})$.
Likewise, if $x\in QV(\mc{C})$ then $QV(\mc{C}\{y/x\}) = (QV(\mc{C})\setminus\{x\}) \cup \{y\}$ but if $x\not \in QV(\mc{C})$ then $QV(\mc{C}\{y/x\}) = QV(\mc{C})$.

There are three cases.
If $x\in QV(\mc{B})$ and $x\in QV(\mc{C})$ then
\begin{align}
QV(\mc{A}\{y/x\}) =& ((QV(\mc{B})\setminus\{x\})\cup \{y\}) \cup ((QV(\mc{C})\setminus\{x\})\cup \{y\}) \notag\\
=& ((QV(\mc{B}) \cup QV(\mc{C}))\setminus\{x\})\cup\{y\} \notag\\
=& (QV(\mc{A})\setminus\{x\})\cup\{y\} \notag
\end{align}

If $x\in QV(\mc{B})$ but $x\not \in QV(\mc{C})$ then
\begin{align}
QV(\mc{A}\{y/x\}) =& ((QV(\mc{B})\setminus\{x\})\cup\{y\})\cup QV(\mc{C}) \notag\\
=& ((QV(\mc{B})\cup QV(\mc{C}))\setminus\{x\})\cup\{y\} \notag\\
=& (QV(\mc{A})\setminus\{x\})\cup\{y\} \notag
\end{align}
Where the second line follows in part because $x\not \in QV(\mc{C})$.

If $x\not \in QV(\mc{C})$ but $x\in QV(\mc{C})$ then
\begin{align}
QV(\mc{A}\{y/x\}) =& QV(\mc{B}) \cup ((QV(\mc{C})\setminus\{x\})\cup\{y\}) \notag\\
=& ((QV(\mc{B})\cup QV(\mc{C}))\setminus\{x\})\cup\{y\} \notag\\
=& (QV(\mc{A})\setminus\{x\})\cup\{y\} \notag
\end{align}
Where the second line follows in part because $x\not \in QV(\mc{B})$.

In all cases $QV(\mc{A}\{y/x\}) = (QV(\mc{A})\setminus\{x\})\cup\{y\}$.

Now suppose $\mc{A}\equiv ((\forall z)\mc{B})$ with $z\in\textbf{Var}$ and $\mc{B}\in\textbf{pForm}$.
We $D(\mc{B}) < D(\mc{A})$.
Since $x\in QV(\mc{A})$ we have that $z\equiv x$ or $x\in QV(\mc{B})$ or both.
We treat all three cases.

Suppose $x\equiv z$ but $x\not \in QV(\mc{B})$.
Then $\mc{A}\{y/x\} \equiv ((\forall y)(\mc{B}[y/x])\{y/x\})$ so $QV(\mc{A}\{y/x\}) = QV((\mc{B}[y/x])\{y/x\})\cup \{y\}$.
We know from Theorem \ref{thm:qvafterfvsub} that $QV(\mc{B}[y/x]) = QV(\mc{B})$ so that $x\not \in QV(\mc{B}[y/x])$.
This means $(\mc{B}[y/x])\{y/x\} \equiv \mc{B}[y/x]$ by Theorem \ref{thm:qvafterfvsub} so that
$$
QV(\mc{A}\{y/x\}) = QV(\mc{B}[y/x])\cup\{y\} = QV(\mc{B})\cup\{y\}
$$
But note that $QV(\mc{A}) = QV(\mc{B})\cup\{x\}$ and since $x\not \in QV(\mc{B})$ we have $QV(\mc{B}) = QV(\mc{A})\setminus\{x\}$ so
$$
QV(\mc{A}\{y/x\}) = (QV(\mc{A})\setminus \{x\})\cup\{y\}
$$

Now suppose $x\not \equiv z$ but $x\in QV(\mc{B})$
Then $\mc{A}\{y/x\} \equiv ((\forall z)\mc{B}\{x/y\})$ so $QV(\mc{A}\{y/x\}) = QV(\mc{B}\{y/x\}) \cup z$.
We have, by the induction hypothesis, that $QV(\mc{B}\{y/x\}) = (QV(\mc{B})\setminus\{x\})\cup\{y\}$ so that
$$
QV(\mc{A}\{y/x\}) = (QV(\mc{B})\setminus\{x\})\cup \{y\} \cup \{z\}
$$
Which is equivalent to
\begin{align}
QV(\mc{A}\{y/x\}) =& ((QV(\mc{B})\cup\{z\})\setminus\{x\})\cup\{y\} \notag \\
=& (QV(\mc{A})\setminus\{x\})\cup\{y\} \notag
\end{align}
Where the first line follows because $z\not \equiv x$.

Now suppose $x\equiv z$ and $x\in QV(\mc{B})$.
Then $\mc{A}\{y/x\} \equiv ((\forall y) (\mc{B}[y/x])\{y/x\})$ so $QV(\mc{A}\{y/x\}) = QV((\mc{B}[y/x])\{y/x\}) \cup \{y\}$.
By Theorem \ref{thm:qvafterfvsub} we have $QV(\mc{B}[y/x]) = QV(\mc{B})$ so $x\in QV(\mc{B}[y/x])$.
So by the induction hypothesis we have
\begin{align}
QV((\mc{B}[y/x])\{y/x\}) =& (QV(\mc{B}[y/x])\setminus\{x\})\cup \{y\} \notag \\
=& (QV(\mc{B}) \setminus\{x\})\cup\{y\}
\end{align}

But $QV(\mc{A}) = QV(\mc{B})\cup\{x\}$  and it follows that $QV(\mc{A})\setminus\{x\} = QV(\mc{B})\setminus\{x\}$ so
\begin{align}
QV(\mc{A}\{y/x\}) =& QV((\mc{B}[y/x])\{y/x\})\cup \{y\} \notag \\
=& (QV(\mc{B}) \setminus\{x\}) \cup\{y\}\notag\\
=& (QV(\mc{A})\setminus\{x\}) \cup \{y\}\notag
\end{align}
\end{theorem}

\begin{theorem}[Non-Participating Free Variables After Bound Variable Substitution - 1]
Suppose $\mc{A}\in\textbf{pForm}$ and $x, y, z \in\textbf{Var}$ with $z\not \equiv y$.
Then if $z\in FV(\mc{A})$ then $z\in FV(\mc{A}\{y/x\})$.
Equivalently, $FV(\mc{A})\setminus\{y\} \subset FV(\mc{A}\{y/x\})$.

Suppose $D(\mc{A})=0$ so $\mc{A}$ is atomic.
Then $\mc{A}\{y/x\} \equiv \mc{A}$ so $FV(\mc{A}\{y/x\}) \equiv FV(\mc{A})$ so $z\in FV(\mc{A}\{y/x\})$.

Now suppose $D(\mc{A}) > 0$ but for $\mc{B}\in\textbf{pForm}$ with $D(\mc{B}) < D(\mc{A})$ that $z\in FV(\mc{B})$ implies $z\in FV(\mc{B}\{y/x\})$.

Suppose $\mc{A}\equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$.
We have $D(\mc{B})<D(\mc{A})$ and $\mc{A}\{y/x\} \equiv (\lnot \mc{B}\{y/x\})$.
Because $z\in FV(\mc{A})$ we have $z\in FV(\mc{B})$.
We then have, by the induction hypothesis, $z\in FV(\mc{B}\{y/x\}$ so $z\in FV(\mc{A}\{y/x\})$.

Suppose $\mc{A}\equiv(\mc{B}\implies \mc{C})$ with $\mc{B}, \mc{C}\in \textbf{pForm}$.
We have $D(\mc{B}), D(\mc{C}) < D(\mc{A})$ and $\mc{A}\{y/x\} \equiv (\mc{B}\{y/x\} \implies \mc{C}\{y/x\})$.
Because $z \in FV(\mc{A})$ we have either $z\in FV(\mc{B})$, in which case $z\in FV(\mc{B}\{y/x\})$ by the induction hypothesis, or $z\in FV(\mc{C})$, in which case $z\in FV(\mc{C}\{y/x\})$ by the induction hypothesis.
In either case $z\in FV(\mc{A}\{y/x\})$.

Suppose $\mc{A}\equiv ((\forall w)\mc{B})$ with $w\in\textbf{Var}$ and $\mc{B}\in\textbf{pForm}$.
We have $D(\mc{B}) < D(\mc{A})$.
Since $z\in FV(\mc{A})$ we have that $z \not \equiv w$ and $z\in FV(\mc{B})$.

If $w\not \equiv x$ then $\mc{A}\{y/x\} \equiv ((\forall w)\mc{B}\{y/x\})$.
by the induction hypothesis, we have that $z\in FV(\mc{B}\{y/x\})$ so $z\in FV(\mc{A}\{y/x\})$, noting that $z\not \equiv w$.

If $w \equiv x$ then $\mc{A}\{y/x\} \equiv ((\forall y) (\mc{B}[y/x]\{y/x\})$.

If $x\not \in FV(\mc{B})$ then $\mc{B}[y/x] \equiv \mc{B}$ by Theorem \ref{thm:fvsubpformnotappear} so $\mc{A}\{y/x\} \equiv ((\forall y) \mc{B}\{y/x\})$.
By the induction hypothesis, $z\in FV(\mc{B}\{y/x\})$ so, since $z\not \equiv y$ this also gives $z\in FV(\mc{A}\{y/x\})$.

If $x\in FV(\mc{B})$ then $FV(\mc{B}[y/x]) = (FV(\mc{B})\setminus\{x\})\cup\{y\}$ by Theorem \ref{thm:fvinpformafterfvsub}.
Since $x \equiv w \not \equiv z$ this means $z\in FV(\mc{B}[y/x])$.
We also have, by Theorem \ref{thm:pformdepthaftertermsub}, that $D(\mc{B}[y/x]) = D(\mc{B})$ so, by the induction hypothesis, we also have that $z\in FV((\mc{B}[y/x])\{y/x\})$.
This means, again because $z \not \equiv y$, that $z\in FV(\mc{A}\{y/x\}$.
\end{theorem}

\begin{theorem}[Free Variables after Bound Variable Substitution]
Suppose $\mc{A}\in\textbf{pForm}$ and $x, y, z\in\textbf{Var}$.
Then if $z\in FV(\mc{A}\{y/x\})$ then $z\in FV(\mc{A})$.
Equivalently, $FV(\mc{A}\{y/x\} \subset FV(\mc{A})$.

Suppose $D(\mc{A})=0$ so that $\mc{A}$ is atomic.
Then $\mc{A}\{y/x\} \equiv \mc{A}$ so $FV(\mc{A}\{y/x\}) = FV(\mc{A})$ so $z\in FV(\mc{A})$.

Now suppose $D(\mc{A})>0$ but for $\mc{B}\in\textbf{pForm}$ with $D(\mc{B})<D(\mc{A})$ that if $z\in \mc{B}\{y/x\}$ that $z\in \mc{B}$.

Suppose $\mc{A}\equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$.
We have $D(\mc{B}) < D(\mc{A})$ and $\mc{A}\{y/x\} \equiv (\lnot \mc{B}\{y/x\})$.
If $z\in FV(\mc{A}\{y/x\})$ then $z\in FV(\mc{B}\{y/x\})$ so, by the induction hypothesis, $z\in FV(\mc{B})$ which means $z\in FV(\mc{A})$.

Suppose $\mc{A}\equiv(\mc{B}\implies \mc{C})$ with $\mc{B}, \mc{C}\in\textbf{pForm}$.
We have $D(\mc{B}), D(\mc{C}) < D(\mc{A})$ and $\mc{A}\{y/x\} \equiv (\mc{B}\{y/x\} \implies \mc{C}\{y/x\})$.
If $z\in FV(\mc{A}\{y/x\})$ then either $z\in FV(\mc{B}\{y/x\}$, in which case we have $z\in FV(\mc{B})$ by the induction hypothesis, or $z\in FV(\mc{C}\{y/x\})$, in which case we have $z\in FV(\mc{C})$ by the induction hypothesis.
In either case we see $z\in FV(\mc{A})$.

Suppose $\mc{A}\equiv ((\forall w)\mc{B})$ with $w\in\textbf{Var}$ and $\mc{B}\in\textbf{pForm}$.
We have $D(\mc{B}) < D(\mc{A})$.

If $w\not \equiv x$ then $\mc{A}\{y/x\} \equiv ((\forall w) \mc{B}\{y/x\})$.
If $z\in FV(\mc{A}\{y/x\})$ then $z\not \equiv w$ and $z\in FV(\mc{B}\{y/x\})$.
By the induction hypothesis, $z\in FV(\mc{B})$ which implies $z\in FV(\mc{A})$, noting that $z\not \equiv w$.

If $w\equiv x$ then $\mc{A}\{y/x\} \equiv ((\forall y) (\mc{B}[y/x])\{y/x\})$.
If $z\in FV(\mc{A}\{y/x\})$ then $z\not \equiv y$ and $z\in FV((\mc{B}[y/x])\{y/x\})$.
By Theorem \ref{thm:pformdepthaftertermsub}, we have $D(\mc{B}[y/x]) = D(\mc{B}) < D(\mc{A})$ so, by the induction hypothesis, $z\in FV(\mc{B}[y/x])$.
We have $FV(\mc{B}[y/x]) = (FV(\mc{B})\setminus \{x\}) \cup \{y\}$.
We also have $FV(\mc{A}) = FV(\mc{B}) \setminus \{x\}$ so $FV(\mc{B}[y/x]) = FV(\mc{A}) \cup \{y\}$ but $z\not \equiv y$ so we must have $z\in FV(\mc{A})$.


\end{theorem}

There are two ways in which bound variable substitution can, unfortunately, alter the logical or semantic meaning of a permissive formula.
In both cases this is due to a variable being captured into the scope of a new quantifier.

In the first case a variable $y$ becomes captured when it is in the scope of a quantifier $\forall x$ when $\forall x \rightarrow \forall y$ after bound variable substitution:
\begin{align}
\mc{A} &\equiv ((\forall y) (Py \land ((\forall x) Qxy))) \notag\\
\mc{A}\{y/x\} &\equiv ((\forall y) (Py \land ((\forall y) Qyy))) \notag
\end{align}

In the second case a variable $x$ is changed into a $y$ after bound variable substitution resulting in it being captured by a $\forall y$ quantifier:
\begin{align}
\mc{B} &\equiv ((\forall x) (Px \land ((\forall y) Qxy))) \notag\\
\mc{B}\{y/x\} &\equiv ((\forall y) (Py \land ((\forall y) Qyy))) \notag
\end{align}

Because of such variable capture it is not always the case that $FV(\mc{A}\{y/x\}) = FV(\mc{A})$.
For example:
\begin{align}
\mc{A} &\equiv ((\forall x) Pxy) \notag\\
\mc{A}\{y/x\} &\equiv ((\forall y) Pyy)\notag
\end{align}
So $FV(\mc{A}) = \{y\}$ but $FV(\mc{A}\{y/x\}) = \emptyset$.

Similar to the case for free variable substitution, we will define the concept of bound variable substitutability to identify when bound variable substitution can be performed without altering the logical/semantic meaning of a permissive formula.

\begin{definition}[Bound Variable Substitutability for Permissive Formulas]
Suppose $\mc{A}\in\textbf{pForm}$ and $x, y \in \textbf{Var}$.
If $y$ is bound variable substitutable (of the first type) for $x$ in $\mc{A}$ we say $S_{B,1}(\mc{A}, y, x)$.
We define bound variable substitutability as follows.

\begin{itemize}
\item{If $\mc{A}$ is atomic then $S_{B,1}(\mc{A}, y, x)$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$ then $S_{B, 1}(\mc{A}, y, x)$ if $S_{B, 1}(\mc{B}, y, x)$.}
\item{If $\mc{A} \equiv (\mc{B}\implies \mc{C})$ with $\mc{B}, \mc{C}\in\textbf{pForm}$ then $S_{B,1}(\mc{A}, y, x)$ if $S_{B,1}(\mc{B}, y, x)$ and $S_{B, 1}(\mc{C}, y, x)$.}
\item{If $\mc{A} \equiv ((\forall z) \mc{B})$ with $z\in\textbf{Var}$ and $\mc{B}\in \textbf{pForm}$ then there are a few cases. If $z\not= x$ and $z\not = y$ then $S_{B,1}(\mc{A}, y, x)$ if $S_{B,1}(\mc{B}, y, x)$. If $z = x\not =y$ then $S_{B, 1}(\mc{A}, y, x)$ if $S_{B,1}(\mc{B}, y, x)$ and $y\not \in FV(\mc{B})$. If $z=y\not=x$ then $S_{B,1}(\mc{A}, y, x)$ if $S_{B,1}(\mc{B}, y, x)$ and $x\not \in FV(\mc{B})$. If $z=x=y$ then $S_{B,1}(\mc{A}, y, x)$.}
\end{itemize}
\end{definition}

\begin{theorem}[afdakls;]
Suppose $\mc{A}\in\textbf{pForm}$, $x, y \in \textbf{Var}$, and $S_{B,1}(\mc{A}, y, x)$.
Then if $y\in FV(\mc{A})$ then $y\in FV(\mc{A}\{y/x\})$.

Suppose $\mc{A} \equiv ((\forall z) \mc{B})$ with $z\in\textbf{Var}$ and $\mc{B}\in\textbf{pForm}$.
We have $D(\mc{B}) < D(\mc{A})$.
Because $y\in FV(\mc{A})$ we have $z\not \equiv y$ and $y\in FV\mc{B})$.
If $z\not \equiv x$ then $\mc{A}\{y/x\} \equiv ((\forall z) \mc{B}\{y/x\})$.
By the induction hypothesis, we have $y \in FV(\mc{B}\{y/x\}$ which implies that $y\in FV(\mc{A}\{y/x\})$ noting that $z\not \equiv y$.

Now suppose $z\equiv x \not \equiv y$.
Because $S_{B,1}(\mc{A}, y, x)$ we have that $y\not \in FV(\mc{B})$ which means that $y\not \in FV(\mc{A})$.

If $z\equiv x \equiv y$ then $\mc{A}\{y/x\} \equiv \mc{A}$ by Theorem \ref{thm:identqvsub} so $FV(\mc{A}\{y/x\}) = FV(\mc{A})$ so if $y\in FV(\mc{A})$ then $y\in FV(\mc{A}\{y/x\})$.

Then $\mc{A}\{y/x\} \equiv ((\forall y) \mc{B})$
\end{theorem}

\begin{theorem}[Bound Variable Substitutability after Free Variable Substitution]
\label{thm:qvsubstitutabilityafterfvsub}
Suppose $\mc{A}\in\textbf{pForm}$, $x, y\in \textbf{Var}$ and $S_{B,1}(\mc{A}, y, x)$. Then $S_{B,1}(\mc{A}[y/x], y, x)$.

Suppose $D(\mc{A}) = 0$ so that $\mc{A}$ is atomic.
Then $\mc{A}[y/x]$ is also atomic so $S_{B,1}(\mc{A}[y/x], y, x)$.

Now suppose $D(\mc{A})>0$ but that for $\mc{B}\in\textbf{pForm}$ with $D(\mc{B}) < D(\mc{A})$ that $S_{B,1}(\mc{B}, y, x)$ implies $S_{B,1}(\mc{B}[y/x],y,x)$.

Suppose $\mc{A} \equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$ so that $\mc{A}[y/x] \equiv (\lnot \mc{B}[y/x])$.
We have $D(\mc{B})<D(\mc{A})$ and, because $S_{B,1}(\mc{A}, y, x)$ that $S_{B,1}(\mc{B}, y, x)$ so, by the induction hypothesis $S_{B,1}(\mc{B}[y/x], y, x)$.
But this means $S_{B,1}(\mc{A}[y/x], y, x)$.

Suppose $\mc{A} \equiv (\mc{B}\implies\mc{C})$ with $\mc{B}, \mc{C}\in\textbf{pForm}$ so that $\mc{A}[y/x] \equiv (\mc{B}[y/x] \implies \mc{C}[y/x])$.
We have $D(\mc{B}), D(\mc{C}) < D(\mc{A})$ and, because $S_{B,1}(\mc{A}, y, x)$, that $S_{B,1}(\mc{B}, y, x)$ and $S_{B,1}(\mc{C}, y, x)$ so, by the induction hypothesis, $S_{B,1}(\mc{B}[y/x], y, x)$ and $S_{B,1}(\mc{C}[y/x], y, x)$.
But this means $S_{B,1}(\mc{A}[y/x], y, x)$.

Now Suppose $\mc{A} \equiv ((\forall z)\mc{B})$ with $z\in\textbf{Var}$ and $\mc{B}\in\textbf{pForm}$.
We have $D(\mc{B}) < D(\mc{A})$.
We consider four cases.

First suppose $z\not =x$ and $z \not = y$.
In this case $\mc{A}[y/x] \equiv ((\forall z)\mc{B}[y/x])$.
Because $S_{B,1}(\mc{A}, y, x)$ and $z\not =x$ and $z\not =y$ we have $S_{B,1}(\mc{B}, y, x)$.
By the induction hypothesis we have that $S_{B,1}(\mc{B}[y/x], y, x)$ but this implies $S_{B,1}(\mc{A}[y/x], y, x)$.

Now suppose $z=x$ but $x\not = y$.
In this case $\mc{A}[y/x] \equiv \mc{A}$ so $S_{B,1}(\mc{A}[y/x], y, x)$.

Now suppose $z\not = x$ but $z = y$.
In this case $\mc{A}[y/x] \equiv ((\forall y)\mc{B}[y/x])$.
Because $S_{B,1}(\mc{A}, y, x)$ and $z\not = x$ and $z=y$ we have that $S_{B,1}(\mc{B}, y, x)$ and $x\not \in FV(\mc{B})$.
This means $\mc{B}[y/x] \equiv \mc{B}$ so $\mc{A}[y/x] \equiv ((\forall y)\mc{B}) \equiv \mc{A}$ so $S_{B,1}(\mc{A}[y/x], y, x)$.

Now suppose $z=x=y$.
In this case $\mc{A}[y/x] \equiv \mc{A}$ since $z\equiv x$ so $S_{B,1}(\mc{A}[y/x], y, x)$.

\end{theorem}

\begin{theorem}[Free Variables after Bound Variable Substitution]
If $\mc{A}\in\textbf{pForm}$, $x, y\in\textbf{Var}$, and $S_{B,1}(\mc{A}, y, x)$ then $FV(\mc{A}\{y/x\}) = FV(\mc{A})$.

Suppose $D(\mc{A})=0$ so that $\mc{A}$ is atomic. Then $\mc{A}\{y/x\} = \mc{A}$ so $FV(\mc{A}\{y/x\}) = FV(\mc{A})$.

Now suppose $D(\mc{A})>0$ but that for $\mc{B}\in\textbf{pForm}$ with $D(\mc{B})<D(\mc{A})$ that if $S_{B,1}(\mc{B}, y, x)$ that $FV(\mc{B}\{y/x\}) = FV(\mc{B})$.

$\mc{A}$ cannot be atomic because $D(\mc{A})>0$.

Suppose $\mc{A} \equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$ so that $FV(\mc{A}) = FV(\mc{B})$
We also have $\mc{A}\{y/x\} = (\lnot \mc{B}\{y/x\})$ so that $FV(\mc{A}\{y/x\}) = FV(\mc{B}\{y/x\})$.
We have $D(\mc{B})<D(\mc{A})$ and, because $S_{B,1}(\mc{A}, y,x )$, that $S_{B, 1}(\mc{B}, y, x)$ so, by the induction hypothesis, $FV(\mc{B}\{y/x\}) = FV(\mc{B})$.
This means $FV(\mc{A}\{y/x\}) = FV(\mc{A})$.

Now suppose $\mc{A}\equiv (\mc{B}\implies \mc{C})$ with $\mc{B}, \mc{C}\in\textbf{pForm}$ so that $FV(\mc{A}) = FV(\mc{B})\cup FV(\mc{C})$.
We also have $\mc{A}\{y/x\} \equiv (\mc{B}\{y/x\} \implies \mc{C}\{y/x\})$ so that $FV(\mc{A}\{y/x\}) = FV(\mc{B}\{y/x\}) \cup FV(\mc{C}\{y/x\})$.
We have $D(\mc{B}), D(\mc{C}) < D(\mc{A})$ and, because $S_{B,1}(\mc{A}, y, x)$, that $S_{B,1}(\mc{B}, y, x)$ and $S_{B,1}(\mc{C}, y, x)$.
So, by the induction hypothesis $FV(\mc{B}\{y/x\}) = FV(\mc{B})$ and $FV(\mc{C}\{y/x\}) = FV(\mc{C})$ so that $FV(\mc{A}\{y/x\}) = FV(\mc{A})$.

Now suppose $\mc{A} \equiv ((\forall z)\mc{B})$ with $z\in\textbf{Var}$ and $\mc{B}\in\textbf{pForm}$ so that $FV(\mc{A}) = FV(\mc{B})\setminus \{z\}$.
We have $D(\mc{B}) < D(\mc{A})$ and, because $S_{B,1}(\mc{A}, y, x)$, that $S_{B, 1}(\mc{B}, y, x)$.
This means that, by the induction hypothesis, $FV(\mc{B}\{y/x\}) = FV(\mc{B})$.
There are multiple cases to consider.

Suppose $z \not = x$ and $z\not = y$.
Then $\mc{A}\{y/x\} \equiv ((\forall z) \mc{B}\{y/x\})$ so that
$$
FV(\mc{A}\{y/x\}) = FV(\mc{B}\{y/x\})\setminus\{z\}=FV(\mc{B})\setminus\{z\} = FV(\mc{A})
$$

Suppose $z=x$ but $z\not =y$.
In this case $\mc{A} \equiv ((\forall x) \mc{B})$ with $FV(\mc{A}) = FV(\mc{B}) \setminus\{x\}$.
We also have $\mc{A}\{y/x\} \equiv ((\forall y) (\mc{B}[y/x])\{y/x\})$ so that $FV(\mc{A}\{y/x\}) = FV((\mc{B}[y/x])\{y/x\}) \setminus \{y\}$.
Since $S_{B,1}(\mc{A}, y, x)$ this means $S_{B,1}(\mc{B}, y, x)$ and $y\not \in FV(\mc{B})$.
Since $S_{B,1}(\mc{B}, y, x)$, Theorem \ref{thm:qvsubstitutabilityafterfvsub} implies $S_{B,1}(\mc{B}[y/x], y, x)$.
Theorem \ref{thm:pformdepthaftertermsub} implies that $D(\mc{B}[y/x]) = D(\mc{B}) < D(\mc{A})$.
The induction hypothesis then implies $FV((\mc{B}[y/x])\{y/x\}) = FV(\mc{B}[y/x])$.
If $x\not \in FV(\mc{B})$ then $FV(\mc{B}[y/x]) = FV(\mc{B})$, by Theorem \ref{thm:fvsubpformnotappear}, so that $FV(\mc{A}\{y/x\}) = FV(\mc{B})\setminus \{y\} = FV(\mc{B})$ because $y\not \in \mc{B}$.
Since $x\not \in FV(\mc{B})$ this is equivalent to $FV(\mc{B})\setminus \{x\} = FV(\mc{A})$.
If $x\in FV(\mc{B})$ then Theorem \ref{thm:fvinpformafterfvsub} implies $FV(\mc{B}[y/x]) = ((FV(\mc{B})\setminus \{x\}) \cup \{y\})$ so
\begin{align}
FV(\mc{A}\{y/x\}) &= ((FV(\mc{B})\setminus \{x\}) \cup \{y\}) \setminus\{y\} \notag \\
&= FV(\mc{B})\setminus \{x\} = FV(\mc{A})
\end{align}
This follows because $y\not \in FV(\mc{B})$.

Now suppose $z \not = x$ but $z=y$.
In this case $\mc{A} \equiv ((\forall y)\mc{B})$ with $FV(\mc{A}) = FV(\mc{B})\setminus \{y\}$.
We also have $\mc{A}\{y/x\} \equiv ((\forall y) \mc{B}\{y/x\})$ so that $FV(\mc{A}\{y/x\}) = FV(\mc{B}\{y/x\})\setminus \{y\}$.
But $FV(\mc{B}\{y/x\}) = FV(\mc{B})$ so $FV(\mc{A}\{y/x\}) = FV(\mc{A})$.

Now suppose $z=x=y$.
Then $\mc{A}\equiv ((\forall z) \mc{B})$ with $FV(\mc{A}) = FV(\mc{B})\setminus \{z\}$.
We also have $\mc{A}\{y/x\} \equiv ((\forall z)(\mc{B}[z/z])\{z/z\})$
which is equivalent to $((\forall z)\mc{B}\{z/z\})$ by Theorem \ref{thm:idenfvsub}.
So $FV(\mc{A}\{y/x\}) = FV(\mc{B}\{y/x\}) \setminus \{z\} = FV(\mc{B})\setminus\{z\} = FV(\mc{A})$.
\end{theorem}

There is another problem with bound variable substitution in addition to the logical/semantic changes that can arise.
If we have $\mc{A}\in\textbf{sForm}$ and $x,y\in\textbf{Var}$ it is not necessarily the case that $\mc{A}\{y/x\}\in\textbf{sForm}$.
This problem persists even if $S_{B,1}(\mc{A}, y, x)$.
For example
\begin{align}
\mc{A} &\equiv ((\forall y) (Py \land ((\forall x) Qx))) \notag \\
\mc{A}\{y/x\} &\equiv ((\forall y) (Py\land ((\forall y) Qy))) \notag
\end{align}
We see that $\mc{A}\in\textbf{sForm}$ and $S_{B,1}(\mc{A}, y, x)$ but we see that $\mc{A}\{y/x\}\not \in \textbf{sForm}$.

To resolve this issue we define a second form of bound variable substitutability which will identify when bound variable substitution can be carried out while maintaining strict formulas.

\begin{definition}[Bound Variable Substitutability for Strict Formulas]
Suppose $\mc{A}\in\textbf{pForm}$ and $x, y\in\textbf{Var}$.
If $y$ is bound variable substitutable (of the second type) for $x$ in $\mc{A}$ we say $S_{B,2}(\mc{A}, y, x)$.
We define bound variable substitutability (of the second type) as follows.
\begin{itemize}
\item{If $\mc{A}$ is atomic then $S_{B,2}(\mc{A}, y, x)$.}
\item{If $\mc{A}\equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{pForm}$ then $S_{B,2}(\mc{A}, y, x)$ if $S_{B,2}(\mc{B}, y, x)$.}
\item{If $\mc{A}\equiv(\mc{B}\implies \mc{C})$ then $S_{B,2}(\mc{A}, y, x)$ if $S_{B,2}(\mc{B}, y, x)$ and $S_{B,2}(\mc{C}, y, x)$.}
\item{If $\mc{A} \equiv ((\forall z)\mc{B})$ with $z\in \textbf{Var}$ and $\mc{B}\in\textbf{pForm}$ then there are a few cases to consider. If $z\not \equiv x$ and $z\not \equiv y$ then $S_{B,2}(\mc{A}, y, x)$ if $S_{B,2}(\mc{B}, y, x)$.
If $z\equiv x$ but $z\not \equiv y$ then $S_{B,2}(\mc{A}, y, x)$ if $S_{B,2}(\mc{B}, y, x)$ and $y\not \in QV(\mc{B})$. If $z \not \equiv x$ but $z\equiv y$ then $S_{B,2}(\mc{A}, y, x)$ if $S_{B,2}(\mc{B}, y, x)$ and $x\not \in QV(\mc{B})$. If $z=x=y$ then $S_{B,2}(\mc{A}, y, x)$.}
\end{itemize}
\end{definition}

\begin{theorem}[Bound Variable Substitutability of the Second Type Maintains Strict Formulas]
Suppose $\mc{A}\in\textbf{sForm}$ and $x,y\in\textbf{Var}$.
Then if $S_{B,2}(\mc{A}, y, x)$ then $\mc{A}\{y/x\}\in\textbf{sForm}$.
\end{theorem}

Above we saw an example of $\mc{A}\in\textbf{pForm}$ with $S_{B,1}(\mc{A}, y, x)$ but without $S_{B,2}(\mc{A}, y, x)$.
It also possible to have the opposite.
Consider
\begin{align}
\mc{A} &\equiv ((\forall x) Qxy)\notag\\
\mc{A}\{y/x\} &\equiv ((\forall y) Qyy) \notag
\end{align}
We see that $S_{B,2}(\mc{A}, y, x)$ but not $S_{B,1}(\mc{A}, y, x)$.
We should think of the first form of substitutability as identifying logical/semantic consistency while the second form of substitutability enforces strict formulas as we will prove below.

\begin{theorem}[Strict Formulas and Bound Variable Substitutability of the Second Kind]
Suppose $\mc{A}\in\textbf{sForm}$, $x, y \in\textbf{Var}$ and $S_{B,2}(\mc{A}, y, x)$.
Then $\mc{A}\{y/x\} \in \textbf{sForm}$.

Suppose $D(\mc{A}) = 0$ so that $\mc{A}$ is atomic.
Then $\mc{A}\{y/x\} \equiv \mc{A}$ so $\mc{A}\{y/x\} \in\textbf{sForm}$.

Now suppose $D(\mc{A})>0$ but, for $\mc{B}\in\textbf{sForm}$ with $D(\mc{B})<D(\mc{A})$ that, if $S_{B,2}(\mc{B}, y, x)$ then $\mc{B}\{y/x\}\in\textbf{sForm}$.

Suppose $\mc{A}\equiv (\lnot \mc{B})$ with $\mc{B}\in\textbf{sForm}$.
We have $D(\mc{B}) < D(\mc{A})$ and $\mc{A}\{y/x\} \equiv (\lnot \mc{B}\{y/x\})$ but, by the induction hypothesis $\mc{B}\{y/x\}\in\textbf{sForm}$, so $\mc{A}\{y/x\}\in\textbf{sForm}$.

Suppose $\mc{A}\equiv (\mc{B}\implies \mc{C})$ with $\mc{B}, \mc{C}\in\textbf{sForm}$.
We have $D(\mc{B}), D(\mc{C}) < D(\mc{A})$ and $\mc{A}\{y/x\} \equiv (\mc{B}\{y/x\} \implies \mc{C}\{y/x\})$ but, by the induction hypothesis, $\mc{B}\{y/x\}, \mc{C}\{y/x\}\in\textbf{sForm}$ so $\mc{A}\{y/x\}\in\textbf{sForm}$.

Now suppose $\mc{A}\equiv((\forall z)\mc{B})$ with $z\in\textbf{Var}$ and $\mc{B}\in\textbf{sForm}$.
We consider multiple cases.

First suppose $y\equiv x$.
Then $\mc{A}\{y/x\} \equiv \mc{A}$ by Theorem \ref{thm:identqvsub} so $\mc{A}\{y/x\} \in \textbf{sForm}$.
We now consider $y\not \equiv x$.

Suppose $z\not \equiv x$.
Then $\mc{A}\{y/x\} \equiv ((\forall z) \mc{B}\{y/x\})$.
By Theorem \ref{thm:pformdepthafterqvsub}, $D(\mc{B}\{y/x\}) = D(\mc{B}) < D(\mc{A})$ so, by the induction hypothesis, $\mc{B}\{y/x\}\in\textbf{sForm}$.




We have $D(\mc{B}) < D(\mc{A})$ so, by the induction hypothesis $\mc{B}\{y/x\} \in \textbf{sForm}$.




We have $D(\mc{B}) < D(\mc{A})$.
Because $\mc{A}\in\textbf{sForm}$ we have $z\in FV(\mc{B})$ and $z\not \in QV(\mc{B})$.
Suppose $z\not \equiv x$.
Then $\mc{A}\{y/x\} \equiv ((\forall z) \mc{B}\{y/x\})$.
By Theorem \ref{thm:bndvarsubforvarnotappear}, $\mc{B}\{y/x\} \equiv \mc{B}$ so $\mc{A}\{y/x\} \equiv \mc{A}$ so $\mc{A}\{y/x\} \in\textbf{sForm}$.
Now suppose $z \equiv x$.
If $y\equiv x$ then $\mc{A}\{y/x\} \equiv \mc{A}$ by Theorem \ref{thm:identqvsub} so $\mc{A}\{y/x\}\in\textbf{sForm}$.
If $y\not \equiv x$ then $\mc{A}\{y/x\} \equiv ((\forall y) (\mc{B}[y/x])\{y/x\})$.
By Theorem \ref{thm:qvafterfvsub}, we have that $QV(\mc{B}[y/x]) = QV(\mc{B})$ so $x\not \in QV(\mc{B}[y/x])$ since $x\not \in QV(\mc{B})$.
This means, again by Theorem \ref{thm:bndvarsubforvarnotappear}, that $(\mc{B}[y/x])\{y/x\} \equiv \mc{B}[y/x]$ so that $\mc{A}\{y/x\} \equiv ((\forall y) \mc{B}[y/x])$.



Because $x\in FV(\mc{B})$ we have, by Theorem \ref{thm:fvinpformafterfvsub}, that $FV(\mc{B}[y/x]) = (FV(\mc{B})\setminus \{x\}) \cup \{y\}$.
We also have by Theorem
\end{theorem}


\section{Formal Proof}

We now transition from discussing the syntax for making and manipulating formulas within the formal language into combining the formulas into natural deduction proofs.
Semantically a proof encapsulates the idea that, if some set of formulas, called the \textit{premises}, are assumed to be true, then we can logically deduce other formulas which are also true.
Recall that, semantically speaking, we can only assign truth values to closed formulas.
For this reason, we will require that all formulas involved in natural deduction proofs be closed.

\subsection{Sequents}

Syntactically, we represent the idea that a (closed) formula $\mc{A}$ logically follows from a set of (closed) formulas $\Gamma$ using a \textit{sequent} or \text{judgment} which we express as
\begin{align}
\Gamma \vdash \mc{A}
\end{align}

The symbol in the middle $\vdash$ is the turnstile symbol.
The set of formulas in the set $\Gamma$ on the left hand side are called the antecedents of the sequent and the formula $\mc{A}$ on the right hand side is called the subsequent.
We say the subsequent of $\mc{A}$ is derivable form the antecedents $\Gamma$.
A more concrete example of a sequent is
$$
\{(\mc{A}\implies \mc{B}), \mc{A}\} \vdash \mc{B}
$$

We will use shorthand notation in writing the antecedents of sequents.
If we have a sequent
$$
\Gamma_1\cup \ldots \cup \Gamma_n \cup \{\mc{A}_1\}\cup \ldots \cup \{\mc{A}_n\} \vdash \mc{B}
$$
Then we can abbreviate this as
$$
\Gamma_1,\ldots, \Gamma_n, \mc{A}_1,\ldots, \mc{A}_n \vdash \mc{B}
$$
That is, singleton sets containing formulas have curly brackets suppressed and union operators are replaced by commas.
The notation $\Gamma_1 - \Gamma_2$ denotes the subtraction of set $\Gamma_2$ from $\Gamma_1$.
If $\Gamma = \{\mc{A}_1, \ldots, \mc{A}_n\}$ and $t$ does not appear in any of $\mc{A}_i$ (that is $\mc{A}_i \equiv {_{\{t\}}\mc{A}}$ for all $1\le i \le n$) then we say $t$ does not appear in $\Gamma$ and we may write $\Gamma = {_{\{t\}}\Gamma}$.

In what follows we will develop our proof theoretic framework which is essentially a calculus of the $\vdash$ symbol over the closed formulas of the language.
The framework will be based around rules of inference which allow us to derive new sequents from old sequents.

\section{Proofs and Proof Tables}

As mentioned above we begin with a set of premises and, through logical manipulations, we draw new conclusions.
In the context of syntactic proof theory, the premises serve as the starting point for a logical symbol manipulation game whereby the rules of inference allow us to infer or derive new logical formulas from the premises and other formulas.

In a proof we begin with one or more sets of closed formulas called premises.
For examples, one set of premises may be may be
$$
\Gamma = \{\gamma_1, \ldots, \gamma_n\}
$$
where each of $\gamma_i$ for $1\le i \le n$ is a closed formula.
We'll give an example of one of the simplest proofs possible.
Suppose
$$
\Gamma = \{(\mc{A}\implies \mc{B}), \mc{A}\}
$$
We can then write down a simple proof table.
\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{3cm} }
(1) & $\Gamma \vdash (\mc{A} \implies \mc{B})$ & Prem\\
(2) & $\Gamma \vdash \mc{A}$ & Prem\\
(3) & $\Gamma \vdash \mc{B}$ & $\implies E$, 1, 2
\end{tabular}
\end{center}

The proof table has three columns.
The left column numbers inferences within the proof, the middle column is a sequent, and the third column indicates the logical justification or inference rule for the conclusion of the sequent in the middle column.

The rules for a proof table are as follows.
The premises for a proof must be specified at the outset, though it is not strictly that the first lines of a proof outline any or all of the premise of the proof..
The premises appear as one or more sets $\Gamma_i$ of closed formulas.
If $\gamma$ is a formula in $\Gamma$ then it may be introduced to the proof via the Prem inference rule as shown above.

Each line of derivation is labeled (1), (2), $\ldots$ (n).
Each line of the proof follows from an inference rule (tabulated and explained in the following section) which may leverage some earlier lines in the proof.
Both the name of the inference rule and the lines on which the inference rule relies are specified in the right column.

The content of the proof above is that if we begin with the set of premises $\Gamma$, we can logically deduce $\mc{B}$.
Generally, if a sequent like $\Gamma \vdash \mc{B}$ appears on line ($n$) of a proof table then we say the proof constitutes a proof of length $n$ of the subsequent of the sequent on line ($n$) from the antecedents of the sequent on line ($n$).
Note that it is not strictly necessary that every premise appears as a Prem line in a proof.
Furthermore, it is not necessary that every premise appears in the antecedent of the final sequent.

Note that this proof format in fact allows us to derive new sequents from old sequents.
Alternatively, it is possible to develop a natural deduction in which the lines of the proofs are formulas rather than sequents.
The advantage of the sequent oriented approach is that it is very natural to keep track of on which earlier formulas a particular conclusion relies.
In the formula based approach the dependency formulas must be kept track of independently.

\subsection{Inference Rules}

Below I will write down all of the native inference rules for this system of natural deduction.
Each of these rules can be used in a formal proof as described above.
The rules of inference will be notated as one set of sequents above a line and one sequent below the line.
This indicates that the set of sequents above the line (the dependencies) logically allow the deduction of the sequent below the line (the conclusion).
If certain lines within a proof table match the format of the dependencies of a particular inference rule then we may append a new row to the proof table whose sequent is the conclusion sequent of the inference rule and indicating the inference rule and dependency lines appropriately.

\hrulefill
\begin{definition}[Rule of Prem (Premise)]
\leavevmode
If the premises for a proof are the sets $\{\Gamma_1, \ldots \Gamma_N\}$ and $\gamma_{ij} \in \Gamma_i$ then we have
\begin{prooftree}
\AxiomC{}
\UnaryInfC{$\Gamma_i \vdash \gamma_{ij}$}
\end{prooftree}

\end{definition}

\begin{definition}[Rule of A (Assumption)]
\leavevmode
\begin{prooftree}
\AxiomC{}
\UnaryInfC{$\mc{A}\vdash \mc{A}$}
\end{prooftree}
We can always assume that a (closed) formula $\mc{A}$ derives itself from no premises.
\end{definition}

\begin{definition}[Rule of W (Weakening)]
\leavevmode
\begin{prooftree}
\AxiomC{$\Gamma_1 \vdash \mc{A}$}
\UnaryInfC{$\Gamma_1, \Gamma_2 \vdash \mc{A}$}
\end{prooftree}

Note that $\Gamma_2$ may be the empty set. This allows us to derive a sequent that has already been derived on a later line. This will be useful to allow to rewrite the same sequent with the inclusion or exclusions of convenient abbreviations. (Each formula in $\Gamma_2$ must be closed)
\end{definition}

\begin{definition}[Rule of $\implies I$ (Implication Introduction)]
\leavevmode
\begin{prooftree}
\AxiomC{$\Gamma \vdash \mc{B}$}
\UnaryInfC{$\Gamma\backslash \{\mc{A}\} \vdash (\mc{A} \implies \mc{B})$}
\end{prooftree}

Note that $\mc{A}$ need not appear in $\Gamma$ for this inference rule. ($\mc{A}$ must be closed).
\end{definition}

\begin{definition}[Rule of $\implies E$ (Implication Elimination)]
\leavevmode
\begin{prooftree}
\AxiomC{$\Gamma_1 \vdash (\mc{A} \implies \mc{B})$}
\AxiomC{$\Gamma_2 \vdash \mc{A}$}
\BinaryInfC{$\Gamma_1, \Gamma_2 \vdash \mc{B}$}
\end{prooftree}
\end{definition}

\begin{definition}[Rule of $\lnot E$ (Negation Elimination)]
\leavevmode
\begin{prooftree}
\AxiomC{$\Gamma_1 \vdash \mc{A}$}
\AxiomC{$\Gamma_2 \vdash (\lnot \mc{A})$}
\BinaryInfC{$\Gamma_1, \Gamma_2 \vdash \curlywedge$}
\end{prooftree}
Here $\curlywedge\in\textbf{Pred}_0$ is the contradiction symbol.
\end{definition}

\begin{definition}[Rule of $\lnot I$ (Negation Introduction)]
\leavevmode
\begin{prooftree}
\AxiomC{$\Gamma \vdash \curlywedge$}
\UnaryInfC{$\Gamma \backslash \{\mc{A}\} \vdash (\lnot \mc{A})$}
\end{prooftree}
Note that $\mc{A}$ need not appear in $\Gamma$. ($\mc{A}$ must be closed.)
\end{definition}

\begin{definition}[Rule of $DN$ (Double Negation)]
\leavevmode
\begin{prooftree}
\AxiomC{$\Gamma \vdash (\lnot(\lnot \mc{A}))$}
\UnaryInfC{$\Gamma \vdash \mc{A}$}
\end{prooftree}
\end{definition}

\begin{definition}[Rule of $\forall I$ (Universal Introduction/Generalization)]
Suppose $\mc{A}\in \textbf{Form}$ with $FV(\mc{A}) = \{x\}$, $t\in \textbf{Term}$ is closed and $S(\mc{A}, t, x)$. Furthermore suppose $t$ does not appear in $\Gamma$ or $\mc{A}$.
Then we have

\begin{prooftree}
\AxiomC{$\Gamma \vdash \mc{A}[t/x]$}
\UnaryInfC{$\Gamma \vdash ((\forall x) \mc{A})$}
\end{prooftree}
\end{definition}

\begin{definition}[Rule of $\forall E$ (Universal Elimination/Instantiation/Specification)]
Suppose $t$ is a closed term, then
\begin{prooftree}
\AxiomC{$\Gamma \vdash ((\forall x)\mc{A})$}
\UnaryInfC{$\Gamma \vdash \mc{A}[t/x]$}
\end{prooftree}
\end{definition}
\hrulefill

We can modularize our proof finding apparatus by introducing so-called derived inference rules.
Suppose we have

Recall that when we have a proof table we may say that we have proven the sequent appearing on any line (typically the last line) from the premises.
Whenever we prove one sequent from some set of sequents we can introduce a new derived inference rule.
Specifically, if we have derived $\Theta \vdash \mc{B}$ on line $(n)$ then we can introduce a new inference rule.
We collect all premises which were used on lines $(j)$ with $j \le n$ and introduce:

\hrulefill

\textbf{Rule of [Der] (Derived Inference Rule)}

\begin{prooftree}
\AxiomC{$\Gamma_{i_1} \vdash \gamma_{j_1}, \ldots, \Gamma_{i_n} \vdash \gamma_{j_n}$}
\UnaryInfC{$\Theta \vdash \mc{B}$}
\end{prooftree}

\hrulefill

A derived inference rule can be thought of as an abbreviation for the proof that justifies the introduction of that derived rule.
That is, rather than repeat the proof of the derived inference in a new proof, one can skip the proof and simply use the derived inference rule.
Each derived inference rule may have its own name.

\newpage

We introduce a number of logical abbreviations to allow us to express the usual full range of logical connectives.

\begin{definition}[Logical Abbreviations]

Suppose $\mc{A}, \mc{B}\in \textbf{pForm}$

\begin{itemize}
\item{\textbf{Conjunction}: We let $(\mc{A} \land \mc{B})$ abbreviate $(\lnot(\mc{A} \implies (\lnot \mc{B})))$}
\item{\textbf{Disjunction}: We let $(\mc{A} \lor \mc{B})$ abbreviate $((\lnot \mc{A}) \implies \mc{B})$}
\item{\textbf{Equivalence}: We let $(\mc{A} \iff \mc{B})$ abbreviate $((\mc{A} \implies \mc{B}) \land (\mc{B} \implies \mc{A}))$}
\item{\textbf{Existential Quantification}: We let $((\exists x) \mc{A})$ abbreviate $(\lnot((\forall x)(\lnot \mc{A})))$. .}
\end{itemize}
\end{definition}

We also introduce abbreviations for removing parentheses.
This abbreviation language is copied directly from Tourlakis.

\begin{definition}[Parentheses Abbreviation]
\begin{itemize}
\item{\textbf{Parentheses:} ``To minimize the use of brackets in the metanotation we adopt standard \textit{priorities} of connectives: $\forall, \exists,$ and $\lnot$ have the highest, and then we have (in decreasing order of priority) $\land, \lor, \implies, \iff$, and we agree not to use outermost brackets. all \textit{associativities} are \textit{right} - that is if we write $\mc{A} \implies \mc{B} \implies \mc{C}$, then it is a (sloppy) counterpart for $(\mc{A} \implies ( \mc{B} \implies \mc{C}))$.''}
\end{itemize}
\end{definition}

\subsection{Metalanguage Concepts}

The next (final?) metalanguage concept we will introduce is that of sub-Wff replacement.
This will allow us to perform the operation of replacing an atomic formula, or sub-Wff within a Wff, by another Wff.

\noindent\fbox{\parbox{\textwidth}{
\textbf{Wff Substitution}

If $\mc{A}$ is a Wff, $\mc{X}$ is an atomic formula and $\mc{P}$ is a Wff then we define the replacement of $\mc{X}$ by $\mc{P}$ in $\mc{A}$, notated as $\mc{A}[\mc{P}/\mc{X}]$ by:

\begin{itemize}
\item{If $\mc{A}$ is atomic then if $\mc{A}\equiv \mc{X}$ then $\mc{A}[\mc{P}/\mc{X}] \equiv \mc{P}$. If $\mc{A}\not\equiv \mc{X}$ then $\mc{A}[\mc{P}/\mc{X}] \equiv \mc{A}$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ the $\mc{A}[\mc{P}/\mc{X}] \equiv (\lnot \mc{B}[\mc{P}/\mc{X}])$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $\mc{A}[\mc{P}/\mc{X}] \equiv (\mc{B}[\mc{P}/\mc{X}] \implies \mc{C}[\mc{P}/\mc{X}])$.}
\item{If $\mc{A} \equiv ((\forall x)\mc{B})$ then $\mc{A}[\mc{P}/\mc{X}] \equiv ((\forall x) (\mc{B}[\mc{P}/\mc{X}])$.}
\end{itemize}
}}

Above, we saw that in term substitution we always ended up with either a term or a Wff after the term substitution took place.
However, it was necessary to define the concept of substitutability to highlight substitutions that did or didn't fundamentally change the structure of a Wff.
In particular, it was important to be able to rule out substitution which would result in variable capture.

The case for Wff substitution is even more drastic.
Wff substitution as defined above does not even always result in a Wff.
There are three ways $\mc{A}[\mc{P}/\mc{X}]$ can be problematic.
The first one results in redundant quantification, the second results in repeated quantification, and the third results in variable capture.

\begin{itemize}
\item{If $\mc{X}$ is within the scope of a quantifier $(\forall x)$ but $x\not \in FV(\mc{P})$, then it is possible that after substitution of $\mc{P}$ for $\mc{X}$ that the quantifier $(\forall x)$ will not quantify over any variable $x$ resulting in redundant quantification. For example, let $\mc{X} \equiv Q_x$, $\mc{P} \equiv P$ and $\mc{A} \equiv ((\forall x) Q_x)$. Then $\mc{A}[\mc{P}/\mc{X}] \equiv ((\forall x) P)$. We can see this is not a valid Wff because $(\forall x)$ is a redundant quantifier. Note however that, even though $FV(\mc{X})$ contains an element not in $FV(\mc{P})$ it is not always the case that replacement of $\mc{X}$ by $\mc{P}$ will result in redundant quantification. For example if $\mc{A} \equiv ((\forall x)(Q_x \land S_x))$ then $\mc{A}[\mc{P}/\mc{X}] \equiv ((\forall x)(P\land S_x))$ which is in fact a Wff with no redundant quantification.}
\item{If $\mc{X}$ appears in $\mc{A}$ within the scope of some quantifier $(\forall x)$ and $\mc{P}$ quantifies over that same variable then $\mc{A}[\mc{P}/\mc{X}]$ will have repeated quantification. For example if $\mc{X} \equiv Q$, $\mc{P} \equiv ((\forall y) P_y)$ and $\mc{A} \equiv ((\forall y) (Q\land S_y))$ then $\mc{A}[\mc{P}/\mc{X}] \equiv ((\forall y)(((\forall y)P_y)\land S_y))$ which we can see is not a Wff because we have repeated quantification of $(\forall y)$.}
\item{Finally, if $\mc{X}$ is within the scope of some quantifier $(\forall x)$ but $x\not \in FV(\mc{X})$ but $x\in FV(\mc{P})$ then the free occurrences of $x$ in $\mc{P}$ will become captured after substitution. For example if $\mc{X} \equiv Q$, $\mc{P} \equiv P_x$ and $\mc{A} \equiv ((\forall x) (Q\land R_x))$ then $\mc{A} \equiv ((\forall x)(P_x\land R_x))$.}
\end{itemize}

We can always avoid the first and final issues by requiring that $FV(\mc{P}) = FV(\mc{X})$ when performing atomic formula substitution.
The second issue can be avoided by noting (as we will prove later) that any Wff $\mc{P}$ is equivalent to one in which all bound variables are replaced by new, fresh, bound variables which do not appear in $\mc{A}$.

We saw above that in term substitution it was necessary to define the concept of substitutability on top of substitution.
In the case of term substitution it was always the case that term substitution resulted in either a term or a Wff after substitution

Similar to the case



Note that $\mc{A}[\mc{P}/\mc{X}]$ is only a Wff if none of the quantifiers with $\mc{P}$ result in repeated quantification after substitution in $\mc{A}$.
For example consider

$$
\mc{A} \equiv ((\forall x) (Q_x \land \mc{X}))
$$

with $Q_x$ a 1-ary predicate and $\chi$ a 0-ary predicate.
Let $\mc{P} \equiv ((\forall x) S_x)$.
We see that $FV(\mc{X}) = FV(\mc{P}) = \emptyset$ so $\mc{A}[\mc{P}/\mc{X}]$ is define as above by

$$
\mc{A}[\mc{P}/\mc{X}] \equiv ((\forall x)(Q_x \land ((\forall x) S_x)))
$$

But we can see that this is not a Wff because of the repeated quantification over $x$.
This issue

Similarly to term replacement, we must also define a concept of substitutability for atoms. We say $\mc{S}(\mc{A}, \mc{P}, \mc{X})$ to mean that $\mc{P}$ is substitutable for $\mc{X}$ in $\mc{A}$. $\mc{S}(\mc{A}, \mc{P}, \mc{X})$ if (1) if $(\forall x)$ appear in $\mc{P}$ that $\mc{X}$ is not within the scope of any $(\forall x)$ quantifier in $\mc{A}$ and (2) no

\subsubsection{Draft}

Bound Variable Substitution

\noindent\fbox{\parbox{\textwidth}{
\textbf{Bound Variable Substitution}

If $\mc{A}$, $\mc{B}$, and $\mc{C}$ are Wffs and $x$ and $y$ are variables then we define $\mc{A}\{y/x\}$ as

\begin{itemize}
\item{If $\mc{A}$ is an atomic formula then $\mc{A}\{y/x\} \equiv \mc{A}$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $\mc{A}\{y/x\} \equiv (\mc{B}\{y/x\} \implies \mc{C}\{y/x\})$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ then $\mc{A}\{y/x\} \equiv (\lnot \mc{B}\{y/x\})$.}
\item{If $\mc{A} \equiv ((\forall z) \mc{B})$ then if $x \equiv z$ then $\mc{A}\{y/x\} \equiv ((\forall y) \mc{B}[y/z])$ and if $x\not \equiv z$ then $\mc{A}\{y/x\} \equiv \mc{A}$.}
\end{itemize}
}}

\noindent\fbox{\parbox{\textwidth}{
\textbf{Bound Variable Substitutability}

If $\mc{A}$, $\mc{B}$, and $\mc{C}$ are Wffs and $x$ and $y$ are variables then we define $\mc{A}\{y/x\}$ as

\begin{itemize}
\item{If $\mc{A}$ is an atomic formula then $S_B(\mc{A}, y, x)$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $S_B(\mc{A}, y, x)$ if $S_B(\mc{B}, y, x)$ and $S_B(\mc{C}, y, x)$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ then $S_B(\mc{A}, y, x)$ if $S_B(\mc{B}, y, x)$.}
\item{If $\mc{A} \equiv ((\forall z) \mc{B})$ then if $x \equiv z$ then $S_B(\mc{A}, y, x)$ if $S_F(\mc{B}, y, z)$. If $x\not \equiv z$ then $S_B(\mc{A}, y, x)$.}
\end{itemize}
}}

Proof that if $S_B(\mc{A}, y, x)$ then $\mc{A}\{y/x\}$ is a Wff. We induct on $D(\mc{A})$

Base case: If $D(\mc{A}) = 0$ then $\mc{A}$ is an atomic formula. Then $\mc{A}\{y/x\}$ is also an atomic formula so $\mc{A}\{y/x\}$ is a Wff.

Induction: Assume that, for any Wff $\mc{B}$, that if $D(\mc{B}) < n$ then $\mc{B}\{y/x\}$ is a Wff. Now assume $D(\mc{A}) = n$

If $\mc{A} \equiv (\lnot \mc{B})$ then $D(\mc{B}) = D(\mc{A}) - 1 < n$. $\mc{A}\{y/x\} \equiv (\lnot \mc{B}\{y/x\})$ but $\mc{B}\{y/x\}$ is a Wff so $\mc{A}\{y/x\}$ is also a Wff.

If $\mc{A} \equiv (\mc{B}\implies \mc{C})$ then $D(\mc{A}) = D(\mc{B}) + d(\mc{C}) + 1$ so $D(\mc{B}), D(\mc{C}) < \mc{A}$ so $\mc{B}\{y/x\}$ and $\mc{C}\{y/x\}$ are Wffs by induction hypothesis. $\mc{A}\{y/x\} \equiv (\mc{B}\{y/x\} \implies \mc{C}\{y/x\})$. Since $\mc{B}\{y/x\}$ and $\mc{C}\{y/x\}$ are Wffs we have that $\mc{A}\{y/x\}$ is a Wff.

Now suppose $\mc{A} \equiv ((\forall z)\mc{B})$. if $x\equiv z$ then $\mc{A}\{y/x\} \equiv ((\forall y)\mc{B}[y/x])$. For this to be a Wff we must have that $\mc{B}[y/z]\equiv$ is a Wff, $\mc{B}[y/z]$ contains $y$, but does not contain $(\forall y)$.

$\mc{B}[y/z]$ is a Wff because $\mc{B}$ is a Wff. <<need some proofs about term substitution to prove that since $z$ appears in $\mc{B}$ (because it's in the scope of $\forall z$ in $\mc{A}$) that $y$ appears in $\mc{B}[y/z]$. I think this will proceed by showing that $z \in FV(\mc{B})$ and that $FV(\mc{B}[y/z]) = FV(\mc{B})\backslash \{z\} \cup \{y\}$ so that $y\in FV(\mc{B}[y/z])$ and if $y\in FV(\mc{B}[y/z])$ then $y$ appears in $\mc{B}[y/z]$>>

But, because $S_B(\mc{A}, y, x)$, we have that $S_F(\mc{B}, y, z)$.

\newpage

\section{Deriving Inference Rules for Abbreviated Logical Symbols}

We will now derive the inference rules for the abbreviation logical symbols $\land, \lor, \iff$ and $\exists$.
We will first introduce some convenience inference rules.

\subsection{Convenience Inference Rules}

\subsubsection*{Weaker $\implies I$}

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{2cm} }
(1) & $\Gamma, \mc{A} \vdash \mc{B}$ & Prem\\
(2) & $\Gamma/\{\mc{A}\} \vdash (\mc{A} \implies \mc{B})$ & $\implies I$, 1\\
(3) & $\Gamma \vdash (\mc{A} \implies \mc{B})$ & $W$, 2
\end{tabular}
\end{center}

\subsubsection*{Weaker $\lnot I$}

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{2cm} }
(1) & $\Gamma, \mc{A} \vdash \curlywedge$ & Prem\\
(2) & $\Gamma/\{\mc{A}\} \vdash (\lnot \mc{A})$ & $\lnot I$, 1\\
(3) & $\Gamma \vdash (\lnot \mc{A})$ & $W$, 2
\end{tabular}
\end{center}

These gives us the weaker, but useful, versions of the inference rules.
On the surface these inference rules look similar, but they are different in the case that $\mc{A} \in \Gamma$.
This is because $(\Gamma \cup \{\mc{A}\})/\{\mc{A}\} = \Gamma / \{\mc{A}\}$ which is not necessarily equal to $\Gamma$.

\hrulefill

\textbf{Rule of $\implies I_W$ (Implication Introduction - Weak)}

\begin{prooftree}
\AxiomC{$\Gamma, \mc{A} \vdash \mc{B}$}
\UnaryInfC{$\Gamma \vdash (\mc{A} \implies \mc{B})$}
\end{prooftree}

\hrulefill

\textbf{Rule of $\lnot I_W$ (Negation Introduction - Weak)}

\begin{prooftree}
\AxiomC{$\Gamma, \mc{A} \vdash \curlywedge$}
\UnaryInfC{$\Gamma \vdash (\lnot \mc{A})$}
\end{prooftree}

\hrulefill

\subsection{Modus Tollens}

I'll prove three forms of Modus Tollens and give three corresponding derived rules.

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{3cm} }
(1) & $\Gamma_1 \vdash (\mc{A} \implies \mc{B})$ & Prem\\
(2) & $\Gamma_2 \vdash (\lnot \mc{B})$ & Prem\\
(3) & $\mc{A} \vdash \mc{A}$ & A\\
(4) & $\Gamma_1, \mc{A} \vdash \mc{B}$ & $\implies E$, 1, 3\\
(5) & $\Gamma_1, \Gamma_2, \mc{A} \vdash \curlywedge$ & $\lnot E$, 2, 4\\
(6) & $\Gamma_1, \Gamma_2 \vdash (\lnot \mc{A})$ & $\lnot I_W$, 5
\end{tabular}
\end{center}

From this we get our first form of the Modus Tollens derived inference rule.

\hrulefill

\textbf{Rule of $MT1$ (Modus Tollens)}

\begin{prooftree}
\AxiomC{$\Gamma_1 \vdash (\mc{A} \implies \mc{B})$}
\AxiomC{$\Gamma_2 \vdash (\lnot \mc{B})$}
\BinaryInfC{$\Gamma_1, \Gamma_2 \vdash (\lnot \mc{A})$}
\end{prooftree}

\hrulefill

We can use the derived inference rule to derive two closely related forms of the Modus Tollens inference rule.

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{3cm} }
(1) & $\Gamma \vdash (\mc{A}\implies \mc{B})$ & Prem\\
(2) & $(\lnot \mc{B}) \vdash (\lnot \mc{B})$ & A\\
(3) & $\Gamma, (\lnot \mc{B}) \vdash (\lnot \mc{A})$ & $MT1$, 1, 2\\
(4) & $\Gamma \vdash ((\lnot \mc{B}) \implies (\lnot \mc{A}))$ & $\implies I_W$, 3
\end{tabular}
\end{center}

\hrulefill

\textbf{Rule of $MT2$ (Modus Tollens)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash (\mc{A} \implies \mc{B})$}
\UnaryInfC{$\Gamma, (\lnot \mc{B}) \vdash (\lnot \mc{A})$}
\end{prooftree}

\hrulefill

\textbf{Rule of $MT3$ (Modus Tollens)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash (\mc{A} \implies \mc{B})$}
\UnaryInfC{$\Gamma \vdash ((\lnot \mc{B}) \implies (\lnot \mc{A}))$}
\end{prooftree}

\hrulefill

\subsection{Conjunction Inference Rules}
\subsubsection*{Proof for Right Conjunction Elimination}

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{2cm} }
(1) & $\Gamma \vdash (\mc{A}\land\mc{B})$ & Prem\\
(2) & $\Gamma \vdash (\lnot(\mc{A}\implies(\lnot\mc{B})))$ & $W$, 1\\
(3) & $(\lnot\mc{B}) \vdash (\lnot\mc{B})$ & $A$ \\
(4) & $(\lnot \mc{B}) \vdash (\mc{A} \implies (\lnot \mc{B}))$ & $\implies I$, 3\\
(5) & $\Gamma, (\lnot \mc{B})\vdash \curlywedge$ & $\lnot E$, 2, 4\\
(6) & $\Gamma \vdash (\lnot(\lnot \mc{B}))$ & $\lnot I_W$, 5\\
(7) & $\Gamma \vdash \mc{B}$ & $DN$, 6
\end{tabular}
\end{center}

\hrulefill

\textbf{Rule of $\land E_R$ (Conjunction Elimination - Right)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash (\mc{A} \land \mc{B})$}
\UnaryInfC{$\Gamma \vdash \mc{B}$}
\end{prooftree}


\hrulefill

\subsubsection*{Proof for Left Conjunction Elimination}

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{2cm} }
(1) & $\Gamma \vdash (\mc{A}\land\mc{B})$ & Prem\\
(2) & $\Gamma \vdash (\lnot(\mc{A}\implies(\lnot\mc{B})))$ & $W$, 1\\
(3) & $(\lnot\mc{A})\vdash (\mc{\lnot A})$ & A\\
(4) & $\mc{A} \vdash \mc{A}$ & A\\
(5) & $\mc{A}, (\lnot \mc{A}) \vdash \curlywedge$ & $\lnot E$, 3, 4\\
(6) & $\mc{A}, (\lnot \mc{A}) \vdash (\lnot \mc{B})$ & $\lnot I$, 5\\
(7) & $(\lnot \mc{A}) \vdash (\mc{A} \implies (\lnot \mc{B})$ & $\implies I$, 6\\
(8) & $\Gamma, (\lnot \mc{A}) \vdash \curlywedge$ & $\lnot E$, 2, 7\\
(9) & $\Gamma \vdash (\lnot (\lnot \mc{A}))$ & $\lnot I_W$, 8\\
(10) & $\Gamma \vdash \mc{A}$ & $DN$ 9
\end{tabular}
\end{center}

\hrulefill

\textbf{Rule of $\land E_L$ (Conjunction Elimination - Left)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash (\mc{A}\land \mc{B})$}
\UnaryInfC{$\Gamma \vdash \mc{A}$}
\end{prooftree}

\hrulefill

\subsubsection*{Proof for Conjunction Introduction}

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{3cm} }
(1) & $\Gamma_1 \vdash \mc{A}$ & Prem\\
(2) & $\Gamma_2 \vdash \mc{B}$ & Prem\\
(3) & $(\mc{A} \implies (\lnot \mc{B})) \vdash (\mc{A} \implies (\lnot \mc{B}))$ & A\\
(4) & $\Gamma_1, (\mc{A} \implies (\lnot \mc{B})) \vdash (\lnot \mc{B})$ & $\implies E$, 1, 3\\
(5) & $\Gamma_1, \Gamma_2, (\mc{A} \implies (\lnot \mc{B})) \vdash \curlywedge$ & $\lnot E$, 2, 4\\
(6) & $\Gamma_1, \Gamma_2 \vdash (\lnot (\mc{A} \implies (\lnot \mc{B})))$ & $\lnot I_W$ 5\\
(7) & $\Gamma_1, \Gamma_2 \vdash (\mc{A} \land \mc{B})$ & $W$, 6
\end{tabular}
\end{center}

\hrulefill

\textbf{Rule of $\land I$ (Conjunction Introduction)}

\begin{prooftree}
\AxiomC{$\Gamma_1 \vdash \mc{A}$}
\AxiomC{$\Gamma_2 \vdash \mc{B}$}
\BinaryInfC{$\Gamma_1, \Gamma_2 \vdash (\mc{A} \land \mc{B})$}
\end{prooftree}

\hrulefill
\subsection{Disjunction Inference Rules}
\subsubsection*{Proof for Disjunction Elimination}

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{3cm} }
(1) & $\Gamma_1 \vdash (\mc{A} \lor \mc{B})$ & Prem\\
(2) & $\Gamma_2 \vdash (\mc{A} \implies \mc{C})$ & Prem\\
(3) & $\Gamma_3 \vdash (\mc{B} \implies \mc{C})$ & Prem\\
(4) & $\Gamma_1 \vdash ((\lnot \mc{A}) \implies \mc{B})$ & $W$, 1\\
(5) & $(\lnot \mc{C}) \vdash (\lnot \mc{C})$ & A\\
(6) & $\Gamma_2, (\lnot \mc{C}) \vdash (\lnot \mc{A})$ & $MT$, 2, 5\\
(7) & $\Gamma_1, \Gamma_2, (\lnot \mc{C}) \vdash \mc{B}$ & $\implies E$, 4, 6\\
(8) & $\Gamma_1, \Gamma_2, \Gamma_3, (\lnot \mc{C}) \vdash \mc{C}$ & $\implies E$, 3, 7\\
(9) & $\Gamma_1, \Gamma_2, \Gamma_3, (\lnot \mc{C}) \vdash \curlywedge$ & $\lnot E$, 5, 8\\
(10) & $\Gamma_1, \Gamma_2, \Gamma_3 \vdash (\lnot(\lnot \mc{C}))$ & $\lnot I_W$, 9\\
(11) & $\Gamma_1, \Gamma_2, \Gamma_3 \vdash \mc{C}$ & $DN$, 10
\end{tabular}
\end{center}

\subsubsection*{Proof for Left Disjunction Introduction}

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{3cm} }
(1) & $\Gamma \vdash \mc{A}$ & Prem\\
(2) & $(\lnot \mc{A}) \vdash (\lnot \mc{A})$ & A\\
(3) & $\Gamma, (\lnot \mc{A}) \vdash \curlywedge$ & $\lnot E$, 1, 2\\
(4) & $\Gamma/\{(\lnot \mc{B})\}, (\lnot \mc{A}) \vdash (\lnot (\lnot \mc{B}))$ & $\lnot I$, 3\\
(5) & $\Gamma/\{(\lnot \mc{B})\}, (\lnot \mc{A}) \vdash \mc{B}$ & $DN$, 4\\
(6) & $\Gamma/\{(\lnot \mc{B})\} \vdash ((\lnot \mc{A}) \implies \mc{B})$ & $\implies I_W$, 5\\
(7) & $\Gamma \vdash ((\lnot \mc{A}) \implies \mc{B})$ & $W$, 6\\
(8) & $\Gamma \vdash (\mc{A} \lor \mc{B})$ & $W$, 7
\end{tabular}
\end{center}


\subsubsection*{Proof for Right Disjunction Introduction}

\begin{center}
\begin{tabular}{ p{1cm} p{6cm} p{3cm} }
(1) & $\Gamma \vdash \mc{B}$ & Prem\\
(2) & $\Gamma/\{(\lnot \mc{A})\} \vdash ((\lnot \mc{A}) \implies \mc{B})$ & $\implies I$, 1\\
(3) & $\Gamma \vdash ((\lnot \mc{A}) \implies \mc{B})$ & $W$, 2\\
(4) & $\Gamma \vdash (\mc{A} \lor \mc{B})$ & $W$, 3
\end{tabular}
\end{center}

With this we introduce the inference rules for disjunction:

\hrulefill

\textbf{Rule of $\lor E$ (Disjunction Elimination)}

\begin{prooftree}
\AxiomC{$\Gamma_1 \vdash (\mc{A}\lor \mc{B})$}
\AxiomC{$\Gamma_2 \vdash (\mc{A} \implies \mc{C})$}
\AxiomC{$\Gamma_3 \vdash (\mc{B} \implies \mc{C})$}
\TrinaryInfC{$\Gamma_1, \Gamma_2, \Gamma_3 \vdash \mc{C}$}
\end{prooftree}

\hrulefill

\textbf{Rule of $\lor I_L$ (Disjunction Introduction - Left)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash \mc{A}$}
\UnaryInfC{$\Gamma \vdash (\mc{A}\lor \mc{B})$}
\end{prooftree}

\hrulefill

\textbf{Rule of $\lor I_R$ (Disjunction Introduction - Right)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash \mc{B}$}
\UnaryInfC{$\Gamma \vdash (\mc{A}\lor \mc{B})$}
\end{prooftree}

\hrulefill

\subsection{Biconditional Inference Rules}
\subsubsection*{Proof for Biconditional Introduction}

\begin{center}
\begin{tabular}{ p{1cm} p{7cm} p{3cm} }
(1) & $\Gamma_1 \vdash (\mc{A} \implies \mc{B})$ & Prem\\
(2) & $\Gamma_2 \vdash (\mc{B} \implies \mc{A})$ & Prem\\
(3) & $\Gamma_1, \Gamma_2 \vdash ((\mc{A} \implies \mc{B}) \land (\mc{B}\implies \mc{A}))$ & $\land I$, 1, 2\\
(4) & $\Gamma_1, \Gamma_2 \vdash (\mc{A} \iff \mc{B})$ & $W$, 3
\end{tabular}
\end{center}

\subsubsection*{Proof for Left Biconditional Elimination}
\begin{center}
\begin{tabular}{ p{1cm} p{7cm} p{3cm} }
(1) & $\Gamma \vdash (\mc{A} \iff \mc{B})$ & Prem\\
(2) & $\Gamma \vdash ((\mc{A} \implies \mc{B}) \land (\mc{B} \implies \mc{A}))$ & $W$, 1\\
(3) & $\Gamma \vdash (\mc{A} \implies \mc{B})$ & $\land E_L$, 2\\
(4) & $\Gamma \vdash (\mc{B} \implies \mc{A})$ & $\land E_R$, 2
\end{tabular}
\end{center}

We get two new inference rules from the last two lines of this proof.
The inference rules for biconditional are

\hrulefill

\textbf{Rule of $\iff I$ (Biconditional Introduction)}

\begin{prooftree}
\AxiomC{$\Gamma_1 \vdash (\mc{A}\implies\mc{B})$}
\AxiomC{$\Gamma_2 \vdash (\mc{B} \implies \mc{A})$}
\BinaryInfC{$\Gamma_1, \Gamma_2 \vdash (\mc{A} \iff \mc{B})$}
\end{prooftree}

\hrulefill

\textbf{Rule of $\iff E_L$ (Biconditional Elimination - Left)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash (\mc{A} \iff \mc{B})$}
\UnaryInfC{$\Gamma \vdash (\mc{A} \implies \mc{B})$}
\end{prooftree}

\hrulefill

\textbf{Rule of $\iff E_R$ (Biconditional Elimination - Right)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash (\mc{A} \iff \mc{B})$}
\UnaryInfC{$\Gamma \vdash (\mc{B} \implies \mc{A})$}
\end{prooftree}

\hrulefill

\subsection{Inference Rules for Existence}

We now must derive the inference rule for existence introduction and elimination.
For reference I repeat the rules for universal introduction and elimination.

\hrulefill

\textbf{Rule of $\forall I$ (Universal Introduction/Generalization)}

\begin{prooftree}
\AxiomC{$_{\{t\}}\Gamma \vdash {_{[t/x], \{t\}}\mc{A}}[t/x]$}
\UnaryInfC{$\Gamma \vdash ((\forall x) \mc{A})$}
\end{prooftree}

\hrulefill

\textbf{Rule of $\forall E$ (Universal Elimination/Instantiation/Specification)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash ((\forall x)_{[t/x]}\mc{A})$}
\UnaryInfC{$\Gamma \vdash \mc{A}[t/x]$}
\end{prooftree}

\hrulefill

The rules for existential introduction and elimination will be

\hrulefill

\textbf{Rule of $\exists I$ (Existential Introduction/Generalization)}

\begin{prooftree}
\AxiomC{$\Gamma \vdash {_{[t/x]}\mc{A}}[t/x]$}
\UnaryInfC{$\Gamma \vdash ((\exists x)\mc{A})$}
\end{prooftree}

\hrulefill

\textbf{Rule of $\exists E$ (Existential Elimination/Instantiation/Specification)}

\begin{prooftree}
\AxiomC{$\Gamma_1 \vdash ((\exists x)\mc{A})$}
\AxiomC{$_{\{t\}}\Gamma_2 \vdash ({_{[t/x], \{t\}}\mc{A}}[t/x] \implies {_{\{t\}}\mc{B}})$}
\BinaryInfC{$\Gamma_1, \Gamma_2 \vdash \mc{B}$}
\end{prooftree}

\hrulefill

\subsubsection*{Proof for Existential Introduction}

\begin{center}
\begin{tabular}{ p{1cm} p{7cm} p{3cm} }
(1) & $\Gamma \vdash {_{[t/x]}\mc{A}}[t/x]$ & Prem\\
(2) & $((\forall x)(\lnot \mc{A})) \vdash ((\forall x)(\lnot \mc{A}))$ & A\\
(3) & $((\forall x)(\lnot \mc{A})) \vdash ((\forall x)(\lnot _{[t/x]}\mc{A}))$ & $W$, 2\\
(4) & $((\forall x)(\lnot \mc{A})) \vdash ((\forall x)_{[t/x]}(\lnot \mc{A}))$ & $W$, 3\\
(5) & $((\forall x)(\lnot \mc{A})) \vdash (\lnot \mc{A})[t/x]$ & $\forall E$, 4\\
(6) & $((\forall x)(\lnot \mc{A})) \vdash (\lnot \mc{A}[t/x])$ & $W$, 5\\
(7) & $\Gamma, ((\forall x)(\lnot \mc{A})) \vdash \curlywedge$ & $\lnot E$, 1, 6\\
(8) & $\Gamma \vdash (\lnot ((\forall x) (\lnot \mc{A})))$ & $\lnot I_W$, 7\\
(9) & $\Gamma \vdash ((\exists x) \mc{A})$ & $W$, 8
\end{tabular}
\end{center}

This proof had some subtlety involving replacement and substitutability.
The inferences from line 1 to line 2 and from line 2 to line 3 follow from the rules for substitutability.
Note that these inferences were labeled as abbreviations.
This is because lines 2, 3, and 4 indicate the same sequents, just with different metalogical annotations.
Similarly, line 6 follows from line 5 by the rules for term replacement.

I repeat the proof above with the metalogical notation carried out implicitly.

\begin{center}
\begin{tabular}{ p{1cm} p{7cm} p{3cm} }
(1) & $\Gamma \vdash \mc{A}[t/x]$ & Prem\\
(2) & $((\forall x)(\lnot \mc{A})) \vdash ((\forall x)(\lnot \mc{A}))$ & A\\
(3) & $((\forall x)(\lnot \mc{A})) \vdash (\lnot \mc{A}[t/x])$ & $\forall E$, 2\\
(4) & $\Gamma, ((\forall x)(\lnot \mc{A})) \vdash \curlywedge$ & $\lnot E$, 1, 3\\
(5) & $\Gamma \vdash (\lnot ((\forall x) (\lnot \mc{A})))$ & $\lnot I_W$, 4\\
(6) & $\Gamma \vdash ((\exists x) \mc{A})$ & $W$, 5
\end{tabular}
\end{center}


\subsubsection*{Proof for Existential Elimination}

\begin{center}
\begin{tabular}{ p{1cm} p{7cm} p{3cm} }
(1) & $\Gamma_1 \vdash ((\exists x)\mc{A})$ & Prem\\
(2) & $_{\{t\}}\Gamma_2 \vdash ({_{[t/x], \{t\}}\mc{A}}[t/x] \implies {_{\{t\}}\mc{B}})$  & Prem\\
(3) & $_{\{t\}}\Gamma_2,  (\lnot {_{\{t\}}\mc{B}}) \vdash (\lnot {_{[t/x], \{t\}}\mc{A}}[t/x])$ & $MT2$, 2\\
(4) & $_{\{t\}} \Gamma_2, {_{\{t\}}(\lnot \mc{B})} \vdash {_{[t/x], \{t\}}(\lnot \mc{A})}[t/x]$ & $W$, 3\\
(5) & $\Gamma_2, (\lnot \mc{B}) \vdash ((\forall x)(\lnot \mc{A}))$ & $\forall I$, 4\\
(6) & $\Gamma_1 \vdash (\lnot ((\forall x)(\lnot \mc{A})))$ & $W$, 1\\
(7) & $\Gamma_1, \Gamma_2, (\lnot \mc{B}) \vdash \curlywedge$ & $\lnot E$, 5, 6\\
(8) & $\Gamma_1, \Gamma_2 \vdash (\lnot(\lnot \mc{B}))$ & $\lnot I_W$, 7\\
(9) & $\Gamma_1, \Gamma_2 \vdash \mc{B}$ & $DN$, 8
\end{tabular}
\end{center}

I repeat this proof with implicit metalogical notation.

\begin{center}
\begin{tabular}{ p{1cm} p{7cm} p{3cm} }
(1) & $\Gamma_1 \vdash ((\exists x)\mc{A})$ & Prem\\
(2) & $\Gamma_2 \vdash (\mc{A}[t/x] \implies \mc{B})$  & Prem\\
(3) & $\Gamma_2,  (\lnot \mc{B}) \vdash (\lnot \mc{A})[t/x]$ & $MT2$, 2\\
(4) & $\Gamma_2, (\lnot \mc{B}) \vdash ((\forall x)(\lnot \mc{A}))$ & $\forall I$, 3\\
(5) & $\Gamma_1 \vdash (\lnot ((\forall x)(\lnot \mc{A})))$ & $W$, 1\\
(6) & $\Gamma_1, \Gamma_2, (\lnot \mc{B}) \vdash \curlywedge$ & $\lnot E$, 4, 5\\
(7) & $\Gamma_1, \Gamma_2 \vdash (\lnot(\lnot \mc{B}))$ & $\lnot I_W$, 6\\
(8) & $\Gamma_1, \Gamma_2 \vdash \mc{B}$ & $DN$, 7
\end{tabular}
\end{center}

These proofs complete the derivation of the inference rules for the existence quantifier.

\section{Theories}

A logical theory $\mc{T}$ consists of a language $\mc{L}$, a set of closed Wffs $\Gamma = \{\gamma_1, \ldots, \gamma_N\}$ in the language $\mc{L}$ which are referred to as the axioms of the theory, and a set of inference rules $\mc{R}$ such as those given above.
A proof in $\mc{T}$ is any proof that begins with $N$ premises of the form $\Gamma \vdash \gamma_i$ and uses the inference rules $\mc{R}$.
If it is possible to prove $\Gamma \vdash \mc{A}$ under these circumstance then we write

$$
\Gamma \vdash_{\mc{T}} \mc{A}
$$

Though we often drop the $\mc{T}$ subscript when the context of the theory is clear.
We would like to collect all of the Wffs which are derivable from $\Gamma$ under the inference rules $\mc{R}$.
If $\Gamma \vdash_{\mc{T}} \mc{A}$ and $\mc{T}$ is the theory with axioms $\Gamma$ and inference rules $\mc{R}$ then we write

$$
\mc{A} \in \Phi_{\mc{T}}
$$

$\Phi_{\mc{T}}$ is the set of all Wffs derivable under theory $\mc{T}$.

\subsection*{Extension}
Suppose $\mc{T}_1$ and $\mc{T}_2$ are theories with languages $\mc{L}_1$ and $\mc{L}_2$, axioms $\Gamma_1$ and $\Gamma_2$ and inference rules $\mc{R}_1$ and $\mc{R}_2$. We say that $\mc{T}_2$ is an extension of $\mc{T}_1$ if $\mc{L}_1 \subset \mc{L}_2$ (this means that $\mc{L}_1$ and $\mc{L}_2$ share their syntax rules but $\mc{L}_2$ has the same, or additional predicate or function symbols compared to $\mc{L}_1$), $\mc{R}_2 = \mc{R}_1$, and $\Gamma_1 \subset \Gamma_2$.
So we see that $\mc{T}_2$ may have more logical symbols than $\mc{T}_1$ and also may have more axioms than $\mc{T}_1$.

\subsection*{Conservative Extension}
If $\mc{T}_2$ is an extension of $\mc{T}_1$ and $\Phi_{\mc{T}_2} \cap \mc{L}_1 = \Phi_{\mc{T}_1}$ then we say that $\mc{T}_2$ is a conservative extension of $\mc{T}_1$.
That is, every Wff which is provable in $\mc{T}_1$ is also provable and $\mc{T}_2$ and every Wff of $\mc{L}_1$ which is provable in $\mc{T}_2$ is also provable in $\mc{T}_1$.

\subsection*{Extension by Definition}
There are two ways we can extend a theory by definition.
We can either introduce a new predicate or new function symbol and a corresponding axiom.
The concept of extension by function definition requires equality $=$ to be included in the language.
We will not yet include equality so we will save the description of that for later.

\subsubsection*{New Predicate Symbol}

Suppose $\mc{T}_1$ is a theory with language $\mc{L}_1$, axioms $\Gamma_1$ and inference rules $\mc{R}$.
Suppose $P$ is not a symbol in $\mc{L}_1$.
Suppose $\mc{Q}$ is a Wff in $\mc{L}_1$ with $FV(\mc{Q}) = \{x_1, \ldots , x_N\}$.
Let $\mc{L}_2$ be the same as $\mc{L}_1$ but with the addition of the $n$-ary predicate symbol $P$.
Let $\nu\in \mc{L}_2$ be the Wff

\begin{align*}
\nu \equiv& ((\forall x_1) (\ldots (\forall x_N) (P(x_1, \ldots, x_N) \iff \mc{Q}) \ldots ))\\
\equiv& \forall x_1,\ldots,x_N (P(x_1, \ldots, x_N) \iff \mc{Q})
\end{align*}

Let $\mc{R}_2 = \mc{R}_1$ and $\Gamma_2 = \Gamma_1 \cup \{\nu\}$.
The theory $\mc{T}_2$ determined by $\mc{L}_2, \Gamma_2$ and $\mc{R}_2$ is an extension by predicate definition of $\mc{T}_1$.


\section{Wff Atom Replacement}

If $\mc{A}$ is a Wff, $\mc{X}$ is an atomic formula, and $\mc{P}$ is a Wff then we define the replacement of $\mc{X}$ by $\mc{P}$ in $\mc{A}$, notated as $\mc{A}[\mc{P}/\mc{X}]$, by:

\hrulefill
\begin{itemize}
\item{If $\mc{A}$ is atomic then if $\mc{A}\equiv \mc{X}$ then $\mc{A}[\mc{P}/\mc{X}] \equiv \mc{P}$. If $\mc{A}\not\equiv \mc{X}$ then $\mc{A}[\mc{P}/\mc{X}] \equiv \mc{A}$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ the $\mc{A}[\mc{P}/\mc{X}] \equiv (\lnot \mc{B}[\mc{P}/\mc{X}])$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $\mc{A}[\mc{P}/\mc{X}] \equiv (\mc{B}[\mc{P}/\mc{X}] \implies \mc{C}[\mc{P}/\mc{X}])$.}
\item{If $\mc{A} \equiv ((\forall x)\mc{B})$ then $\mc{A}[\mc{P}/\mc{X}] \equiv ((\forall x) (\mc{B}[\mc{P}/\mc{X}])$.}
\end{itemize}
\hrulefill

Note that $\mc{A}[\mc{P}/\mc{X}]$ is not necessarily a Wff due to constraints with respect to quantification.
For $\mc{A}[\mc{P}/\mc{X}]$ to be a Wff it is necessary that, if $\mc{P}$ has any quantifier $(\forall x)$ that $\mc{X}$ is not within the scope of any $(\forall x)$ quantifier within $\mc{A}$.

Similarly to term replacement, we must also define a concept of substitutability for atoms. We say $\mc{S}(\mc{A}, \mc{P}, \mc{X})$ to mean that $\mc{P}$ is substitutable for $\mc{X}$ in $\mc{A}$. $\mc{S}(\mc{A}, \mc{P}, \mc{X})$ if (1) if $(\forall x)$ appear in $\mc{P}$ that $\mc{X}$ is not within the scope of any $(\forall x)$ quantifier in $\mc{A}$ and (2) no

\section{Wff Breakdown Tree}

Consider a Wff $\mc{A}$.
In what follows we will be interested in identifying particular Wff substrings of $\mc{A}$ such as $\mc{P}$.
For example, we may be interested in the Wff $\mc{A}$ with one instance of $\mc{P}$ replaced by $\mc{Q}$.
Unfortunately if $\mc{A} \equiv (\mc{P} \implies \mc{P})$, for example, then this sentence is ambiguous.
To resolve this problem we will come up with a rigorous schematic to uniquely specify an instance of a substring Wff in a main Wff.

We first define the overall depth of a Wff which, in some way, encodes the complexity of the Wff.
We will often induct on the depth of a Wff to prove metalogical theorems.

\hrulefill
\subsubsection*{Wff Depth}

\begin{itemize}
\item{If $\mc{A}$ is an atomic formula then $D(\mc{A}) = 0$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ then $D(\mc{A}) = D(\mc{B}) + 1$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $D(\mc{A}) = \text{max}(D(\mc{B}) + D(\mc{C})) + 1$.}
\item{If $\mc{A} \equiv ((\forall x) \mc{B})$ then $D(\mc{A}) = D(\mc{B}) + 1$.}
\end{itemize}
\hrulefill

We now need the ability to break a Wff down into its constituent parts.
The constituent parts of a Wff are contained in its parsing tree $T(\mc{A})$.
We develop the parsing tree $T(\mc{A})$ by applying a recursive algorithm on the Wff.
For any Wff we can find it's children by noticing its main connective, or top-level structure.
The elements of the parsing tree will be 4-tuples.
The first element of the tuple is one of $\{\text{Wff}, A, \lnot, \implies, \forall\}$.
This first element indicates how the sub-part is brought into $\mc{A}$.
The second element is an integer indicating the depth of the part within the overall Wff.
The third element is an integer indicating the branch number of the part within the Wff.
And finally the fourth element is a Wff representing the part.

We extract the children of a top level Wff $\mc{A}$ via $C(\mc{A})$.
If $C(\mc{A})$ is the children tree for $\mc{A}$ then $C_{+n, +m}(\mc{A})$ is the same tree but with $n$ added to the second element of every tuple and $m$ added to the third element of every tuple.


\hrulefill
\begin{itemize}
\item{If $\mc{A}$ is atomic then $C(\mc{A}) = \emptyset$.}
\item{If $\mc{A} \equiv (\lnot \mc{B})$ then $C(\mc{A}) = \{(\lnot, 1, 1, \mc{B})\} \cup C_{+1, +0}(\mc{B})$.}
\item{If $\mc{A} \equiv (\mc{B} \implies \mc{C})$. Let $b$ equal the maximum branch number in $C(\mc{B})$. then $C(\mc{A}) = \{(\implies_L, 1, 1, \mc{B}), (\implies_R, 1, b+1, \mc{C})\} \cup C_{+1, +0}(\mc{B}) \cup C_{+1, +b}(\mc{C})$.}
\item{If $\mc{A} \equiv ((\forall x)\mc{B})$ then $C(\mc{A}) \equiv \{(\forall, 1, 1, \mc{B})\} \cup C_{+1, +0}(\mc{B})$.}
\end{itemize}
\hrulefill

Finally the total parsing tree for Wff $\mc{A}$ is

$$
T(\mc{A}) = \{(\text{Wff}, 0, 1, \mc{A})\} \cup C(\mc{A})
$$

\subsection{Parsing Tree Unique Numbers}

\subsubsection{Smallest Depth and Branch Number}
Here we will prove that $C(\mc{A})$ contains no depth or branch number less than 1. We prove this by induction on the depth of $\mc{A}$, $D(\mc{A})$.

If $D(\mc{A}) = 1$ the $\mc{A}$ is atomic so $C(\mc{A}) = \emptyset$. There are no depth and branch numbers appearing in $C(\mc{A})$ so the condition is satisfied.

Now for the induction hypothesis we assume that if $D(\mc{B}) < n$ then $C(\mc{A})$ contains no depth or branch numbers less than 1.

Consider $\mc{A}$ with $D(\mc{A}) = n > 1$. $\mc{A}$ must not be atomic since $D(\mc{A}) \neq 1$.

If $\mc{A} \equiv (\lnot \mc{B})$ then $C(\mc{A}) = \{(\lnot, 1, 1, \mc{B})\} \cup C_{+1, +0}(\mc{B})$. Since $D(\mc{B}) < D(\mc{A}) = n$ no depth or branch number smaller than 1 appear in $C(\mc{B})$ by the induction hypothesis. We see then that no depth or branch number smaller than 1 appears in $C(\mc{A})$.

If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $C(\mc{A}) = \{(\implies_L, 1, 1, \mc{B}), (\implies, 1, b+1, \mc{C})\} \cup C_{+1, +0}(\mc{B}) \cup C_{+1, +b}(\mc{C})$ with $b$ the maximum branch number in $C(\mc{B})$. Since $D(\mc{B}), D(\mc{C}) < D(\mc{A}) = n$ no depth or branch number smaller than 1 appears in either $C(\mc{B})$ or $C(\mc{C})$ by the induction hypothesis. We see then that no depth or branch number smaller than 1 appears in $C(\mc{A})$.

If $\mc{A} \equiv ((\forall x)\mc{B})$ then $C(\mc{A}) = \{(\forall, 1, 1, \mc{B})\} \cup C_{+1, +0}(\mc{B})$. Since $D(\mc{B}) < D(\mc{A}) = n$ no depth or branch number smaller than 1 appears in $C(\mc{B})$. We see therefore that no depth or branch number smaller than 1 appears in $C(\mc{A})$.

\subsubsection{Unique Depth and Branch Numbers}

We now prove that the tuple $(n, m)$ specifying the depth and branch number for every element of $C(\mc{A})$ is unique within $C(\mc{A})$.

If $D(\mc{A}) = 1$ then $\mc{A}$ is atomic so $C(\mc{A}) = \emptyset$. There are no elements in $C(\mc{A})$ so the condition is vacuously satisfied.

For the induction hypothesis we assume that if $D(\mc{B}) < n$ then the depth and branch number tuples in $C(\mc{B})$ are unique.

Suppose $D(\mc{A}) = n > 1$. $\mc{A}$ cannot be atomic.

If $\mc{A} \equiv (\lnot \mc{B})$ then $C(\mc{A}) = \{(\lnot, 1, 1, \mc{B})\} \cup C_{+1, +0}(\mc{B})$. Since $D(\mc{B}) < D(\mc{A}) = n$ all of the tuples in $C(\mc{B})$ are unique by the induction hypothesis. There is no depth number smaller than 1 in $C(\mc{B})$ by the previous theorem so there is no depth number in $C_{+1, +0}(\mc{B})$ smaller than 2. This means all of the tuples in $C(\mc{A})$ are unique since $C(\mc{A})$ only adds one tuple, with depth number 1, to $C_{+1, +0}(\mc{B})$.

If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $C(\mc{A}) = \{(\implies, 1, 1, \mc{B}), (\implies, 1, b+1, \mc{C})\} \cup C_{+1, +0}(\mc{B}) \cup C_{+1, +b}(\mc{C})$ with $b$ the maximum branch number in $C(\mc{B})$. Since $D(\mc{B}), D(\mc{C}) < D(\mc{A}) = n$ all of the tuples in $C(\mc{B})$ are unique and all of the tuples in $C(\mc{C})$ are unique by the induction hypothesis. However, there is not yet guarantee against duplication of tuples between $C(\mc{B})$ and $C(\mc{C})$. There is no depth number smaller than 1 in both $C(\mc{B})$ and $C(\mc{C})$ by the previous theorem. Therefore, there is no depth number smaller than 2 in either $C_{+1, +0}(\mc{B})$ or $C_{+1, +b}(\mc{C})$. In particular this means that the set of depth and branch numbers from $\{(\implies, 1, 1, \mc{B})\} \cup C_{+1, +0}$ are unique amongst each other and the set of depth and branch numbers from $\{(\implies, 1, b+1, \mc{C})\} \cup C_{+1, +b}(\mc{C})$ are also unique amongst each other. There is no branch number smaller than 1 appearing in $C(\mc{C})$ so the smallest branch number appearing in the latter set from the previous sentence is $b+1$. This means the branch numbers between the two sets are unique since the former has a largest branch number of $b$ and the latter has a smaller branch number of $b+1$. This concludes the proof that if $\mc{A} \equiv (\mc{B} \implies \mc{C})$ that all depth and branch tuples in $C(\mc{A})$ are unique.

If $\mc{A} \equiv ((\forall x)\mc{B})$ then $C(\mc{A}) = \{(\forall, 1, 1, \mc{B})\} \cup C_{+1, +0}(\mc{B})$. Since $D(\mc{B}) < D(\mc{A}) = n$ all of the tuples in $C(\mc{B})$ are unique by the induction hypothesis. There is no depth number smaller than 1 in $C(\mc{B})$ by the previous theorem so the smallest depth number in $C_{+1, +0}(\mc{B})$ is 2. This means all of the tuples in $C(\mc{A})$ are unique.

\subsubsection{Unique Depth and Branch Numbers for Full Tree}

Since no depth number smaller than 1 appears in $C(\mc{A})$, we have that the depth and branch numbers in

$$
T(\mc{A}) = \{(\text{Wff}, 0, 1, \mc{A})\} \cup C(\mc{A})
$$

are unique.


\subsubsection{Maximum Depth Number is Equal to Wff Depth}

If $\mc{A}$ is atomic then $D(\mc{A}) = 1$ and $C(\mc{A}) = \{(A, 1, 1, \mc{A})\}$ so we see the maximum depth number is 1 as needed.

For the induction hypothesis we assume that if $D(\mc{B}) < n$ then the maximum depth number in $C(\mc{B})$ is equal to $D(\mc{B})$.

If $\mc{A} \equiv (\lnot \mc{B})$ then $D(\mc{A}) = D(\mc{B}) + 1$ and $C(\mc{A}) = \{(\lnot, 1, 1, \mc{B})\} \cup C_{+1, +0}(\mc{B})$. The maximum depth number in $C(\mc{B})$ is $D(\mc{B})$ by induction hypothesis so the maximum depth number in $C(\mc{A})$ is $D(\mc{B}) + 1 = D(\mc{A})$ as needed.

If $\mc{A} \equiv (\mc{B} \implies \mc{C})$ then $D(\mc{A}) = \text{max}(\mc{B}, \mc{C}) + 1$. The maximum depth number in $C(\mc{B})$ is $D(\mc{B})$ and the maximum depth number in $C(\mc{C})$ is $D(\mc{C})$ by the induction hypothesis. The maximum depth number in $C(\mc{A})$ is then $\text{max}(D(\mc{B}) + 1, D(\mc{C})+1) = \text{max}(D(\mc{B}), D(\mc{C})) + 1$ as needed.

If $\mc{A} \equiv ((\forall x)\mc{B})$ then $D(\mc{A}) = D(\mc{B}) + 1$. The maximum depth number in $C(\mc{B})$ is $D(\mc{B})$ by the induction hypothesis so the maximum depth number in $C(\mc{A})$ is $D(\mc{B}) + 1$ as needed.

It is clear than the maximum depth number in $T(\mc{A})$ is also equal to $D(\mc{A})$.

\newpage

\section{Proof for Equivalent Wff Replacement}

Consider the closed Wff $\mc{A}$.
Suppose the Wff $\mc{P}$ is a part of $\mc{A}$. By this we mean that $\mc{P}$ appears as the fourth element of one of the tuples listed in $T(\mc{A})$. In this case we say $\mc{P} \in T(\mc{A})$ (with a slight abuse of notation).
Suppose we have another Wff $\mc{Q}$ with $FV(\mc{Q}) \subset FV(\mc{P})$ and we have

$$
\nu \vdash ((\forall x_1)(\ldots((\forall x_n) (\mc{P} \iff \mc{Q})))).
$$

Let $\mc{A}' \equiv B\mc{Q}C$. I will now prove that $\nu \vdash (\mc{A} \iff \mc{A}')$.
For this I will need to define the depth of a Wff, $D(\mc{A})$.



We will prove the desired theorem by metalogical strong induction on the depth of the Wff $\mc{A}$.
That is, we will prove that the theorem holds in the case when $D(\mc{A}) = 1$.
Then, if $D(\mc{A}) = n$, we will assume the theorem holds for all integers $<n$ and the prove again that the theorem holds by $n$.
In this way we will be assured the theorem holds for all Wff depths.

\subsection*{Base Case}

If $D(\mc{A}) = 1$ then $\mc{A}$ is an atomic Wff.
Since $\mc{A}$ contains $\mc{P}$ as a substring $\mc{P}$ must also be atomic and we must have $\mc{A} \equiv \mc{P}$.
We then also have that $\mc{A}' \equiv \mc{Q}$.
Note that we must have $FV(\mc{P}) = FV(\mc{Q}) = \emptyset$ since $\mc{A}$ is an atomic closed Wff.
It is then the case that $(\mc{A} \iff \mc{A}') \equiv (\mc{P} \iff \mc{Q})$, but we already have that $\nu \vdash (\mc{P} \iff \mc{Q})$.

\subsection*{Induction Step}

Suppose $D(\mc{A}) = n$.
We have that
$$
\nu \vdash ((\forall x_1)(\ldots((\forall x_n) (\mc{P} \iff \mc{Q})))).
$$
We assume then that, for any Wff $\mc{B}$ with $D(\mc{B}) < n$ which contains $\mc{P}$ as a substring that we also can derive
$$
\nu \vdash (\mc{B} \iff \mc{B}')
$$
where $\mc{B}'$ is the result of replacing $\mc{P}$ by $\mc{Q}$ as above.




\section{Proof that Extension by Predicate Definition is a Conservative Extension}

Suppose $\mc{T}_2$ is an extension by predicate definition of $\mc{T}_1$ as in the previous section.
We will now prove that $\mc{T}_2$ is a conservative extension of $\mc{T}_1$.
For this we must prove that

$$
\Phi_{\mc{T}_2} \cap \mc{L}_1 = \Phi_{\mc{T}_1}
$$

Suppose $\mc{A} \in \Phi_{\mc{T}_1}$.
Then $\Gamma_1 \vdash_{\mc{T}_1} \mc{A}$.
We can apply the weakening inference rule ($W$) in $\mc{T}_2$ to then prove that $\Gamma_2 = \Gamma_1 \cup \{\nu\} \vdash_{\mc{T}_2} \mc{A}$.
This means $\mc{A} \in \Phi_{\mc{T}_2}$.
Since $\mc{A} \in \Phi_{\mc{T}_2}$ and $\mc{A} \in \mc{L}_1$ this means that

$$
\Phi_{\mc{T}_1} \subset \Phi_{\mc{T}_2} \cap \mc{L}_1
$$

The challenge for this proof is to prove that

$$
\Phi_{\mc{T}_2} \cap \mc{L}_1 \subset \Phi_{\mc{T}_1}
$$

Suppose $\mc{A} \in \Phi_{\mc{T}_2} \cap \mc{L}_1$.
This means that $\mc{A} \in \mc{L}_1$ and $\mc{A} \in \Phi_{\mc{T}_2}$.
Because $\mc{A} \in \Phi_{\mc{T}_2}$ so that there is a proof tree in $\mc{T}_2$ concluding $\Gamma_2 \vdash_{\mc{T}_2} \mc{A}$.
Our goal is to prove that $\Gamma_1 = \Gamma_2 \backslash \{\nu\} \vdash_{\mc{T}_1} \mc{A}$.

We will prove this in a few steps.
First we must define the concept of the translation of a Wff from $\mc{L}_2$ into $\mc{L}_1$.
Suppose $\mc{A} \in \mc{L}_2$.
We then define $\mc{A}^\# \in \mc{L}_1$ by induction.

\hrulefill
\subsubsection*{Wff Translation}
\begin{itemize}
\item{\textbf{Atomic Formulas:} If $\mc{A}$ is an atomic formula then if $\mc{A} \equiv Pt_1\ldots t_n$ then $\mc{A}^\# \equiv \mc{Q}$}
\end{itemize}
\hrulefill


We will accomplish this by metalogically deconstruction of the $\mc{T}_2$ proof of $\Gamma_2 \vdash_{\mc{T}_2} \mc{A}$.
This proof has length $N$.
We will prove by metalogical induction on proof length that if $\Gamma_2 \vdash_{\mc{T}_2} \mc{A}$ via a proof of length $N$, then it is possible to find a proof of $\Gamma_1 \vdash_{\mc{T}_1} \mc{A}$ (of unspecified length).

Suppose we have a proof that concludes $\Gamma \vdash_{\mc{T}} \mc{A}$.
We define the length of the proof to be the line number on which $\Gamma \vdash_{\mc{T}} \mc{A}$ appears minus the number of premises.
This means that if $\mc{A}$ is an axiom of $\mc{T}$ then it is possible to prove $\Gamma \vdash_{\mc{T}} \mc{A}$ in zero steps.

For induction we first work with a proof of length zero as our base case.
If $\Gamma \vdash_{\mc{T}_2} \mc{A}$ in zero steps then $\mc{A}$ is an axiom of $\mc{T}_2$.
This means that $\mc{A} \in \Gamma \cup \{\nu\}$

\end{document}


